#!/usr/bin/env python3
"""
å®æ—¶äººä½“åˆ†å‰²å’Œ3Dç‚¹äº‘ç”Ÿæˆè„šæœ¬

æ•´åˆç°æœ‰åŠŸèƒ½ï¼š
1. RealSenseç›¸æœºè¯»å–RGBå’Œæ·±åº¦æ•°æ®
2. SAM + Grounding DINOè¿›è¡Œäººä½“åˆ†å‰²
3. å°†æ©ç è½¬æ¢ä¸º3Dç‚¹äº‘

ä½¿ç”¨æ–¹æ³•:
    python3 realtime_segmentation_3d.py --output_dir output_data --save_pointcloud

ä¾èµ–:
    - ç°æœ‰çš„seg.py, mask_to_3d.py, realsense_capture.py
    - pyrealsense2, opencv-python, numpy, torch
    - segment_anything, transformers, open3d
"""

import argparse
import os
import sys
import time
import math
import numpy as np
import cv2
import torch
from datetime import datetime
from tqdm import tqdm
from PIL import Image

# å¯¼å…¥ç°æœ‰æ¨¡å—çš„åŠŸèƒ½
from seg import init_models# process_image_cv2
from mask_to_3d import mask_to_3d_pointcloud, save_pointcloud, load_camera_intrinsics
from realsense_capture import setup_realsense, depth_to_pointcloud, save_pointcloud_to_file

# è¿½åŠ è‡ªå®šä¹‰æ¨¡å—æœç´¢è·¯å¾„ï¼ˆæ‰‹çœ¼æ ‡å®šç›®å½•ï¼‰
_extra_paths = [
    "/home/ai/AI_perception/hand_eye_calibrate",
]
for _p in _extra_paths:
    try:
        if os.path.isdir(_p) and _p not in sys.path:
            sys.path.insert(0, _p)
    except Exception:
        pass

class RealtimeSegmentation3D:
    def __init__(self, output_dir, device="cpu", save_pointcloud=True, intrinsics_file=None, hand_eye_file=None, bbox_selection="highest_confidence", debug=False, use_yolo=False, yolo_weights=None):
        """
        åˆå§‹åŒ–å®æ—¶åˆ†å‰²å’Œ3Dç‚¹äº‘ç”Ÿæˆå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            device: è¿è¡Œè®¾å¤‡ (cpu/cuda)
            save_pointcloud: æ˜¯å¦ä¿å­˜3Dç‚¹äº‘
            intrinsics_file: ç›¸æœºå†…å‚JSONæ–‡ä»¶è·¯å¾„
            hand_eye_file: æ‰‹çœ¼æ ‡å®š4x4é½æ¬¡çŸ©é˜µçš„.npyæ–‡ä»¶è·¯å¾„ï¼ˆç›¸æœºâ†’å¤¹çˆªï¼‰
            bbox_selection: è¾¹ç•Œæ¡†é€‰æ‹©ç­–ç•¥ ("smallest" æˆ– "largest" æˆ– "highest_confidence")
            debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆä¿å­˜æ‰€æœ‰ä¸­é—´æ–‡ä»¶ï¼‰
            use_yolo: æ˜¯å¦ä½¿ç”¨YOLOä½œä¸ºæ£€æµ‹å™¨
            yolo_weights: YOLOæƒé‡è·¯å¾„ï¼ˆ.ptï¼‰
        """
        self.output_dir = output_dir
        self.device = device
        self.save_pointcloud = save_pointcloud
        self.bbox_selection = bbox_selection
        self.debug = debug
        self.use_yolo = use_yolo
        self.yolo_weights = yolo_weights
        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹åˆ›å»ºï¼‰
        if self.debug:
            self.rgb_dir = os.path.join(output_dir, "rgb")
            self.depth_dir = os.path.join(output_dir, "depth")
            self.mask_dir = os.path.join(output_dir, "masks")
            self.pointcloud_dir = os.path.join(output_dir, "pointclouds")
            self.segmentation_dir = os.path.join(output_dir, "segmentation")
            self.detection_dir = os.path.join(output_dir, "detection")
            
            os.makedirs(self.rgb_dir, exist_ok=True)
            os.makedirs(self.depth_dir, exist_ok=True)
            os.makedirs(self.mask_dir, exist_ok=True)
            if save_pointcloud:
                os.makedirs(self.pointcloud_dir, exist_ok=True)
            os.makedirs(self.segmentation_dir, exist_ok=True)
            os.makedirs(self.detection_dir, exist_ok=True)
            print("è°ƒè¯•æ¨¡å¼å·²å¯ç”¨ï¼Œå°†ä¿å­˜æ‰€æœ‰ä¸­é—´æ–‡ä»¶")
        else:
            print("æ­£å¸¸æ¨¡å¼ï¼Œä¸ä¿å­˜ä¸­é—´æ–‡ä»¶")
        
        # åˆå§‹åŒ–æ¨¡å‹
        print("æ­£åœ¨åˆå§‹åŒ–AIæ¨¡å‹...")
        self.sam_predictor, self.grounding_dino_model, self.processor = init_models(device)
        
        if self.use_yolo:
            if not self.yolo_weights or not os.path.exists(self.yolo_weights):
                print(f"[è­¦å‘Š] å·²å¯ç”¨YOLOæ£€æµ‹ï¼Œä½†æœªæ‰¾åˆ°æƒé‡: {self.yolo_weights}ï¼Œå°†å›é€€Grounding DINO")
                self.use_yolo = False
        
        # åˆå§‹åŒ–RealSenseç›¸æœº
        print("æ­£åœ¨åˆå§‹åŒ–RealSenseç›¸æœº...")
        self.pipeline, self.config = setup_realsense()
        if self.pipeline is None:
            raise RuntimeError("æ— æ³•å¯åŠ¨RealSenseç›¸æœº")
        
        # è·å–ç›¸æœºå†…å‚å’Œç•¸å˜ç³»æ•°
        self.fx, self.fy, self.cx, self.cy, self.dist, self.mtx = load_camera_intrinsics(intrinsics_file)
        print(f"ä½¿ç”¨ç›¸æœºå†…å‚: fx={self.fx}, fy={self.fy}, cx={self.cx}, cy={self.cy}")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç•¸å˜æ ¡æ­£
        if np.any(self.dist != 0):
            print("æ£€æµ‹åˆ°ç•¸å˜ç³»æ•°ï¼Œå°†è¿›è¡Œå®æ—¶å›¾åƒç•¸å˜æ ¡æ­£")
            print(f"ç•¸å˜ç³»æ•°: k1={self.dist[0]:.6f}, k2={self.dist[1]:.6f}, k3={self.dist[4]:.6f}")
        else:
            print("æœªæ£€æµ‹åˆ°ç•¸å˜ç³»æ•°ï¼Œè·³è¿‡ç•¸å˜æ ¡æ­£")
            self.mtx = None
            self.dist = None
        
        # åˆ›å»ºå¯¹é½å¯¹è±¡
        import pyrealsense2 as rs
        self.align = rs.align(rs.stream.color)
        
        # å¸§è®¡æ•°å™¨
        self.frame_count = 0
        self.start_time = time.time()
        
        # è®¡æ—¶å™¨
        self.timers = {
            'detection': [],
            'segmentation': [],
            'pointcloud_generation': [],
            'grasp_calculation': [],
            'robot_movement': [],
            'total_cycle': []
        }
        
        # åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µï¼ˆå¯é€‰ï¼‰
        self.hand_eye_transform = None  # 4x4 é½æ¬¡çŸ©é˜µï¼Œç›¸æœºåæ ‡â†’å¤¹çˆªåæ ‡
        if hand_eye_file is not None and os.path.exists(hand_eye_file):
            try:
                mat = np.load(hand_eye_file)
                if mat.shape == (4, 4):
                    self.hand_eye_transform = mat.astype(np.float32)
                    print("å·²åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µ (ç›¸æœºâ†’å¤¹çˆª):")
                    print(self.hand_eye_transform)
                else:
                    print(f"hand_eye_file æ ¼å¼ä¸æ­£ç¡®ï¼ŒæœŸæœ›(4,4)ï¼Œå®é™…{mat.shape}ï¼Œå¿½ç•¥ã€‚")
            except Exception as e:
                print(f"åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µå¤±è´¥: {e}")
        # è‹¥æœªåŠ è½½åˆ°ï¼Œåˆ™ä½¿ç”¨ç¡¬ç¼–ç çš„Rã€tï¼ˆç›¸æœºâ†’å¤¹çˆªï¼‰
        if self.hand_eye_transform is None:
            R_default = np.array([
                [-0.99791369, -0.06094636, -0.02130291],
                [ 0.06027516, -0.99770511,  0.03084494],
                [-0.02313391,  0.02949655,  0.99929714]
            ], dtype=np.float32)
            t_default = np.array([[0.04], [0.113], [-0.22081495]], dtype=np.float32)
            self.hand_eye_transform = np.eye(4, dtype=np.float32)
            self.hand_eye_transform[:3, :3] = R_default
            self.hand_eye_transform[:3, 3:4] = t_default
            print("ä½¿ç”¨ç¡¬ç¼–ç æ‰‹çœ¼æ ‡å®šçŸ©é˜µ (ç›¸æœºâ†’å¤¹çˆª):")
            print(self.hand_eye_transform)
        
        print("åˆå§‹åŒ–å®Œæˆï¼")


        import jkrc 
        self.robot = jkrc.RC("192.168.80.116")
        self.robot.login()   
    
    def time_step(self, step_name):
        """è®¡æ—¶å™¨è£…é¥°å™¨ï¼Œç”¨äºæµ‹é‡å„ä¸ªæ­¥éª¤çš„æ—¶é—´"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                start_time = time.time()
                result = func(*args, **kwargs)
                end_time = time.time()
                elapsed = end_time - start_time
                self.timers[step_name].append(elapsed)
                print(f"â±ï¸  {step_name}: {elapsed:.3f}s")
                return result
            return wrapper
        return decorator
    
    def print_timing_summary(self):
        """æ‰“å°æ—¶é—´ç»Ÿè®¡æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æ—¶é—´ç»Ÿè®¡æ‘˜è¦")
        print("="*60)
        
        for step_name, times in self.timers.items():
            if times:
                avg_time = np.mean(times)
                min_time = np.min(times)
                max_time = np.max(times)
                total_time = np.sum(times)
                print(f"{step_name:20s}: å¹³å‡={avg_time:.3f}s, æœ€å°={min_time:.3f}s, æœ€å¤§={max_time:.3f}s, æ€»è®¡={total_time:.3f}s")
            else:
                print(f"{step_name:20s}: æ— æ•°æ®")
        
        print("="*60)
    
    def capture_frames(self):
        """
        æ•è·RGBå’Œæ·±åº¦å¸§
        
        Returns:
            color_image: RGBå›¾åƒ
            depth_image: æ·±åº¦å›¾åƒ (æ¯«ç±³)
            success: æ˜¯å¦æˆåŠŸ
        """
        try:
            # ç­‰å¾…æ–°çš„å¸§
            frames = self.pipeline.wait_for_frames()
            
            # å¯¹é½æ·±åº¦å¸§åˆ°RGBå¸§
            aligned_frames = self.align.process(frames)
            
            # è·å–å¯¹é½åçš„å¸§
            color_frame = aligned_frames.get_color_frame()
            depth_frame = aligned_frames.get_depth_frame()
            
            if not color_frame or not depth_frame:
                return None, None, False
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆRealSenseé…ç½®ä¸ºbgr8ï¼Œå› æ­¤è¿™é‡Œç›´æ¥å¾—åˆ°BGRæ ¼å¼ï¼Œé€‚ç”¨äºOpenCVï¼‰
            color_image = np.asanyarray(color_frame.get_data())

            # è·å–æ·±åº¦æ•°æ®
            height, width = depth_frame.get_height(), depth_frame.get_width()
            depth_image = np.zeros((height, width), dtype=np.uint16)
            
            for y in range(height):
                for x in range(width):
                    dist = depth_frame.get_distance(x, y)
                    if dist > 0:
                        depth_image[y, x] = int(dist * 1000)  # è½¬æ¢ä¸ºæ¯«ç±³
            
            # å¦‚æœå¯ç”¨äº†ç•¸å˜æ ¡æ­£ï¼Œæ ¡æ­£å›¾åƒ
            # if self.mtx is not None and self.dist is not None:
            #     color_image = cv2.undistort(color_image, self.mtx, self.dist)
            #     # # æ·±åº¦å›¾åƒéœ€è¦è½¬æ¢ä¸ºfloat32ç±»å‹è¿›è¡Œç•¸å˜æ ¡æ­£
            #     # depth_image_float = depth_image.astype(np.float32)
            #     # depth_image_undistorted = cv2.undistort(depth_image_float, self.mtx, self.dist)
            #     # depth_image = depth_image_undistorted.astype(np.uint16)
            
            return color_image, depth_image, True
            
        except Exception as e:
            print(f"æ•è·å¸§æ—¶å‡ºé”™: {e}")
            return None, None, False
    
    def detect_and_segment_and_dump(self, color_image):
        """
        æœ¬åœ°å®Œæˆæ£€æµ‹->è½ç›˜->åˆ†å‰²->è½ç›˜ï¼Œè¿”å›ç”¨äºæ˜¾ç¤ºçš„å•é€šé“uint8æ©ç ï¼ˆ0/255ï¼‰ã€‚
        åªé€‰æ‹©ä¸€æ¡é±¼è¿›è¡Œåˆ†å‰²ï¼Œæ— æ£€æµ‹æ—¶è¿”å›Noneã€‚
        """
        # æ£€æµ‹ï¼ˆåªé€‰æ‹©ä¸€æ¡é±¼ï¼‰
        detection_start = time.time()
        if getattr(self, 'use_yolo', False):
            boxes = self.detect_yolo(color_image, self.yolo_weights, conf=0.25, iou=0.45, imgsz=640)
        else:
            boxes = self._detect_boxes(color_image)
        detection_time = time.time() - detection_start
        self.timers['detection'].append(detection_time)
        print(f"â±ï¸  detection: {detection_time:.3f}s")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        base_name = f"frame_{self.frame_count:06d}_{timestamp}"
        
        # ä¿å­˜æ£€æµ‹å¯è§†åŒ–ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹ï¼‰
        if self.debug:
            det_vis = color_image.copy()
            if len(boxes) > 0:
                # åªæ ‡è®°é€‰ä¸­çš„é±¼ï¼ˆç»¿è‰²æ¡†ï¼‰
                x1, y1, x2, y2, confidence = boxes[0]
                cv2.rectangle(det_vis, (x1, y1), (x2, y2), (0, 255, 0), 3)  # ç»¿è‰²ç²—æ¡†è¡¨ç¤ºé€‰ä¸­çš„é±¼
                cv2.putText(det_vis, f"SELECTED (conf: {confidence:.2f})", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
                # ä¿å­˜é€‰ä¸­çš„é±¼çš„è£å‰ªå›¾åƒ
                crop = color_image[y1:y2, x1:x2]
                if crop.size > 0:
                    cv2.imwrite(os.path.join(self.detection_dir, f"{base_name}_selected_fish.png"), crop)
                
                cv2.imwrite(os.path.join(self.detection_dir, f"{base_name}_dino_detection.png"), det_vis)

        if not boxes:
            print("æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼Œè·³è¿‡åˆ†å‰²ã€‚")
            return None, None

        # åˆ†å‰²ï¼ˆSAMï¼‰- åªå¤„ç†é€‰ä¸­çš„ä¸€æ¡é±¼
        segmentation_start = time.time()
        try:
            image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)
            self.sam_predictor.set_image(image_rgb)
            
            # åªä½¿ç”¨é€‰ä¸­çš„è¾¹ç•Œæ¡†
            x1, y1, x2, y2, confidence = boxes[0]
            boxes_tensor = torch.tensor([[x1, y1, x2, y2]], device=self.device)
            transformed_boxes = self.sam_predictor.transform.apply_boxes_torch(boxes_tensor, image_rgb.shape[:2])

            # ä½¿ç”¨multimask_output=Falseç¡®ä¿åªè¿”å›ä¸€ä¸ªæ©ç 
            masks, scores, logits = self.sam_predictor.predict_torch(
                point_coords=None,
                point_labels=None,
                boxes=transformed_boxes,
                multimask_output=False  # åªè¿”å›ä¸€ä¸ªæœ€ä½³æ©ç 
            )

            # å¤„ç†é€‰ä¸­çš„é±¼çš„æ©ç  - ç¡®ä¿åªä½¿ç”¨ç¬¬ä¸€ä¸ªæ©ç 
            if masks.shape[0] > 0 and masks.shape[1] > 0:
                # åªå–ç¬¬ä¸€ä¸ªæ©ç ï¼ˆç´¢å¼•[0][0]ï¼‰
                m_bool = masks[0][0].cpu().numpy().astype(np.uint8)
                mask_np = m_bool * 255
                
                # è¿›ä¸€æ­¥é™åˆ¶æ©ç åªåœ¨è¾¹ç•Œæ¡†åŒºåŸŸå†…
                # åˆ›å»ºä¸€ä¸ªå…¨é›¶æ©ç 
                restricted_mask = np.zeros_like(mask_np)
                # åªåœ¨è¾¹ç•Œæ¡†åŒºåŸŸå†…åº”ç”¨æ©ç 
                restricted_mask[y1:y2, x1:x2] = mask_np[y1:y2, x1:x2]
                mask_np = restricted_mask
                
                # ä¿å­˜æ©ç ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹ï¼‰
                if self.debug:
                    mask_path = os.path.join(self.segmentation_dir, f"{base_name}_selected_fish_mask.png")
                    cv2.imwrite(mask_path, mask_np)
                    
                    # ä¿å­˜è£å‰ªæ©ç 
                    mask_crop = mask_np[y1:y2, x1:x2]
                    if mask_crop.size > 0:
                        mask_crop_path = os.path.join(self.segmentation_dir, f"{base_name}_selected_fish_mask_crop.png")
                        cv2.imwrite(mask_crop_path, mask_crop)
                    
                    # ä¿å­˜è£å‰ªå¯è§†åŒ–
                    crop = color_image[y1:y2, x1:x2]
                    if crop.size > 0 and mask_crop.size > 0:
                        overlay = np.zeros_like(crop)
                        overlay[mask_crop > 0] = [0, 255, 0]
                        vis_crop = cv2.addWeighted(crop, 1.0, overlay, 0.4, 0)
                        vis_crop_path = os.path.join(self.segmentation_dir, f"{base_name}_selected_fish_vis.png")
                        cv2.imwrite(vis_crop_path, vis_crop)
                    
                    # ä¿å­˜æ•´ä½“å¯è§†åŒ–
                    colored = np.zeros_like(color_image)
                    colored[mask_np > 0] = [0, 255, 0]
                    vis = cv2.addWeighted(color_image, 1.0, colored, 0.4, 0)
                    vis_path = os.path.join(self.segmentation_dir, f"{base_name}_selected_fish_overlay.png")
                    cv2.imwrite(vis_path, vis)
                
                segmentation_time = time.time() - segmentation_start
                self.timers['segmentation'].append(segmentation_time)
                print(f"â±ï¸  segmentation: {segmentation_time:.3f}s")
                
                print(f"æˆåŠŸåˆ†å‰²é€‰ä¸­çš„é±¼ï¼Œæ©ç ç‚¹æ•°: {np.sum(mask_np > 0)}")
                print(f"æ©ç é™åˆ¶åœ¨è¾¹ç•Œæ¡†å†…: ({x1}, {y1}) åˆ° ({x2}, {y2})")
                return mask_np, base_name
            else:
                segmentation_time = time.time() - segmentation_start
                self.timers['segmentation'].append(segmentation_time)
                print(f"â±ï¸  segmentation: {segmentation_time:.3f}s")
                print("åˆ†å‰²å¤±è´¥ï¼Œæœªç”Ÿæˆæ©ç ")
                return None, None
                
        except Exception as e:
            segmentation_time = time.time() - segmentation_start
            self.timers['segmentation'].append(segmentation_time)
            print(f"â±ï¸  segmentation: {segmentation_time:.3f}s")
            print(f"åˆ†å‰²æ—¶å‡ºé”™: {e}")
            return None, None

    def _detect_boxes(self, color_image):
        """
        ä½¿ç”¨ä¸ seg.py ç›¸åŒçš„æ–¹å¼è¿›è¡Œæ£€æµ‹ï¼Œè¿”å›bboxåˆ—è¡¨
        åªé€‰æ‹©ä¸€æ¡é±¼è¿›è¡Œåˆ†å‰²å’ŒæŠ“å–
        """
        # è½¬æ¢ä¸ºPILå›¾åƒï¼ˆä¸ seg.py ä¸€è‡´ï¼‰
        image_pil = Image.fromarray(cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB))
        text_prompt = "fish. crab. marine animal"
        inputs = self.processor(images=image_pil, text=text_prompt, return_tensors="pt").to(self.device)
        with torch.no_grad():
            outputs = self.grounding_dino_model(**inputs)
        h, w = color_image.shape[0], color_image.shape[1]
        results = self.processor.post_process_grounded_object_detection(
            outputs,
            inputs.input_ids,
            text_threshold=0.3,
            # ä¸ seg.py ç›¸åŒçš„å°ºå¯¸ä¼ å…¥æ–¹å¼
            target_sizes=[image_pil.size[::-1]]
        )
        result = results[0]
        boxes = []
        print("\næ£€æµ‹ç»“æœè¯¦æƒ…:")
        print(f"æ£€æµ‹åˆ°çš„ç›®æ ‡æ•°é‡: {len(result['boxes'])}")
        if len(result["boxes"]) == 0:
            return boxes
        
        # è¿‡æ»¤è¾¹ç•Œæ¡†ï¼šé¢ç§¯å¿…é¡»å¤§äº1000åƒç´ 
        valid_boxes = []
        for box in result["boxes"]:
            x1, y1, x2, y2 = [int(c) for c in box.tolist()]
            x1 = max(0, min(x1, w - 1))
            y1 = max(0, min(y1, h - 1))
            x2 = max(0, min(x2, w - 1))
            y2 = max(0, min(y2, h - 1))
            
            # è®¡ç®—è¾¹ç•Œæ¡†é¢ç§¯
            area = (x2 - x1) * (y2 - y1)
            if area > 1000:  # é¢ç§¯è¿‡æ»¤
                valid_boxes.append(((x1, y1, x2, y2), area))
        
        if valid_boxes:
            # æ ¹æ®é€‰æ‹©ç­–ç•¥é€‰æ‹©è¾¹ç•Œæ¡†
            if self.bbox_selection == "smallest":
                selected_box = min(valid_boxes, key=lambda x: x[1])
                selection_type = "é¢ç§¯æœ€å°çš„"
            elif self.bbox_selection == "largest":
                selected_box = max(valid_boxes, key=lambda x: x[1])
                selection_type = "é¢ç§¯æœ€å¤§çš„"
            else:
                # é»˜è®¤é€‰æ‹©æœ€å°çš„
                selected_box = min(valid_boxes, key=lambda x: x[1])
                selection_type = "é¢ç§¯æœ€å°çš„"
                print(f"è­¦å‘Š: æœªçŸ¥çš„é€‰æ‹©ç­–ç•¥ '{self.bbox_selection}'ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥ 'smallest'")
            
            boxes.append(selected_box[0])
            print(f"æ£€æµ‹åˆ° {len(valid_boxes)} æ¡é±¼ï¼Œé€‰æ‹©{selection_type}è¿›è¡ŒæŠ“å–ï¼Œé¢ç§¯: {selected_box[1]} åƒç´ ")
            print(f"é€‰æ‹©çš„é±¼ä½ç½®: {selected_box[0]}")
        else:
            print("æ²¡æœ‰æ»¡è¶³é¢ç§¯è¦æ±‚çš„è¾¹ç•Œæ¡†")
        
        return boxes

    def detect_yolo(self, color_image, yolo_weights_path, conf=0.5, iou=0.45, imgsz=640, min_area=1000):
        """
        ä½¿ç”¨Ultralytics YOLOè¿›è¡Œé±¼çš„æ£€æµ‹ï¼Œè¿”å›æ‰€æœ‰æ£€æµ‹åˆ°çš„bboxã€‚
        
        Args:
            color_image: OpenCV BGRå›¾åƒ (H,W,3)
            yolo_weights_path: è®­ç»ƒå¥½çš„YOLOæƒé‡ .pt è·¯å¾„
            conf: ç½®ä¿¡åº¦é˜ˆå€¼
            iou: NMS IOU é˜ˆå€¼
            imgsz: æ¨ç†è¾“å…¥å°ºå¯¸
            min_area: è¿‡æ»¤æœ€å°é¢ç§¯ï¼ˆåƒç´ ï¼‰
        
        Returns:
            boxes: List[Tuple[x1, y1, x2, y2, confidence]] æ‰€æœ‰æ»¡è¶³æ¡ä»¶çš„bboxï¼›è‹¥æ— åˆ™è¿”å›ç©ºåˆ—è¡¨
        """
        try:
            from ultralytics import YOLO
        except Exception as e:
            print("[é”™è¯¯] æœªæ‰¾åˆ° ultralyticsï¼Œè¯·å…ˆ: pip install ultralytics")
            print(e)
            return []

        # åŠ è½½æ¨¡å‹ï¼ˆæ¯æ¬¡è°ƒç”¨åŠ è½½é¿å…ä¸å…¶ä»–ä¾èµ–å†²çªï¼›è‹¥é¢‘ç¹è°ƒç”¨å¯å¤–éƒ¨ç¼“å­˜æ¨¡å‹å®ä¾‹ï¼‰
        try:
            model = YOLO(yolo_weights_path)
        except Exception as e:
            print(f"[é”™è¯¯] åŠ è½½YOLOæƒé‡å¤±è´¥: {yolo_weights_path} -> {e}")
            return []

        # YOLOæ”¯æŒç›´æ¥ä¼ å…¥numpyå›¾åƒï¼›ç¡®ä¿ä¸ºRGB
        #image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)
        try:
            results = model.predict(
                source=[color_image],
                imgsz=imgsz,
                conf=conf,
                iou=iou,
                verbose=False,
                save=False
            )
        except Exception as e:
            print(f"[é”™è¯¯] YOLO æ¨ç†å¤±è´¥: {e}")
            return []

        if not results:
            return []

        res = results[0]
        boxes_np = None
        confidences = None
        try:
            # xyxy (N,4) å’Œ conf (N,)
            boxes_np = res.boxes.xyxy.cpu().numpy() if hasattr(res, 'boxes') and res.boxes is not None else None
            confidences = res.boxes.conf.cpu().numpy() if hasattr(res, 'boxes') and res.boxes is not None else None
        except Exception:
            boxes_np = None
            confidences = None

        if boxes_np is None or len(boxes_np) == 0:
            return []

        # è¿‡æ»¤é¢ç§¯ã€è£å‰ªåˆ°å›¾åƒèŒƒå›´ï¼ŒåŒæ—¶ä¿å­˜ç½®ä¿¡åº¦ä¿¡æ¯
        H, W = color_image.shape[0], color_image.shape[1]
        valid_boxes = []
        for i, xyxy in enumerate(boxes_np):
            x1, y1, x2, y2 = [int(round(v)) for v in xyxy[:4].tolist()]
            x1 = max(0, min(x1, W - 1))
            y1 = max(0, min(y1, H - 1))
            x2 = max(0, min(x2, W - 1))
            y2 = max(0, min(y2, H - 1))
            area = max(0, x2 - x1) * max(0, y2 - y1)
            if area > min_area:
                confidence = confidences[i] if confidences is not None else 0.0
                valid_boxes.append(((x1, y1, x2, y2), area, confidence))

        boxes = []
        if valid_boxes:
            # è¿”å›æ‰€æœ‰æ£€æµ‹åˆ°çš„bboxï¼ŒåŒ…å«ç½®ä¿¡åº¦ä¿¡æ¯
            for bbox_info in valid_boxes:
                bbox_coords, area, confidence = bbox_info
                # è¿”å›æ ¼å¼: (x1, y1, x2, y2, confidence)
                boxes.append((*bbox_coords, confidence))
            
            print(f"[YOLO] æ£€æµ‹åˆ° {len(valid_boxes)} ä¸ªå€™é€‰æ¡†ï¼Œå…¨éƒ¨è¿”å›ç”¨äºå¤„ç†")
            for i, (bbox_coords, area, confidence) in enumerate(valid_boxes):
                print(f"[YOLO] æ¡† {i+1}: {bbox_coords}, ç½®ä¿¡åº¦: {confidence:.3f}, é¢ç§¯: {area} åƒç´ ")
        else:
            print("[YOLO] æ²¡æœ‰æ»¡è¶³é¢ç§¯è¦æ±‚çš„è¾¹ç•Œæ¡†")

        return boxes
    
    def detect_yolo_all(self, color_image, yolo_weights_path, conf=0.5, iou=0.45, imgsz=640):
        """
        ä½¿ç”¨Ultralytics YOLOå¯¹å•å¸§è¿›è¡Œæ¨ç†ï¼Œè¿”å›æ‰€æœ‰æ£€æµ‹åˆ°çš„bboxï¼ˆä¸åšé¢ç§¯è¿‡æ»¤ä¸å•æ¡†é€‰æ‹©ï¼‰ã€‚
        è¿”å›ï¼šList[Tuple[int,int,int,int,float,int]] -> (x1,y1,x2,y2,conf,cls)
        """
        try:
            from ultralytics import YOLO
        except Exception as e:
            print("[é”™è¯¯] æœªæ‰¾åˆ° ultralyticsï¼Œè¯·å…ˆ: pip install ultralytics")
            print(e)
            return []

        # åŠ è½½æ¨¡å‹ï¼ˆç®€åŒ–ä¸ºæ¯æ¬¡åŠ è½½ï¼›å¦‚éœ€ä¼˜åŒ–å¯åœ¨å¤–éƒ¨ç¼“å­˜ï¼‰
        try:
            model = YOLO(yolo_weights_path)
        except Exception as e:
            print(f"[é”™è¯¯] åŠ è½½YOLOæƒé‡å¤±è´¥: {yolo_weights_path} -> {e}")
            return []

        # BGR -> RGB
        #image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)
        try:
            results = model.predict(
                source=[color_image],
                imgsz=imgsz,
                conf=conf,
                iou=iou,
                device=(0 if self.device == 'cuda' else 'cpu'),
                verbose=False,
                save=False,
            )
        except Exception as e:
            print(f"[é”™è¯¯] YOLO æ¨ç†å¤±è´¥: {e}")
            return []

        if not results:
            print("[YOLO] æ— æ£€æµ‹ç»“æœ")
            return []

        res = results[0]
        if not hasattr(res, 'boxes') or res.boxes is None or res.boxes.shape[0] == 0:
            print("[YOLO] boxes ä¸ºç©º")
            return []

        xyxy = res.boxes.xyxy.cpu().numpy()  # (N,4)
        conf_arr = res.boxes.conf.cpu().numpy() if hasattr(res.boxes, 'conf') else None
        cls_arr = res.boxes.cls.cpu().numpy() if hasattr(res.boxes, 'cls') else None

        all_boxes = []
        for i, b in enumerate(xyxy):
            x1, y1, x2, y2 = [int(round(v)) for v in b[:4].tolist()]
            conf_v = float(conf_arr[i]) if conf_arr is not None else 0.0
            cls_v = int(cls_arr[i]) if cls_arr is not None else -1
            all_boxes.append((x1, y1, x2, y2, conf_v, cls_v))

        print(f"[YOLO] æ£€æµ‹åˆ° {len(all_boxes)} ä¸ªæ¡†ï¼ˆconf>={conf}ï¼‰ï¼šå‰3ä¸ª: {all_boxes[:3]}")
        return all_boxes

    def dump_detections(self, color_image):
        """
        å°†æ£€æµ‹åˆ°çš„ç›®æ ‡è£å‰ªå¹¶ä¿å­˜åˆ° detection/ ç›®å½•
        """
        boxes = self._detect_boxes(color_image)
        if not boxes:
            return 0
        base_ts = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        saved = 0
        for idx, (x1, y1, x2, y2) in enumerate(boxes):
            crop = color_image[y1:y2, x1:x2]
            if crop.size == 0:
                continue
            filename = f"frame_{self.frame_count:06d}_{base_ts}_det_{idx}.png"
            path = os.path.join(self.detection_dir, filename)
            cv2.imwrite(path, crop)
            saved += 1
        if saved:
            print(f"å·²ä¿å­˜ {saved} ä¸ªæ£€æµ‹è£å‰ªåˆ°: {self.detection_dir}")
        return saved

    
    def generate_pointcloud(self, color_image, depth_image, mask):
        """
        ä»æ©ç ç”Ÿæˆ3Dç‚¹äº‘
        
        Args:
            color_image: RGBå›¾åƒ
            depth_image: æ·±åº¦å›¾åƒ (æ¯«ç±³)
            mask: åˆ†å‰²æ©ç 
            
        Returns:
            points: 3Dç‚¹åæ ‡
            colors: RGBé¢œè‰²
        """
        try:
            # è½¬æ¢æ·±åº¦å›¾åƒå•ä½ä¸ºç±³
            depth_image_meters = depth_image.astype(np.float32) / 1000.0
            
            # è½¬æ¢ä¸ºRGBæ ¼å¼
            color_image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)
            
            # ä½¿ç”¨mask_to_3d_pointcloudå‡½æ•°ï¼ˆæ”¯æŒç•¸å˜æ ¡æ­£ï¼‰
            points, colors = mask_to_3d_pointcloud(
                color_image_rgb, 
                depth_image_meters, 
                mask, 
                self.fx, self.fy, self.cx, self.cy,
                self.mtx, self.dist
            )
            
            return points, colors
            
        except Exception as e:
            print(f"ç”Ÿæˆç‚¹äº‘æ—¶å‡ºé”™: {e}")
            return np.array([]), np.array([])

    def apply_hand_eye_transform(self, points):
        """
        å°†ç‚¹äº‘ä»ç›¸æœºç³»è½¬æ¢åˆ°å¤¹çˆªç³»ï¼Œä½¿ç”¨ self.hand_eye_transform (4x4)ã€‚
        æ—‹è½¬çŸ©é˜µï¼š
            [[-0.99462885  0.07149648  0.07484454]
            [-0.06962775 -0.9971997   0.02728984]
            [ 0.07658608  0.021932    0.99682173]]
            å¹³ç§»å‘é‡ï¼š
            [[ 0.0247092 ]
            [ 0.09912939]
            [-0.25357213]]
        """
        if self.hand_eye_transform is None or points.size == 0:
            return points
        ones = np.ones((points.shape[0], 1), dtype=np.float32)
        homo = np.hstack([points.astype(np.float32), ones])  # (N,4)
        transformed = (self.hand_eye_transform @ homo.T).T  # (N,4)
        return transformed[:, :3]

    def _rpy_to_rotation_matrix(self, rx, ry, rz):
        """
        å°†æœ«ç«¯çš„ RPY (rx, ry, rz) è½¬ä¸ºæ—‹è½¬çŸ©é˜µ R (åŸºåº§â†’æœ«ç«¯)ã€‚
        é‡‡ç”¨å¸¸è§çš„å¤–æ—‹é¡ºåº R = Rz @ Ry @ Rxã€‚
        """
        sx, cx = np.sin(rx), np.cos(rx)
        sy, cy = np.sin(ry), np.cos(ry)
        sz, cz = np.sin(rz), np.cos(rz)

        Rx = np.array([[1, 0, 0],
                       [0, cx, -sx],
                       [0, sx,  cx]], dtype=np.float32)
        Ry = np.array([[ cy, 0, sy],
                       [  0, 1,  0],
                       [-sy, 0, cy]], dtype=np.float32)
        Rz = np.array([[cz, -sz, 0],
                       [sz,  cz, 0],
                       [ 0,   0, 1]], dtype=np.float32)

        return (Rz @ Ry @ Rx).astype(np.float32)

    def _tool_offset_to_base(self, delta_tool_xyz_mm, tcp_rpy):
        """
        å°†å¤¹çˆª(å·¥å…·)åæ ‡ç³»ä¸‹çš„ä½ç§»(mm)è½¬æ¢åˆ°åŸºåæ ‡ç³»ä¸‹çš„ä½ç§»(mm)ã€‚
        delta_tool_xyz_mm: [dx, dy, dz] in tool frame
        tcp_rpy: [rx, ry, rz] in radians
        è¿”å›: [dx_base, dy_base, dz_base]
        """
        rx, ry, rz = tcp_rpy
        R_base_tool = self._rpy_to_rotation_matrix(rx, ry, rz)
        delta_tool = np.asarray(delta_tool_xyz_mm, dtype=np.float32).reshape(3, 1)
        delta_base = (R_base_tool @ delta_tool).reshape(3)
        return delta_base.tolist()

    def calculate_pointcloud_bbox(self, points):
        """
        è®¡ç®—ç‚¹äº‘çš„è¾¹ç•Œæ¡†ä¿¡æ¯ï¼Œç”¨äºé«˜åº¦å’Œå§¿æ€ä¼°è®¡
        
        Args:
            points: ç‚¹äº‘åæ ‡ (N, 3)
            
        Returns:
            bbox_info: å­—å…¸åŒ…å«ä¸­å¿ƒç‚¹ã€å°ºå¯¸ã€è¾¹ç•Œæ¡†ç­‰
        """
        if points.size == 0:
            return None
            
        # è®¡ç®—è¾¹ç•Œæ¡†
        min_coords = np.min(points, axis=0)  # [min_x, min_y, min_z]
        max_coords = np.max(points, axis=0)  # [max_x, max_y, max_z]
        
        # è®¡ç®—ä¸­å¿ƒç‚¹
        center = (min_coords + max_coords) / 2.0  # [center_x, center_y, center_z]
        
        # è®¡ç®—å°ºå¯¸
        dimensions = max_coords - min_coords  # [width, height, depth]
        
        # è®¡ç®—é«˜åº¦ï¼ˆzæ–¹å‘ï¼‰
        height = dimensions[2]  # zæ–¹å‘çš„é«˜åº¦
        
        # è®¡ç®—8ä¸ªè§’ç‚¹
        corners = []
        for x in [min_coords[0], max_coords[0]]:
            for y in [min_coords[1], max_coords[1]]:
                for z in [min_coords[2], max_coords[2]]:
                    corners.append([x, y, z])
        corners = np.array(corners)
        
        # è®¡ç®—ç‚¹äº‘çš„ä¸»æ–¹å‘ï¼ˆPCAï¼‰
        try:
            from sklearn.decomposition import PCA
            pca = PCA(n_components=3)
            pca.fit(points)
            principal_axes = pca.components_  # ä¸»æ–¹å‘å‘é‡
            explained_variance = pca.explained_variance_ratio_  # è§£é‡Šæ–¹å·®æ¯”ä¾‹
        except ImportError:
            print("sklearnæœªå®‰è£…ï¼Œè·³è¿‡PCAå§¿æ€ä¼°è®¡")
            principal_axes = np.eye(3)
            explained_variance = [1.0, 0.0, 0.0]
        
        bbox_info = {
            'center': center,
            'dimensions': dimensions,
            'height': height,
            'min_coords': min_coords,
            'max_coords': max_coords,
            'corners': corners,
            'principal_axes': principal_axes,
            'explained_variance': explained_variance,
            'num_points': len(points)
        }
        
        return bbox_info

    def calculate_surface_normal(self, points, method='pca'):
        """
        è®¡ç®—ç‚¹äº‘è´¨å¿ƒå¤„çš„è¡¨é¢æ³•å‘é‡
        
        Args:
            points: ç‚¹äº‘åæ ‡ (N, 3)
            method: æ³•å‘é‡è®¡ç®—æ–¹æ³• ('pca', 'plane_fitting', 'nearest_neighbors')
            
        Returns:
            normal: æ³•å‘é‡ (3,) å•ä½å‘é‡
            centroid: è´¨å¿ƒåæ ‡ (3,)
        """
        if points.size == 0 or len(points) < 3:
            return np.array([0, 0, 1]), np.array([0, 0, 0])
        
        centroid = np.mean(points, axis=0)
        
        if method == 'pca':
            # ä½¿ç”¨PCAè®¡ç®—æ³•å‘é‡
            try:
                from sklearn.decomposition import PCA
                pca = PCA(n_components=3)
                pca.fit(points)
                # æœ€å°ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡å°±æ˜¯æ³•å‘é‡
                normal = pca.components_[2]  # ç¬¬ä¸‰ä¸ªä¸»æˆåˆ†ï¼ˆæœ€å°æ–¹å·®æ–¹å‘ï¼‰
            except ImportError:
                print("sklearnæœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•å¹³é¢æ‹Ÿåˆ")
                return self._simple_plane_fitting(points, centroid)
        
        elif method == 'plane_fitting':
            return self._simple_plane_fitting(points, centroid)
        
        elif method == 'nearest_neighbors':
            return self._nearest_neighbors_normal(points, centroid)
        
        else:
            raise ValueError(f"æœªçŸ¥çš„æ³•å‘é‡è®¡ç®—æ–¹æ³•: {method}")
        
        # ç¡®ä¿æ³•å‘é‡æŒ‡å‘æ­£ç¡®çš„æ–¹å‘ï¼ˆé€šå¸¸æŒ‡å‘ç›¸æœºæ–¹å‘ï¼‰
        # å¦‚æœæ³•å‘é‡çš„zåˆ†é‡ä¸ºè´Ÿï¼Œåˆ™ç¿»è½¬æ–¹å‘
        if normal[2] < 0:
            normal = -normal
        
        # å½’ä¸€åŒ–
        normal = normal / np.linalg.norm(normal)
        
        return normal, centroid

    def _simple_plane_fitting(self, points, centroid):
        """
        ä½¿ç”¨ç®€å•å¹³é¢æ‹Ÿåˆè®¡ç®—æ³•å‘é‡
        """
        # å°†ç‚¹äº‘ä¸­å¿ƒåŒ–
        centered_points = points - centroid
        
        # æ„å»ºåæ–¹å·®çŸ©é˜µ
        cov_matrix = np.cov(centered_points.T)
        
        # è®¡ç®—ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
        
        # æœ€å°ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡å°±æ˜¯æ³•å‘é‡
        normal = eigenvectors[:, 0]  # æœ€å°ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡
        
        # ç¡®ä¿æ³•å‘é‡æŒ‡å‘æ­£ç¡®çš„æ–¹å‘
        if normal[2] < 0:
            normal = -normal
        
        # å½’ä¸€åŒ–
        normal = normal / np.linalg.norm(normal)
        
        return normal, centroid

    def _nearest_neighbors_normal(self, points, centroid, k=20):
        """
        ä½¿ç”¨æœ€è¿‘é‚»æ–¹æ³•è®¡ç®—æ³•å‘é‡
        """
        # è®¡ç®—æ¯ä¸ªç‚¹åˆ°è´¨å¿ƒçš„è·ç¦»
        distances = np.linalg.norm(points - centroid, axis=1)
        
        # æ‰¾åˆ°æœ€è¿‘çš„kä¸ªç‚¹
        nearest_indices = np.argsort(distances)[:k]
        nearest_points = points[nearest_indices]
        
        # ä½¿ç”¨è¿™äº›æœ€è¿‘é‚»ç‚¹è¿›è¡Œå¹³é¢æ‹Ÿåˆ
        return self._simple_plane_fitting(nearest_points, centroid)

    def normal_to_rpy(self, normal_vector, current_rpy=None):
        """
        å°†æ³•å‘é‡è½¬æ¢ä¸ºæœºå™¨äººæœ«ç«¯å§¿æ€çš„RPYè§’åº¦
        
        Args:
            normal_vector: æ³•å‘é‡ (3,) å•ä½å‘é‡ï¼Œè¡¨ç¤ºæœŸæœ›çš„Zè½´æ–¹å‘
            current_rpy: å½“å‰RPYè§’åº¦ [rx, ry, rz] (å¯é€‰ï¼Œç”¨äºå¹³æ»‘è¿‡æ¸¡)
            
        Returns:
            target_rpy: ç›®æ ‡RPYè§’åº¦ [rx, ry, rz]
        """
        # æœŸæœ›çš„Zè½´æ–¹å‘ï¼ˆæ³•å‘é‡ï¼‰
        z_target = normal_vector / np.linalg.norm(normal_vector)
        
        # å®šä¹‰å‚è€ƒåæ ‡ç³»ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
        # è¿™é‡Œå‡è®¾Xè½´æŒ‡å‘æœºå™¨äººå‰æ–¹ï¼ŒYè½´æŒ‡å‘æœºå™¨äººå·¦ä¾§
        x_ref = np.array([1, 0, 0])  # å‚è€ƒXè½´
        y_ref = np.array([0, 1, 0])  # å‚è€ƒYè½´
        
        # è®¡ç®—æ–°çš„åæ ‡ç³»
        # Zè½´ = æ³•å‘é‡
        z_new = z_target
        
        # Xè½´ = å‚è€ƒXè½´åœ¨å‚ç›´äºZè½´çš„å¹³é¢ä¸Šçš„æŠ•å½±
        x_new = x_ref - np.dot(x_ref, z_new) * z_new
        x_new = x_new / np.linalg.norm(x_new)
        
        # Yè½´ = Zè½´ Ã— Xè½´
        y_new = np.cross(z_new, x_new)
        y_new = y_new / np.linalg.norm(y_new)
        
        # æ„å»ºæ—‹è½¬çŸ©é˜µ
        R = np.column_stack([x_new, y_new, z_new])
        
        # å°†æ—‹è½¬çŸ©é˜µè½¬æ¢ä¸ºRPYè§’åº¦
        # ä½¿ç”¨ZYXæ¬§æ‹‰è§’é¡ºåºï¼ˆRoll-Pitch-Yawï¼‰
        sy = np.sqrt(R[0, 0] * R[0, 0] + R[1, 0] * R[1, 0])
        
        singular = sy < 1e-6
        
        if not singular:
            rx = np.arctan2(R[2, 1], R[2, 2])  # Roll
            ry = np.arctan2(-R[2, 0], sy)      # Pitch
            rz = np.arctan2(R[1, 0], R[0, 0])  # Yaw
        else:
            rx = np.arctan2(-R[1, 2], R[1, 1])  # Roll
            ry = np.arctan2(-R[2, 0], sy)       # Pitch
            rz = 0                               # Yaw
        
        target_rpy = np.array([rx, ry, rz])
        
        # å¦‚æœæä¾›äº†å½“å‰RPYï¼Œè¿›è¡Œå¹³æ»‘è¿‡æ¸¡
        if current_rpy is not None:
            target_rpy = self._smooth_rpy_transition(current_rpy, target_rpy)
        
        return target_rpy

    def _smooth_rpy_transition(self, current_rpy, target_rpy, max_change=0.1):
        """
        å¹³æ»‘RPYè§’åº¦è¿‡æ¸¡ï¼Œé¿å…çªå˜
        
        Args:
            current_rpy: å½“å‰RPYè§’åº¦
            target_rpy: ç›®æ ‡RPYè§’åº¦
            max_change: å•æ¬¡æœ€å¤§å˜åŒ–é‡ï¼ˆå¼§åº¦ï¼‰
            
        Returns:
            smoothed_rpy: å¹³æ»‘åçš„RPYè§’åº¦
        """
        current_rpy = np.array(current_rpy)
        target_rpy = np.array(target_rpy)
        
        # è®¡ç®—è§’åº¦å·®
        diff = target_rpy - current_rpy
        
        # å¤„ç†è§’åº¦è·³è·ƒï¼ˆÂ±Ï€ï¼‰
        for i in range(3):
            if diff[i] > np.pi:
                diff[i] -= 2 * np.pi
            elif diff[i] < -np.pi:
                diff[i] += 2 * np.pi
        
        # é™åˆ¶å˜åŒ–é‡
        for i in range(3):
            if abs(diff[i]) > max_change:
                diff[i] = np.sign(diff[i]) * max_change
        
        # è®¡ç®—å¹³æ»‘åçš„è§’åº¦
        smoothed_rpy = current_rpy + diff
        
        return smoothed_rpy

    def calculate_grasp_pose_with_normal(self, points_gripper, current_tcp):
        """
        è®¡ç®—è€ƒè™‘æ³•å‘é‡çš„æŠ“å–å§¿æ€
        
        Args:
            points_gripper: å¤¹çˆªåæ ‡ç³»ä¸­çš„ç‚¹äº‘ (N, 3)
            current_tcp: å½“å‰TCPä½ç½® [x, y, z, rx, ry, rz]
            
        Returns:
            grasp_pose: æŠ“å–å§¿æ€ [x, y, z, rx, ry, rz]
            normal_info: æ³•å‘é‡ä¿¡æ¯å­—å…¸
        """
        if points_gripper.size == 0 or len(points_gripper) < 3:
            print("ç‚¹äº‘ç‚¹æ•°ä¸è¶³ï¼Œæ— æ³•è®¡ç®—æ³•å‘é‡")
            return current_tcp, None
        
        # è®¡ç®—è´¨å¿ƒå’Œæ³•å‘é‡
        normal, centroid = self.calculate_surface_normal(points_gripper, method='pca')
        
        print(f"è´¨å¿ƒåæ ‡: {centroid}")
        print(f"æ³•å‘é‡: {normal}")
        
        # å°†æ³•å‘é‡è½¬æ¢ä¸ºRPYè§’åº¦
        current_rpy = current_tcp[3:6]
        target_rpy = self.normal_to_rpy(normal, current_rpy)
        
        print(f"å½“å‰RPY: {np.degrees(current_rpy)} åº¦")
        print(f"ç›®æ ‡RPY: {np.degrees(target_rpy)} åº¦")
        
        # æ„å»ºæŠ“å–å§¿æ€
        grasp_pose = np.array([
            centroid[0] * 1000,  # è½¬æ¢ä¸ºæ¯«ç±³
            centroid[1] * 1000,
            centroid[2] * 1000,
            target_rpy[0],       # ä¿æŒå¼§åº¦
            target_rpy[1],
            target_rpy[2]
        ])
        
        # æ³•å‘é‡ä¿¡æ¯
        normal_info = {
            'centroid': centroid,
            'normal': normal,
            'current_rpy': current_rpy,
            'target_rpy': target_rpy,
            'rpy_change': target_rpy - current_rpy
        }
        
        return grasp_pose, normal_info
    
    def save_results(self, color_image, depth_image, mask, points, colors):
        """
        ä¿å­˜æ‰€æœ‰ç»“æœ
        
        Args:
            color_image: RGBå›¾åƒ
            depth_image: æ·±åº¦å›¾åƒ
            mask: åˆ†å‰²æ©ç 
            points: 3Dç‚¹åæ ‡
            colors: RGBé¢œè‰²
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        base_name = f"frame_{self.frame_count:06d}_{timestamp}"
        
        # ä¿å­˜RGBå›¾åƒ
        rgb_path = os.path.join(self.rgb_dir, f"{base_name}.png")
        cv2.imwrite(rgb_path, color_image)
        
        # ä¿å­˜æ·±åº¦å›¾åƒ
        depth_path = os.path.join(self.depth_dir, f"{base_name}.png")
        cv2.imwrite(depth_path, depth_image.astype(np.uint16))
        
        # ä¿å­˜æ©ç 
        if mask is not None:
            mask_path = os.path.join(self.mask_dir, f"{base_name}_mask.png")
            cv2.imwrite(mask_path, mask.astype(np.uint8) * 255)
            
            # åˆ›å»ºå¯è§†åŒ–ç»“æœ
            colored_mask = np.zeros_like(color_image)
            colored_mask[mask] = [0, 255, 0]  # ç»¿è‰²æ©ç 
            alpha = 0.5
            visualization = cv2.addWeighted(color_image, 1, colored_mask, alpha, 0)
            vis_path = os.path.join(self.segmentation_dir, f"{base_name}_vis.png")
            cv2.imwrite(vis_path, visualization)
        
        # ä¿å­˜ç‚¹äº‘
        if self.save_pointcloud and len(points) > 0:
            pointcloud_path = os.path.join(self.pointcloud_dir, f"{base_name}_pointcloud.ply")
            save_pointcloud_to_file(points, colors, pointcloud_path)
        
        print(f"å·²ä¿å­˜ç¬¬ {self.frame_count} å¸§ç»“æœ")
    
    def show_preview(self, color_image, depth_image, mask):
        """
        åœ¨ä¸€ä¸ªçª—å£ä¸­æ˜¾ç¤ºRGBã€æ·±åº¦å›¾å’Œåˆ†å‰²ç»“æœ
        """
        # åˆ›å»ºæ·±åº¦å¯è§†åŒ–
        valid_depth = depth_image > 0
        if valid_depth.any():
            depth_min = depth_image[valid_depth].min()
            depth_max = depth_image[valid_depth].max()
            depth_normalized = np.zeros_like(depth_image, dtype=np.uint8)
            if depth_max > depth_min:
                depth_normalized[valid_depth] = ((depth_image[valid_depth] - depth_min) / (depth_max - depth_min) * 255).astype(np.uint8)
            depth_colormap = cv2.applyColorMap(depth_normalized, cv2.COLORMAP_JET)
        else:
            depth_colormap = np.zeros((depth_image.shape[0], depth_image.shape[1], 3), dtype=np.uint8)
        
        # åˆ›å»ºåˆ†å‰²å¯è§†åŒ–
        if mask is not None:
            # å°†æ©ç è½¬æ¢ä¸ºå½©è‰²å›¾åƒ
            mask_colored = np.zeros_like(color_image)
            mask_colored[mask > 0] = [0, 255, 0]  # ç»¿è‰²æ©ç 
            # å åŠ åˆ°åŸå›¾ä¸Š
            segmentation_vis = cv2.addWeighted(color_image, 0.7, mask_colored, 0.3, 0)
        else:
            segmentation_vis = color_image.copy()
        
        # è°ƒæ•´å›¾åƒå¤§å°
        display_size = (400, 300)
        color_display = cv2.resize(color_image, display_size)
        depth_display = cv2.resize(depth_colormap, display_size)
        seg_display = cv2.resize(segmentation_vis, display_size)
        
        # æ°´å¹³æ‹¼æ¥ä¸‰ä¸ªå›¾åƒ
        combined = np.hstack((color_display, depth_display, seg_display))
        
        # æ·»åŠ æ ‡ç­¾
        cv2.putText(combined, "RGB", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        cv2.putText(combined, "Depth", (410, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        cv2.putText(combined, "Segmentation", (810, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        cv2.putText(combined, f"Frame: {self.frame_count}", (10, combined.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        cv2.imshow('RGB | Depth | Segmentation', combined)

    
    def run_realtime(self, max_frames=None, show_preview=True):
        """
        è¿è¡Œå®æ—¶å¤„ç†
        
        Args:
            max_frames: æœ€å¤§å¸§æ•° (Noneè¡¨ç¤ºæ— é™)
            show_preview: æ˜¯å¦æ˜¾ç¤ºé¢„è§ˆçª—å£
        """
        print("å¼€å§‹å®æ—¶å¤„ç†...")
        print("æŒ‰ 'q' é”®åœæ­¢")
        

        tcp_result = self.robot.get_tcp_position()
        if isinstance(tcp_result, tuple) and len(tcp_result) == 2:
            tcp_ok, original_tcp = tcp_result
        else:
            # å¦‚æœåªè¿”å›ä¸€ä¸ªå€¼ï¼Œå‡è®¾å®ƒæ˜¯ä½ç½®ä¿¡æ¯
            original_tcp = tcp_result
            tcp_ok = True

        try:
            while True:
                # æ•´ä¸ªå¾ªç¯è®¡æ—¶å¼€å§‹
                cycle_start = time.time()
                
                # æ•è·å¸§
                color_image, depth_image, success = self.capture_frames()
                if not success:
                    continue
                
                self.frame_count = self.frame_count + 1
                # è·³è¿‡å‰3å¸§ï¼Œè®©ç›¸æœºç¨³å®š
                # if self.frame_count < 10:
                #     print(f"è·³è¿‡ç¬¬ {self.frame_count + 1} å¸§ï¼Œç­‰å¾…ç›¸æœºç¨³å®š...")
                #     #self.frame_count += 1
                #     continue

                # æ£€æµ‹ + åˆ†å‰² + è½ç›˜
                mask_vis, base_name = self.detect_and_segment_and_dump(color_image)
                
                # ä¿å­˜RGBå’Œæ·±åº¦å›¾åƒï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹ï¼‰
                if self.debug and base_name is not None:
                    # ä¿å­˜RGBå›¾åƒ
                    rgb_path = os.path.join(self.rgb_dir, f"{base_name}.png")
                    cv2.imwrite(rgb_path, color_image)
                    
                    # ä¿å­˜æ·±åº¦å›¾åƒï¼ˆåŸå§‹16ä½ï¼‰
                    depth_path = os.path.join(self.depth_dir, f"{base_name}.png")
                    cv2.imwrite(depth_path, depth_image.astype(np.uint16))
                    
                    # ä¿å­˜å¯è§†åŒ–æ·±åº¦å›¾åƒï¼ˆ8ä½å½©è‰²ï¼‰
                    valid_depth = depth_image > 0
                    if valid_depth.any():
                        depth_min = depth_image[valid_depth].min()
                        depth_max = depth_image[valid_depth].max()
                        depth_normalized = np.zeros_like(depth_image, dtype=np.uint8)
                        if depth_max > depth_min:
                            depth_normalized[valid_depth] = ((depth_image[valid_depth] - depth_min) / (depth_max - depth_min) * 255).astype(np.uint8)
                        depth_colormap = cv2.applyColorMap(depth_normalized, cv2.COLORMAP_JET)
                        depth_vis_path = os.path.join(self.depth_dir, f"{base_name}_visualization.png")
                        cv2.imwrite(depth_vis_path, depth_colormap)

                # æ˜¾ç¤ºé¢„è§ˆçª—å£
                self.show_preview(color_image, depth_image, mask_vis)
                
                # ç¡®ä¿çª—å£æ˜¾ç¤ºå¹¶å¤„ç†æŒ‰é”®
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("ç”¨æˆ·æŒ‰ 'q' é”®åœæ­¢")
                    break

                # æ ¹æ®æ©ç ç”Ÿæˆ3Dç‚¹äº‘å¹¶ä¿å­˜ï¼ˆå¯é€‰åº”ç”¨æ‰‹çœ¼æ ‡å®šï¼‰
                points_gripper = None  # åˆå§‹åŒ–å˜é‡

                #import pdb; pdb.set_trace()
                if mask_vis is not None and base_name is not None:
                    # ç‚¹äº‘ç”Ÿæˆè®¡æ—¶
                    pointcloud_start = time.time()
                    mask_bool = (mask_vis > 0)
                    points, colors = self.generate_pointcloud(color_image, depth_image, mask_bool)
                    pointcloud_time = time.time() - pointcloud_start
                    self.timers['pointcloud_generation'].append(pointcloud_time)
                    print(f"â±ï¸  pointcloud_generation: {pointcloud_time:.3f}s")

                    #import pdb; pdb.set_trace()
                    if len(points) > 0:
                        # åº”ç”¨æ‰‹çœ¼å˜æ¢ï¼šç›¸æœºâ†’å¤¹çˆª
                        points_gripper = self.apply_hand_eye_transform(points)
                        
                        # ä¿å­˜ç‚¹äº‘ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹ï¼‰
                        if self.debug:
                            # ä¿å­˜ç›¸æœºåæ ‡ç³»ç‚¹äº‘
                            cam_ply = os.path.join(self.pointcloud_dir, f"{base_name}_cam_pointcloud.ply")
                            save_pointcloud_to_file(points, colors, cam_ply)
                            # ä¿å­˜å¤¹çˆªåæ ‡ç³»ç‚¹äº‘
                            grip_ply = os.path.join(self.pointcloud_dir, f"{base_name}_gripper_pointcloud.ply")
                            save_pointcloud_to_file(points_gripper, colors, grip_ply)
                
                # don't forget to transform the units, the point cloud is in meter, but robot
                # control would like to be in mm. 

                # è®¡ç®—ç‚¹äº‘è´¨å¿ƒå’Œæ³•å‘é‡ï¼ˆåœ¨å¤¹çˆªåæ ‡ç³»ä¸­ï¼‰
                if points_gripper is not None and len(points_gripper) > 0:
                    # æŠ“å–ç‚¹è®¡ç®—è®¡æ—¶
                    grasp_calc_start = time.time()
                    
                    # è·å–å½“å‰æœºå™¨äººTCPä½ç½®
                    tcp_result = self.robot.get_tcp_position()
                    if isinstance(tcp_result, tuple) and len(tcp_result) == 2:
                        tcp_ok, current_tcp = tcp_result
                    else:
                        # å¦‚æœåªè¿”å›ä¸€ä¸ªå€¼ï¼Œå‡è®¾å®ƒæ˜¯ä½ç½®ä¿¡æ¯
                        current_tcp = tcp_result
                        tcp_ok = True
                    print(f"å½“å‰TCPä½ç½®: {current_tcp}")
                    
                    # è®¡ç®—è€ƒè™‘æ³•å‘é‡çš„æŠ“å–å§¿æ€
                    grasp_pose, normal_info = self.calculate_grasp_pose_with_normal(points_gripper, current_tcp)
                    
                    if normal_info is not None:
                        print("ğŸ¯ æ³•å‘é‡å¯¹é½æŠ“å–:")
                        print(f"  è´¨å¿ƒ: {normal_info['centroid']}")
                        print(f"  æ³•å‘é‡: {normal_info['normal']}")
                        print(f"  RPYå˜åŒ–: {np.degrees(normal_info['rpy_change'])} åº¦")
                        
                        # è®¡ç®—ç›¸å¯¹ç§»åŠ¨ï¼ˆåŒ…å«å§¿æ€è°ƒæ•´ï¼‰
                        # åœ¨å¤¹çˆªåæ ‡ç³»ä¸­ï¼Œä½ç½®å˜åŒ–å°±æ˜¯ç›®æ ‡ä½ç½®ï¼ˆå› ä¸ºå½“å‰TCPåœ¨åŸç‚¹ï¼‰
                        position_change = grasp_pose[:3]  # ç›®æ ‡ç‰©ä½“åœ¨å¤¹çˆªåæ ‡ç³»ä¸­çš„ä½ç½®
                        
                        # å§¿æ€å˜åŒ–ï¼šä»å½“å‰RPYåˆ°ç›®æ ‡RPY
                        orientation_change = grasp_pose[3:6] - current_tcp[3:6]
                        
                        # ç»„åˆç›¸å¯¹ç§»åŠ¨
                        relative_move = np.concatenate([position_change, orientation_change])
                        
                        print(f"ä½ç½®å˜åŒ–: {position_change} mm")
                        print(f"å§¿æ€å˜åŒ–: {np.degrees(orientation_change)} åº¦")
                        
                    else:
                        # å›é€€åˆ°ç®€å•è´¨å¿ƒæŠ“å–
                        print("âš ï¸  æ³•å‘é‡è®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨ç®€å•è´¨å¿ƒæŠ“å–")
                        centroid = np.mean(points_gripper, axis=0)
                        print(f"å¤¹çˆªåæ ‡ç³»ç‚¹äº‘è´¨å¿ƒ: {centroid}")
                        
                        # ç¡¬ç¼–ç é«˜åº¦ä¸º0.025m
                        hardcoded_height = 0.025  # 2.5cm
                        print(f"ä½¿ç”¨ç¡¬ç¼–ç é«˜åº¦: {hardcoded_height:.3f}m")
                        
                        # å¤¹çˆªåæ ‡ç³»ä¸­çš„ç›®æ ‡ä¸­å¿ƒç‚¹ï¼ˆè½¬æ¢ä¸ºæ¯«ç±³ï¼‰
                        center_gripper_mm = centroid * 1000
                        
                        # è®¡ç®—ç›¸å¯¹ç§»åŠ¨ï¼šä»å½“å‰TCPä½ç½®ç§»åŠ¨åˆ°å¤¹çˆªåæ ‡ç³»ä¸­çš„ç›®æ ‡ä½ç½®
                        delta_tool_mm = [center_gripper_mm[0], center_gripper_mm[1], hardcoded_height * 1000]
                        delta_base_xyz = self._tool_offset_to_base(delta_tool_mm, current_tcp[3:6])
                        z_offset = -(current_tcp[2] - hardcoded_height * 1000) + 200 - 20
                        relative_move = [delta_base_xyz[0], delta_base_xyz[1], z_offset, 0, 0, 0]
                    
                    grasp_calc_time = time.time() - grasp_calc_start
                    self.timers['grasp_calculation'].append(grasp_calc_time)
                    print(f"â±ï¸  grasp_calculation: {grasp_calc_time:.3f}s")
                    
                    print("Step1 : å‡†å¤‡æŠ“å–")
                    print("ç›¸å¯¹ç§»åŠ¨é‡:", relative_move)
                    
                    # æœºå™¨äººç§»åŠ¨è®¡æ—¶
                    robot_movement_start = time.time()
                    
                    # æ‰§è¡Œç›¸å¯¹ç§»åŠ¨
                    #import pdb; pdb.set_trace()
                    #self.robot.set_digital_output(0, 0, 1)

                    ret = self.robot.linear_move(relative_move, 1, True, 50)
                    # if ret != 0:
                    #     print(f"æœºå™¨äººç§»åŠ¨å¤±è´¥: {ret}")
                    #     self.robot.linear_move(original_tcp, 0 , True, 400)
                    #     self.robot.set_digital_output(0, 0, 0)
                    #     continue

                    #  robot move up of 20 cm relatively 
                    #  ret = self.robot.linear_move([current_tcp[0], current_tcp[1], current_tcp[2] -100, current_tcp[3], current_tcp[4], current_tcp[5]], 0, True, 400)
                    # if ret != 0:
                    #     print(f"æœºå™¨äººç§»åŠ¨å¤±è´¥: {ret}")
                    #     self.robot.linear_move(original_tcp, 0 , True, 400)
                    #     self.robot.set_digital_output(0, 0, 0)
                    #     continue
                    self.robot.linear_move(original_tcp, 0 , True, 500)

                    # æ—‹è½¬åŸºåº§90åº¦ (Yawè½´æ—‹è½¬)
                    # 90åº¦ = Ï€/2 å¼§åº¦ â‰ˆ 1.57 å¼§åº¦
                    # rotation_angle = math.pi / 2  # 90åº¦
                    # ret = self.robot.joint_move([-np.pi  * 0.6, 0, 0, 0, 0, 0], 1, True, 1)

                    # self.robot.set_digital_output(0, 0, 0)
                    # time.sleep(0.4)
                    # ret = self.robot.joint_move([np.pi  * 0.6, 0, 0, 0, 0, 0], 1, True, 2)
                
                    # #time.sleep(0.01)
                    # #robot move back to the original position
                    # self.robot.linear_move(original_tcp, 0 , True, 500)
                   
                    robot_movement_time = time.time() - robot_movement_start
                    self.timers['robot_movement'].append(robot_movement_time)
                    print(f"â±ï¸  robot_movement: {robot_movement_time:.3f}s")
            
                else:
                    print("ç‚¹äº‘ä¸ºç©ºï¼Œè·³è¿‡æœºå™¨äººæ§åˆ¶")

                # æ•´ä¸ªå¾ªç¯è®¡æ—¶ç»“æŸ
                cycle_time = time.time() - cycle_start
                self.timers['total_cycle'].append(cycle_time)
                print(f"â±ï¸  total_cycle: {cycle_time:.3f}s")
                print("-" * 50)
 
                # self.robot.logout()
                # exit()
                

        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­å¤„ç†")
        except Exception as e:
            print(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        finally:
            self.cleanup()

    def cleanup(self):
        """
        æ¸…ç†èµ„æº
        """
        cv2.destroyAllWindows()
        if self.pipeline:
            self.pipeline.stop()
        
        # æ‰“å°æ—¶é—´ç»Ÿè®¡æ‘˜è¦
        self.print_timing_summary()
        
        print(f"å¤„ç†å®Œæˆï¼æ€»å…±å¤„ç†äº† {self.frame_count} å¸§")
        print(f"ç»“æœä¿å­˜åœ¨: {self.output_dir}")

def main():
    parser = argparse.ArgumentParser(description='å®æ—¶äººä½“åˆ†å‰²å’Œ3Dç‚¹äº‘ç”Ÿæˆ')
    parser.add_argument('--output_dir', type=str, default='realtime_output',
                      help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: realtime_output)')
    parser.add_argument('--device', type=str, default='cuda',
                      choices=['cpu', 'cuda'],
                      help='è¿è¡Œè®¾å¤‡ (é»˜è®¤: cuda)')
    parser.add_argument('--save_pointcloud', action='store_true',
                      help='ä¿å­˜3Dç‚¹äº‘')
    parser.add_argument('--max_frames', type=int, default=None,
                      help='æœ€å¤§å¤„ç†å¸§æ•° (é»˜è®¤: æ— é™)')
    parser.add_argument('--no_preview', action='store_true',
                      help='ä¸æ˜¾ç¤ºé¢„è§ˆçª—å£')
    parser.add_argument('--intrinsics_file', type=str, default=None,
                      help='ç›¸æœºå†…å‚JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--hand_eye_file', type=str, default=None,
                      help='æ‰‹çœ¼æ ‡å®š4x4é½æ¬¡çŸ©é˜µçš„.npyæ–‡ä»¶è·¯å¾„ï¼ˆç›¸æœºâ†’å¤¹çˆªï¼‰')
    parser.add_argument('--bbox_selection', type=str, default='highest_confidence',
                      choices=['smallest', 'largest', 'highest_confidence'],
                      help='è¾¹ç•Œæ¡†é€‰æ‹©ç­–ç•¥: smallest(é€‰æ‹©é¢ç§¯æœ€å°çš„é±¼) æˆ– largest(é€‰æ‹©é¢ç§¯æœ€å¤§çš„é±¼) (é»˜è®¤: largest)')
    parser.add_argument('--debug', action='store_true',
                      help='å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜æ‰€æœ‰ä¸­é—´æ–‡ä»¶ï¼ˆRGBã€æ·±åº¦ã€æ£€æµ‹ã€åˆ†å‰²ã€ç‚¹äº‘ï¼‰')
    parser.add_argument('--use_yolo', action='store_true',
                      help='ä½¿ç”¨YOLOä½œä¸ºæ£€æµ‹å™¨ï¼ˆæ›¿ä»£Grounding DINOï¼‰')
    parser.add_argument('--yolo_weights', type=str, default=None,
                      help='YOLOæƒé‡æ–‡ä»¶(.pt)è·¯å¾„ï¼ˆä¸ --use_yolo æ­é…ä½¿ç”¨ï¼‰')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºå¤„ç†å™¨
        processor = RealtimeSegmentation3D(
            output_dir=args.output_dir,
            device=args.device,
            save_pointcloud=args.save_pointcloud,
            intrinsics_file=args.intrinsics_file,
            hand_eye_file=args.hand_eye_file,
            bbox_selection=args.bbox_selection,
            debug=args.debug,
            use_yolo=args.use_yolo,
            yolo_weights=args.yolo_weights
        )
        
        # è¿è¡Œå®æ—¶å¤„ç†
        processor.run_realtime(
            max_frames=args.max_frames,
            show_preview=not args.no_preview
        )
        
    except Exception as e:
        print(f"ç¨‹åºå‡ºé”™: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
