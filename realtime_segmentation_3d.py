#!/usr/bin/env python3
"""
å®æ—¶äººä½“åˆ†å‰²å’Œ3Dç‚¹äº‘ç”Ÿæˆè„šæœ¬

æ•´åˆç°æœ‰åŠŸèƒ½ï¼š
1. RealSenseç›¸æœºè¯»å–RGBå’Œæ·±åº¦æ•°æ®
2. SAM + Grounding DINOè¿›è¡Œäººä½“åˆ†å‰²
3. å°†æ©ç è½¬æ¢ä¸º3Dç‚¹äº‘

ä½¿ç”¨æ–¹æ³•:
    python3 realtime_segmentation_3d.py --output_dir output_data --save_pointcloud

ä¾èµ–:
    - ç°æœ‰çš„seg.py, mask_to_3d.py, realsense_capture.py
    - pyrealsense2, opencv-python, numpy, torch
    - segment_anything, transformers, open3d



    add a training mechasim that grasp only on the fish with masked out other region 
"""

import argparse
import os
import sys
import time
import numpy as np
import cv2
import torch
from datetime import datetime
from PIL import Image
import json

# å¯¼å…¥ç°æœ‰æ¨¡å—çš„åŠŸèƒ½
from seg import init_models
from util import (
    estimate_body_angle_alpha1,
    draw_principal_axis,
    angle_between_2d_from_origin,
    apply_hand_eye_transform as util_apply_hand_eye_transform,
    tool_offset_to_base as util_tool_offset_to_base,
)
from mask_to_3d import mask_to_3d_pointcloud, save_pointcloud, load_camera_intrinsics
from filter_mask import divide_mask
from realsense_capture import (
    setup_realsense,
    save_pointcloud_to_file,
    capture_frames as rs_capture_frames,
    capture_frames_with_retry as rs_capture_frames_with_retry,
    validate_camera_connection as rs_validate_camera_connection,
    check_camera_health as rs_check_camera_health,
)
# Landmark detector for AI-based grasp point estimation
try:
    # ä¼˜å…ˆä»¥åŒ…å½¢å¼å¯¼å…¥
    from landmarks.fish_landmark_detector import FishLandmarkDetector
except Exception:
    # å…¼å®¹ç›´æ¥åœ¨å·¥ä½œåŒºæ ¹ç›®å½•è¿è¡Œ
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'landmarks'))
    try:
        from fish_landmark_detector import FishLandmarkDetector
    except Exception:
        FishLandmarkDetector = None

# è¿½åŠ è‡ªå®šä¹‰æ¨¡å—æœç´¢è·¯å¾„ï¼ˆæ‰‹çœ¼æ ‡å®šç›®å½•ï¼‰
_extra_paths = [
    "/home/ai/AI_perception/hand_eye_calibrate",
]
for _p in _extra_paths:
    try:
        if os.path.isdir(_p) and _p not in sys.path:
            sys.path.insert(0, _p)
    except Exception:
        pass

# å¯¼å…¥é±¼å®¹å™¨è·Ÿè¸ªå™¨
try:
    from FishContainerTracker import FishContainerTracker
except ImportError:
    print("[è­¦å‘Š] æ— æ³•å¯¼å…¥ FishContainerTrackerï¼Œå°†è·³è¿‡é‡é‡è·Ÿè¸ªåŠŸèƒ½")
    FishContainerTracker = None


class RealtimeSegmentation3D:
    def __init__(self, output_dir, device="cpu", save_pointcloud=True, intrinsics_file=None, hand_eye_file=None, bbox_selection="highest_confidence", debug=False, use_yolo=False, yolo_weights=None,
                 grasp_point_mode: str = "centroid", landmark_model_path: str = None, enable_weight_tracking: bool = True, max_container_weight: float = 12.5, det_gray: bool = False,
                 camera_calib_json: str = None, robot_config: str = "config/robot.json", erode_bbox: bool = False, erode_ratio: float = 0.1, bbox_scale: float = 1.0,
                 seg_model: str = "sam", yolo_seg_weights: str = None):
        """
        åˆå§‹åŒ–å®æ—¶åˆ†å‰²å’Œ3Dç‚¹äº‘ç”Ÿæˆå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            device: è¿è¡Œè®¾å¤‡ (cpu/cuda)
            save_pointcloud: æ˜¯å¦ä¿å­˜3Dç‚¹äº‘
            intrinsics_file: ç›¸æœºå†…å‚JSONæ–‡ä»¶è·¯å¾„
            hand_eye_file: æ‰‹çœ¼æ ‡å®š4x4é½æ¬¡çŸ©é˜µçš„.npyæ–‡ä»¶è·¯å¾„ï¼ˆç›¸æœºâ†’å¤¹çˆªï¼‰
            bbox_selection: è¾¹ç•Œæ¡†é€‰æ‹©ç­–ç•¥ ("smallest" æˆ– "largest" æˆ– "highest_confidence")
            debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆä¿å­˜æ‰€æœ‰ä¸­é—´æ–‡ä»¶ï¼‰
            use_yolo: æ˜¯å¦ä½¿ç”¨YOLOä½œä¸ºæ£€æµ‹å™¨
            yolo_weights: YOLOæƒé‡è·¯å¾„ï¼ˆ.ptï¼‰
            grasp_point_mode: æŠ“å–ç‚¹æ¨¡å¼ ("centroid" æˆ– "ai")
            landmark_model_path: AIå…³é”®ç‚¹æ¨¡å‹è·¯å¾„
            enable_weight_tracking: æ˜¯å¦å¯ç”¨é‡é‡è·Ÿè¸ª
            max_container_weight: å®¹å™¨æœ€å¤§é‡é‡ï¼ˆkgï¼‰
            erode_bbox: æ˜¯å¦å¯¹maskè¿›è¡Œä¸Šä¸‹æ–¹å‘è…èš€ï¼ˆç”¨äºæ›´ç²¾ç¡®çš„è´¨å¿ƒè®¡ç®—ï¼‰
            erode_ratio: è…èš€æ¯”ä¾‹ï¼Œä¸Šä¸‹å„è…èš€çš„æ¯”ä¾‹ï¼ˆé»˜è®¤0.1ï¼Œå³10%ï¼‰
            bbox_scale: è¾¹ç•Œæ¡†ç¼©æ”¾å› å­ï¼ˆé»˜è®¤1.0ï¼Œå³ä¸ç¼©æ”¾ï¼›>1.0æ”¾å¤§ï¼Œ<1.0ç¼©å°ï¼‰
            seg_model: åˆ†å‰²æ¨¡å‹ç±»å‹ ("sam" æˆ– "yolov8_seg")ï¼Œé»˜è®¤ "sam"
            yolo_seg_weights: YOLOv8åˆ†å‰²æ¨¡å‹æƒé‡è·¯å¾„ï¼ˆå½“seg_model="yolov8_seg"æ—¶å¿…éœ€ï¼‰
        """
        self.output_dir = output_dir
        self.device = device
        self.save_pointcloud = save_pointcloud
        self.bbox_selection = bbox_selection
        self.debug = debug
        self.use_yolo = use_yolo
        self.yolo_weights = yolo_weights
        self.erode_bbox = erode_bbox
        self.erode_ratio = erode_ratio
        self.bbox_scale = bbox_scale
        # detection-only grayscale support (optional)
        self.det_gray = det_gray
        # æŠ“å–ç‚¹æ¨¡å¼ï¼šcentroid æˆ– ai
        self.grasp_point_mode = grasp_point_mode
        self.landmark_model_path = landmark_model_path
        # é‡é‡è·Ÿè¸ªç›¸å…³
        self.enable_weight_tracking = enable_weight_tracking
        self.max_container_weight = max_container_weight
        # configs
        self.camera_calib_json = camera_calib_json
        self.robot_config = robot_config
        # åˆ†å‰²æ¨¡å‹ç›¸å…³
        self.seg_model = seg_model
        self.yolo_seg_weights = yolo_seg_weights
        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹åˆ›å»ºï¼‰
        if self.debug:
            self.rgb_dir = os.path.join(output_dir, "rgb")
            self.depth_dir = os.path.join(output_dir, "depth")
            self.mask_dir = os.path.join(output_dir, "masks")
            self.pointcloud_dir = os.path.join(output_dir, "pointclouds")
            self.segmentation_dir = os.path.join(output_dir, "segmentation")
            self.detection_dir = os.path.join(output_dir, "detection")
            
            os.makedirs(self.rgb_dir, exist_ok=True)
            os.makedirs(self.depth_dir, exist_ok=True)
            os.makedirs(self.mask_dir, exist_ok=True)
            if save_pointcloud:
                os.makedirs(self.pointcloud_dir, exist_ok=True)
            os.makedirs(self.segmentation_dir, exist_ok=True)
            os.makedirs(self.detection_dir, exist_ok=True)
            print("è°ƒè¯•æ¨¡å¼å·²å¯ç”¨ï¼Œå°†ä¿å­˜æ‰€æœ‰ä¸­é—´æ–‡ä»¶")
        else:
            print("æ­£å¸¸æ¨¡å¼ï¼Œä¸ä¿å­˜ä¸­é—´æ–‡ä»¶")
        
        # åˆå§‹åŒ–æ¨¡å‹
        print("æ­£åœ¨åˆå§‹åŒ–AIæ¨¡å‹...")
        #self.sam_predictor, self.grounding_dino_model, self.processor = init_models(device)
        self.seg_predictor = init_models(device, seg_model=seg_model, yolo_seg_weights=yolo_seg_weights)
        # ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™ sam_predictor å±æ€§
        if seg_model == "sam":
            self.sam_predictor = self.seg_predictor
        else:
            self.sam_predictor = None  # ä½¿ç”¨yolov8_segæ—¶ä¸éœ€è¦sam_predictor

        if self.use_yolo:
            if not self.yolo_weights or not os.path.exists(self.yolo_weights):
                print(f"[è­¦å‘Š] å·²å¯ç”¨YOLOæ£€æµ‹ï¼Œä½†æœªæ‰¾åˆ°æƒé‡: {self.yolo_weights}ï¼Œå°†å›é€€Grounding DINO")
                self.use_yolo = False
        
            try:
                from ultralytics import YOLO
            except Exception as e:
                print("[é”™è¯¯] æœªæ‰¾åˆ° ultralyticsï¼Œè¯·å…ˆ: pip install ultralytics")
                print(e)
                return []

            # åŠ è½½æ¨¡å‹ï¼ˆæ¯æ¬¡è°ƒç”¨åŠ è½½é¿å…ä¸å…¶ä»–ä¾èµ–å†²çªï¼›è‹¥é¢‘ç¹è°ƒç”¨å¯å¤–éƒ¨ç¼“å­˜æ¨¡å‹å®ä¾‹ï¼‰
            self.yolo_model = YOLO(self.yolo_weights)
            print(f"å·²åŠ è½½YOLOæƒé‡: {self.yolo_weights}")
                
        # åˆå§‹åŒ–RealSenseç›¸æœº
        print("æ­£åœ¨åˆå§‹åŒ–RealSenseç›¸æœº...")
        self.pipeline, self.config = setup_realsense()
        if self.pipeline is None:
            raise RuntimeError("æ— æ³•å¯åŠ¨RealSenseç›¸æœº")
        
        # è·å–ç›¸æœºå†…å‚å’Œç•¸å˜ç³»æ•°
        self.fx, self.fy, self.cx, self.cy, self.dist, self.mtx = load_camera_intrinsics(intrinsics_file)
        print(f"ä½¿ç”¨ç›¸æœºå†…å‚: fx={self.fx}, fy={self.fy}, cx={self.cx}, cy={self.cy}")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç•¸å˜æ ¡æ­£
        if np.any(self.dist != 0):
            print("æ£€æµ‹åˆ°ç•¸å˜ç³»æ•°ï¼Œå°†è¿›è¡Œå®æ—¶å›¾åƒç•¸å˜æ ¡æ­£")
            print(f"ç•¸å˜ç³»æ•°: k1={self.dist[0]:.6f}, k2={self.dist[1]:.6f}, k3={self.dist[4]:.6f}")
        else:
            print("æœªæ£€æµ‹åˆ°ç•¸å˜ç³»æ•°ï¼Œè·³è¿‡ç•¸å˜æ ¡æ­£")
            self.mtx = None
            self.dist = None
        
        # åˆ›å»ºå¯¹é½å¯¹è±¡
        import pyrealsense2 as rs
        self.align = rs.align(rs.stream.color)
        
        # å¸§è®¡æ•°å™¨
        self.frame_count = 0
        self.start_time = time.time()
        
        # è®¡æ—¶å™¨
        self.timers = {
            'detection': [],
            'segmentation': [],
            'pointcloud_generation': [],
            'grasp_calculation': [],
            'robot_movement': [],
            'total_cycle': []
        }
        
        # åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µï¼ˆå¯é€‰ï¼‰
        self.hand_eye_transform = None  # 4x4 é½æ¬¡çŸ©é˜µï¼Œç›¸æœºåæ ‡â†’å¤¹çˆªåæ ‡
        if hand_eye_file is not None and os.path.exists(hand_eye_file):
            try:
                mat = np.load(hand_eye_file)
                if mat.shape == (4, 4):
                    self.hand_eye_transform = mat.astype(np.float32)
                    print("å·²åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µ (ç›¸æœºâ†’å¤¹çˆª):")
                    print(self.hand_eye_transform)
                else:
                    print(f"hand_eye_file æ ¼å¼ä¸æ­£ç¡®ï¼ŒæœŸæœ›(4,4)ï¼Œå®é™…{mat.shape}ï¼Œå¿½ç•¥ã€‚")
            except Exception as e:
                print(f"åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µå¤±è´¥: {e}")
        # è‹¥æœªåŠ è½½åˆ°ï¼Œå°è¯•ä» camera_calib_json åŠ è½½ï¼Œå¦åˆ™ä½¿ç”¨ç¡¬ç¼–ç çš„Rã€tï¼ˆç›¸æœºâ†’å¤¹çˆªï¼‰
        if self.hand_eye_transform is None:
            loaded_json = False
            try:
                if self.camera_calib_json and os.path.exists(self.camera_calib_json):
                    with open(self.camera_calib_json, 'r', encoding='utf-8') as f:
                        calib = json.load(f)
                    he = calib.get('hand_eye', {})
                    R = np.array(he.get('R', []), dtype=np.float32)
                    t = np.array(he.get('t', []), dtype=np.float32).reshape(3, 1) if he.get('t', None) is not None else None
                    if R.shape == (3, 3) and t is not None and t.shape == (3, 1):
                        self.hand_eye_transform = np.eye(4, dtype=np.float32)
                        self.hand_eye_transform[:3, :3] = R
                        self.hand_eye_transform[:3, 3:4] = t
                        loaded_json = True
                        print(f"å·²ä» JSON åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µ: {self.camera_calib_json}")
            except Exception as e:
                print(f"è¯»å– camera_calib_json å¤±è´¥: {e}")
            if not loaded_json:
                R_default = np.array([
                    [-0.99455141, -0.08982915, -0.05289825],
                    [ 0.09064269, -0.99579624, -0.01318166],
                    [-0.05149178, -0.01790468,  0.9985129 ]
                ], dtype=np.float32)
                t_default = np.array([[0.0607777], [0.10496735], [-0.18889416]], dtype=np.float32)

                self.hand_eye_transform = np.eye(4, dtype=np.float32)
                self.hand_eye_transform[:3, :3] = R_default
                self.hand_eye_transform[:3, 3:4] = t_default
                print("ä½¿ç”¨ç¡¬ç¼–ç æ‰‹çœ¼æ ‡å®šçŸ©é˜µ (ç›¸æœºâ†’å¤¹çˆª):")
                print(self.hand_eye_transform)
        
        print("åˆå§‹åŒ–å®Œæˆï¼")

        # åˆå§‹åŒ–AIå…³é”®ç‚¹æ£€æµ‹å™¨ï¼ˆå¯é€‰ï¼‰
        self.landmark_detector = None
        if self.grasp_point_mode == "ai":
            if FishLandmarkDetector is None:
                print("[è­¦å‘Š] æ— æ³•å¯¼å…¥ FishLandmarkDetectorï¼Œå°†å›é€€ä¸ºè´¨å¿ƒæ¨¡å¼")
                self.grasp_point_mode = "centroid"
            else:
                try:
                    if landmark_model_path is None:
                        print("[è­¦å‘Š] æœªæä¾› landmark_model_pathï¼Œå°†å›é€€ä¸ºè´¨å¿ƒæ¨¡å¼")
                        self.grasp_point_mode = "centroid"
                    else:
                        self.landmark_detector = FishLandmarkDetector(model_path=landmark_model_path, device=('cuda' if self.device=='cuda' and torch.cuda.is_available() else 'cpu'))
                        print(f"å·²åŠ è½½AIå…³é”®ç‚¹æ¨¡å‹: {landmark_model_path}")
                except Exception as e:
                    print(f"[è­¦å‘Š] å…³é”®ç‚¹æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†å›é€€ä¸ºè´¨å¿ƒæ¨¡å¼")
                    self.grasp_point_mode = "centroid"

        # åˆå§‹åŒ–é±¼å®¹å™¨è·Ÿè¸ªå™¨ï¼ˆå¯é€‰ï¼‰
        self.fish_tracker = None
        if self.enable_weight_tracking and FishContainerTracker is not None:
            try:
                self.fish_tracker = FishContainerTracker(
                    max_weight_kg=self.max_container_weight,
                    data_file=os.path.join(self.output_dir, "fish_tracking_data.json")
                )
                print(f"å·²å¯ç”¨é±¼å®¹å™¨è·Ÿè¸ªå™¨ï¼Œæœ€å¤§å®¹é‡: {self.max_container_weight}kg")
            except Exception as e:
                print(f"[è­¦å‘Š] é±¼å®¹å™¨è·Ÿè¸ªå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.fish_tracker = None
        else:
            print("é±¼å®¹å™¨è·Ÿè¸ªå™¨æœªå¯ç”¨")

        import jkrc 
        self.robot = jkrc.RC("192.168.80.116")
        self.robot.login()   
        self.robot.set_digital_output(0, 0, 0)

    
    def time_step(self, step_name):
        """è®¡æ—¶å™¨è£…é¥°å™¨ï¼Œç”¨äºæµ‹é‡å„ä¸ªæ­¥éª¤çš„æ—¶é—´"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                start_time = time.time()
                result = func(*args, **kwargs)
                end_time = time.time()
                elapsed = end_time - start_time
                self.timers[step_name].append(elapsed)
                print(f"â±ï¸  {step_name}: {elapsed:.3f}s")
                return result
            return wrapper
        return decorator
    
    def print_timing_summary(self):
        """æ‰“å°æ—¶é—´ç»Ÿè®¡æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æ—¶é—´ç»Ÿè®¡æ‘˜è¦")
        print("="*60)
        
        for step_name, times in self.timers.items():
            if times:
                avg_time = np.mean(times)
                min_time = np.min(times)
                max_time = np.max(times)
                total_time = np.sum(times)
                print(f"{step_name:20s}: å¹³å‡={avg_time:.3f}s, æœ€å°={min_time:.3f}s, æœ€å¤§={max_time:.3f}s, æ€»è®¡={total_time:.3f}s")
            else:
                print(f"{step_name:20s}: æ— æ•°æ®")
        
        print("="*60)
    
    def capture_frames(self, timeout_ms=10000):
        """å§”æ‰˜åˆ° realsense_capture.capture_frames"""
        return rs_capture_frames(self.pipeline, self.align, timeout_ms)
    
    def capture_frames_with_retry(self, max_retries=3, timeout_ms=10000):
        return rs_capture_frames_with_retry(self.pipeline, self.align, max_retries, timeout_ms)
    
    def validate_camera_connection(self, timeout_ms=5000):
        return rs_validate_camera_connection(self.pipeline, self.align, timeout_ms)
    
    def check_camera_health(self):
        return rs_check_camera_health(self.pipeline)
    
    # write seperate function to do detection and segmentation togther but segmentation is done for all the objects, 
    # and then we can select the  grasp object based on the distance of camera,.
    def detect_and_segment_and_dump_all(self, color_image, depth_image):
        """
        æ£€æµ‹æ‰€æœ‰å€™é€‰é±¼ï¼Œåˆ†åˆ«åˆ†å‰²å¹¶è®¡ç®—ç‚¹äº‘è´¨å¿ƒæ·±åº¦ï¼Œé€‰å–ç¦»ç›¸æœºæœ€è¿‘çš„ä¸€ä¸ªã€‚

        Args:
            color_image: BGR å›¾åƒ (H,W,3)
            depth_image: æ·±åº¦å›¾ (æ¯«ç±³, uint16)

        Returns:
            mask_np: é€‰ä¸­é±¼çš„å•é€šé“uint8æ©ç ï¼ˆ0/255ï¼‰ï¼Œè‹¥å¤±è´¥è¿”å›None
            base_name: æ–‡ä»¶åŸºåå­—ç¬¦ä¸²ï¼Œè‹¥å¤±è´¥è¿”å›None
        """
        # 1) æ£€æµ‹æ‰€æœ‰å€™é€‰æ¡†
        detection_start = time.time()
        #if getattr(self, 'use_yolo', False):
        # YOLO è·¯å¾„ï¼šdetect_yolo å·²è¿”å›æ‰€æœ‰æ»¡è¶³æ¡ä»¶çš„æ¡† (x1,y1,x2,y2,conf)
        boxes = self.detect_yolo(color_image, self.yolo_weights, conf=0.6, iou=0.45, imgsz=640, min_area=8000)
       
        detection_time = time.time() - detection_start
        self.timers['detection'].append(detection_time)
        print(f"â±ï¸  detection(all): {detection_time:.3f}s  å€™é€‰æ•°: {len(boxes) if boxes else 0}")

        if not boxes:
            print("æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼Œè·³è¿‡åˆ†å‰²ã€‚")
            return None, None

        # 2) é€ä¸ªå€™é€‰æ¡†è¿›è¡Œåˆ†å‰²ï¼Œå¹¶è®¡ç®—ç‚¹äº‘è´¨å¿ƒæ·±åº¦
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        base_name = f"frame_{self.frame_count:06d}_{timestamp}"

        image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)
        
        # æ ¹æ®åˆ†å‰²æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹å¼
        if self.seg_model == "yolov8_seg":
            # ä½¿ç”¨YOLOv8åˆ†å‰²æ¨¡å‹
            # YOLOv8å¯ä»¥ç›´æ¥å¯¹æ•´ä¸ªå›¾åƒè¿›è¡Œåˆ†å‰²ï¼Œç„¶åæ ¹æ®bboxæå–å¯¹åº”çš„mask
            try:
                yolo_results = self.seg_predictor(color_image, verbose=False)
                # æå–æ‰€æœ‰æ£€æµ‹ç»“æœ
                all_yolo_boxes = []
                all_yolo_masks = []
                for result in yolo_results:
                    if result.masks is not None:
                        boxes_yolo = result.boxes.xyxy.cpu().numpy()
                        masks_yolo = result.masks.data.cpu().numpy()
                        all_yolo_boxes.extend(boxes_yolo)
                        all_yolo_masks.extend(masks_yolo)
            except Exception as e:
                print(f"[åˆ†å‰²] YOLOv8åˆ†å‰²å¤±è´¥: {e}")
                return None, None
        else:
            # ä½¿ç”¨SAMæ¨¡å‹
            self.sam_predictor.set_image(image_rgb)

        best_idx = -1
        best_depth_m = float('inf')
        best_mask = None
        best_confidence = -1.0  # For highest_confidence selection

        segmentation_start = time.time()
        for i, b in enumerate(boxes):
            x1, y1, x2, y2 = b[:4]
            confidence = b[4] if len(b) > 4 else 0.0  # Get confidence from bbox
            
            if self.seg_model == "yolov8_seg":
                # ä½¿ç”¨YOLOv8åˆ†å‰²
                try:
                    # ä»YOLOv8ç»“æœä¸­æ‰¾åˆ°ä¸å½“å‰bboxæœ€åŒ¹é…çš„æ£€æµ‹
                    mask_np = None
                    best_iou = 0.0
                    best_mask_idx = -1
                    
                    for j, (box_yolo, mask_yolo) in enumerate(zip(all_yolo_boxes, all_yolo_masks)):
                        # è®¡ç®—IoU
                        box_yolo_x1, box_yolo_y1, box_yolo_x2, box_yolo_y2 = box_yolo
                        intersection_x1 = max(x1, box_yolo_x1)
                        intersection_y1 = max(y1, box_yolo_y1)
                        intersection_x2 = min(x2, box_yolo_x2)
                        intersection_y2 = min(y2, box_yolo_y2)
                        
                        if intersection_x2 > intersection_x1 and intersection_y2 > intersection_y1:
                            intersection_area = (intersection_x2 - intersection_x1) * (intersection_y2 - intersection_y1)
                            box_area = (x2 - x1) * (y2 - y1)
                            box_yolo_area = (box_yolo_x2 - box_yolo_x1) * (box_yolo_y2 - box_yolo_y1)
                            union_area = box_area + box_yolo_area - intersection_area
                            iou = intersection_area / union_area if union_area > 0 else 0.0
                            
                            if iou > best_iou and iou > 0.3:  # é˜ˆå€¼0.3
                                best_iou = iou
                                best_mask_idx = j
                    
                    if best_mask_idx >= 0:
                        # å°†maskè½¬æ¢ä¸ºuint8æ ¼å¼ï¼Œå¹¶è°ƒæ•´åˆ°å›¾åƒå°ºå¯¸
                        mask_yolo = all_yolo_masks[best_mask_idx]
                        h, w = color_image.shape[:2]
                        if mask_yolo.shape != (h, w):
                            # å¦‚æœmaskå°ºå¯¸ä¸åŒ¹é…ï¼Œéœ€è¦è°ƒæ•´
                            mask_yolo = cv2.resize(mask_yolo, (w, h), interpolation=cv2.INTER_NEAREST)
                        mask_np = (mask_yolo * 255).astype(np.uint8)
                    else:
                        print(f"[åˆ†å‰²] å€™é€‰æ¡† {i} æœªæ‰¾åˆ°åŒ¹é…çš„YOLOv8åˆ†å‰²ç»“æœ (IoUé˜ˆå€¼: 0.3)")
                        continue
                except Exception as e:
                    print(f"[åˆ†å‰²] å€™é€‰æ¡† {i} YOLOv8åˆ†å‰²å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    continue
            else:
                # ä½¿ç”¨SAMåˆ†å‰²
                boxes_tensor = torch.tensor([[x1, y1, x2, y2]], device=self.device)
                transformed_boxes = self.sam_predictor.transform.apply_boxes_torch(boxes_tensor, image_rgb.shape[:2])

                try:
                    masks, scores, logits = self.sam_predictor.predict_torch(
                        point_coords=None,
                        point_labels=None,
                        boxes=transformed_boxes,
                        multimask_output=False
                    )
                except Exception as e:
                    print(f"[åˆ†å‰²] å€™é€‰æ¡† {i} é¢„æµ‹å¤±è´¥: {e}")
                    continue

                if masks.shape[0] == 0 or masks.shape[1] == 0:
                    print(f"[åˆ†å‰²] å€™é€‰æ¡† {i} æœªç”Ÿæˆæ©ç ")
                    continue

                m_bool = masks[0][0].detach().cpu().numpy().astype(np.uint8)
                mask_np = m_bool * 255
            # é™åˆ¶åœ¨ bbox å†…
            restricted_mask = np.zeros_like(mask_np, dtype=np.uint8)
            restricted_mask[y1:y2, x1:x2] = mask_np[y1:y2, x1:x2]
            mask_np = restricted_mask

            # åº”ç”¨divide_maskä¼˜åŒ–maskï¼Œæå–æœ€å¤§è¿é€šåŒºåŸŸ
            try:
                mask_np_refined = divide_mask(mask_np, verbose=False)
                # ç¡®ä¿è¿”å›çš„maskæ ¼å¼ä¸€è‡´ï¼ˆ0/255ï¼‰
                if mask_np_refined.max() <= 1:
                    mask_np_refined = mask_np_refined * 255
                mask_np = mask_np_refined.astype(np.uint8)
            except Exception as e:
                # å¦‚æœä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹mask
                if self.debug:
                    print(f"[ä¼˜åŒ–] å€™é€‰æ¡† {i} maskä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹mask: {e}")

            # åº”ç”¨å‚ç›´è…èš€ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.erode_bbox:
                mask_np = self.erode_mask_vertical(mask_np)

            # ä½¿ç”¨2Dè´¨å¿ƒ+æ·±åº¦è®¡ç®—æ·±åº¦ï¼ˆç›¸æœºåæ ‡ç³»ï¼Œå•ä½ç±³ï¼‰
            mask_bool = (mask_np > 0)
            if not np.any(mask_bool):
                print(f"[åˆ†å‰²] å€™é€‰æ¡† {i} æ©ç ä¸ºç©ºï¼Œè·³è¿‡")
                continue

            # è®¡ç®—2Dè´¨å¿ƒ
            try:
                ys_mask, xs_mask = np.where(mask_bool)
                if ys_mask.size == 0 or xs_mask.size == 0:
                    print(f"[åˆ†å‰²] å€™é€‰æ¡† {i} æ©ç ä¸ºç©ºï¼Œè·³è¿‡")
                    continue
                
                centroid_x_2d = int(np.mean(xs_mask))
                centroid_y_2d = int(np.mean(ys_mask))
                
                # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
                h, w = depth_image.shape
                centroid_x_2d = max(0, min(w - 1, centroid_x_2d))
                centroid_y_2d = max(0, min(h - 1, centroid_y_2d))
                
                # è·å–æ·±åº¦å€¼ï¼ˆæ¯«ç±³ï¼‰
                depth_mm = depth_image[int(centroid_y_2d), int(centroid_x_2d)]
                
                if depth_mm > 0:
                    # è½¬æ¢ä¸ºç±³
                    depth_m = depth_mm / 1000.0
                    print(f"å€™é€‰æ¡† {i} 2Dè´¨å¿ƒ: ({centroid_x_2d}, {centroid_y_2d}), æ·±åº¦: {depth_m:.4f} m  bbox=({x1},{y1},{x2},{y2})")
                else:
                    print(f"[æ·±åº¦] å€™é€‰æ¡† {i} è´¨å¿ƒä½ç½®æ·±åº¦å€¼ä¸º0ï¼Œè·³è¿‡")
                    continue
                    
            except Exception as e:
                print(f"[è®¡ç®—] å€™é€‰æ¡† {i} è®¡ç®—2Dè´¨å¿ƒæ·±åº¦å¤±è´¥: {e}ï¼Œè·³è¿‡")
                continue

            # è®°å½•è°ƒè¯•è¾“å‡º
            if self.debug:
                cv2.imwrite(os.path.join(self.segmentation_dir, f"{base_name}_cand{i}_mask.png"), mask_np)
                det_vis = color_image.copy()
                cv2.rectangle(det_vis, (x1, y1), (x2, y2), (0, 255, 255), 2)
                cv2.putText(det_vis, f"cand {i}", (x1, max(0, y1-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)
                cv2.imwrite(os.path.join(self.detection_dir, f"{base_name}_cand{i}_box.png"), det_vis)

            # Select best bbox based on bbox_selection strategy
            should_update = False
            if self.bbox_selection == "highest_confidence":
                # Select bbox with highest confidence
                if confidence > best_confidence:
                    should_update = True
            elif self.bbox_selection == "smallest":
                # Select bbox with smallest area (closest depth)
                if depth_m < best_depth_m:
                    should_update = True
            elif self.bbox_selection == "largest":
                # Select bbox with largest area (farthest depth)
                if depth_m > best_depth_m:
                    should_update = True
            else:
                # Default: smallest depth (closest)
                if depth_m < best_depth_m:
                    should_update = True
            
            if should_update:
                best_depth_m = depth_m
                best_mask = mask_np
                best_idx = i
                best_confidence = confidence

        segmentation_time = time.time() - segmentation_start
        self.timers['segmentation'].append(segmentation_time)
        print(f"â±ï¸  segmentation(all): {segmentation_time:.3f}s")

        if best_idx == -1 or best_mask is None:
            print("åˆ†å‰²/ç‚¹äº‘å‡å¤±è´¥ï¼Œæœªé€‰å‡ºå€™é€‰")
            return None, None
        
        if self.bbox_selection == "highest_confidence":
            print(f"é€‰æ‹©æœ€é«˜ç½®ä¿¡åº¦å€™é€‰: idx={best_idx}, ç½®ä¿¡åº¦={best_confidence:.3f}, æ·±åº¦={best_depth_m:.4f} m")
        elif self.bbox_selection == "smallest":
            print(f"é€‰æ‹©æœ€è¿‘å€™é€‰: idx={best_idx}, æ·±åº¦={best_depth_m:.4f} m")
        elif self.bbox_selection == "largest":
            print(f"é€‰æ‹©æœ€è¿œå€™é€‰: idx={best_idx}, æ·±åº¦={best_depth_m:.4f} m")
        else:
            print(f"é€‰æ‹©æœ€è¿‘å€™é€‰: idx={best_idx}, æ·±åº¦={best_depth_m:.4f} m")

        if best_depth_m > 0.8:
            print(f"æ·±åº¦è¶…è¿‡0.8mï¼Œè·³è¿‡")
            return None, None

        # å¯è§†åŒ–æœ€ç»ˆé€‰æ‹©
        if self.debug:
            colored = np.zeros_like(color_image)
            colored[best_mask > 0] = [0, 255, 0]
            vis = cv2.addWeighted(color_image, 1.0, colored, 0.4, 0)
            vis_path = os.path.join(self.segmentation_dir, f"{base_name}_closest_overlay.png")
            cv2.imwrite(vis_path, vis)

        return best_mask, base_name


    def detect_yolo(self, color_image, yolo_weights_path, conf=0.25, iou=0.45, imgsz=640, min_area=1000):
        """
        ä½¿ç”¨Ultralytics YOLOè¿›è¡Œé±¼çš„æ£€æµ‹ï¼Œè¿”å›æ‰€æœ‰æ£€æµ‹åˆ°çš„bboxã€‚
        
        Args:
            color_image: OpenCV BGRå›¾åƒ (H,W,3)
            yolo_weights_path: è®­ç»ƒå¥½çš„YOLOæƒé‡ .pt è·¯å¾„
            conf: ç½®ä¿¡åº¦é˜ˆå€¼
            iou: NMS IOU é˜ˆå€¼
            imgsz: æ¨ç†è¾“å…¥å°ºå¯¸
            min_area: è¿‡æ»¤æœ€å°é¢ç§¯ï¼ˆåƒç´ ï¼‰
        
        Returns:
            boxes: List[Tuple[x1, y1, x2, y2, confidence]] æ‰€æœ‰æ»¡è¶³æ¡ä»¶çš„bboxï¼›è‹¥æ— åˆ™è¿”å›ç©ºåˆ—è¡¨
        """
     

        # YOLOæ”¯æŒç›´æ¥ä¼ å…¥numpyå›¾åƒï¼›ç¡®ä¿ä¸ºRGB
        try:
            # grayscale only for detection if enabled
            det_input = color_image
            if getattr(self, 'det_gray', False):
                gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)
                det_input = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
            results = self.yolo_model.predict(
                source=[det_input],
                imgsz=imgsz,
                conf=conf,
                iou=iou,
                verbose=False,
                save=False
            )
        except Exception as e:
            print(f"[é”™è¯¯] YOLO æ¨ç†å¤±è´¥: {e}")
            return []

        if not results:
            return []

        res = results[0]
        boxes_np = None
        confidences = None
        try:
            # xyxy (N,4) å’Œ conf (N,)
            boxes_np = res.boxes.xyxy.cpu().numpy() if hasattr(res, 'boxes') and res.boxes is not None else None
            confidences = res.boxes.conf.cpu().numpy() if hasattr(res, 'boxes') and res.boxes is not None else None
        except Exception:
            boxes_np = None
            confidences = None

        if boxes_np is None or len(boxes_np) == 0:
            return []

        # è¿‡æ»¤é¢ç§¯ã€è£å‰ªåˆ°å›¾åƒèŒƒå›´ï¼ŒåŒæ—¶ä¿å­˜ç½®ä¿¡åº¦ä¿¡æ¯
        H, W = color_image.shape[0], color_image.shape[1]
        valid_boxes = []
        for i, xyxy in enumerate(boxes_np):
            x1, y1, x2, y2 = [int(round(v)) for v in xyxy[:4].tolist()]
            
            # åº”ç”¨è¾¹ç•Œæ¡†ç¼©æ”¾ï¼ˆåœ¨YOLOæ£€æµ‹åç«‹å³åº”ç”¨ï¼‰
            x1, y1, x2, y2 = self.scale_bbox(x1, y1, x2, y2, H, W)
            
            # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…ï¼ˆç¼©æ”¾åå†æ¬¡æ£€æŸ¥ï¼‰
            x1 = max(0, min(x1, W - 1))
            y1 = max(0, min(y1, H - 1))
            x2 = max(0, min(x2, W - 1))
            y2 = max(0, min(y2, H - 1))
            area = max(0, x2 - x1) * max(0, y2 - y1)
            if area > min_area:
                confidence = confidences[i] if confidences is not None else 0.0
                valid_boxes.append(((x1, y1, x2, y2), area, confidence))

        boxes = []
        if valid_boxes:
            # è¿”å›æ‰€æœ‰æ£€æµ‹åˆ°çš„bboxï¼ŒåŒ…å«ç½®ä¿¡åº¦ä¿¡æ¯
            for bbox_info in valid_boxes:
                bbox_coords, area, confidence = bbox_info
                # è¿”å›æ ¼å¼: (x1, y1, x2, y2, confidence)
                boxes.append((*bbox_coords, confidence))
            
            print(f"[YOLO] æ£€æµ‹åˆ° {len(valid_boxes)} ä¸ªå€™é€‰æ¡†ï¼Œå…¨éƒ¨è¿”å›ç”¨äºå¤„ç†")
            for i, (bbox_coords, area, confidence) in enumerate(valid_boxes):
                print(f"[YOLO] æ¡† {i+1}: {bbox_coords}, ç½®ä¿¡åº¦: {confidence:.3f}, é¢ç§¯: {area} åƒç´ ")
        else:
            print("[YOLO] æ²¡æœ‰æ»¡è¶³é¢ç§¯è¦æ±‚çš„è¾¹ç•Œæ¡†")

        return boxes
    

    def generate_pointcloud(self, color_image, depth_image, mask):
        """
        ä»æ©ç ç”Ÿæˆ3Dç‚¹äº‘
        
        Args:
            color_image: RGBå›¾åƒ
            depth_image: æ·±åº¦å›¾åƒ (æ¯«ç±³)
            mask: åˆ†å‰²æ©ç 
            
        Returns:
            points: 3Dç‚¹åæ ‡
            colors: RGBé¢œè‰²
        """
        try:
            # è½¬æ¢æ·±åº¦å›¾åƒå•ä½ä¸ºç±³
            depth_image_meters = depth_image.astype(np.float32) / 1000.0
            
            # è½¬æ¢ä¸ºRGBæ ¼å¼
            color_image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)
            
            # ä½¿ç”¨mask_to_3d_pointcloudå‡½æ•°ï¼ˆæ”¯æŒç•¸å˜æ ¡æ­£ï¼‰
            points, colors = mask_to_3d_pointcloud(
                color_image_rgb, 
                depth_image_meters, 
                mask, 
                self.fx, self.fy, self.cx, self.cy,
                self.mtx, self.dist
            )
            
            return points, colors
            
        except Exception as e:
            print(f"ç”Ÿæˆç‚¹äº‘æ—¶å‡ºé”™: {e}")
            return np.array([]), np.array([])

    def apply_hand_eye_transform(self, points):
        """ä½¿ç”¨ util.apply_hand_eye_transform åº”ç”¨æ‰‹çœ¼æ ‡å®šå˜æ¢"""
        return util_apply_hand_eye_transform(points, self.hand_eye_transform)
    
    def pixel_to_3d_camera(self, u, v, depth_mm):
        """
        å°†2Dåƒç´ åæ ‡å’Œæ·±åº¦å€¼è½¬æ¢ä¸º3Dç›¸æœºåæ ‡ç³»åæ ‡
        
        Args:
            u: åƒç´ xåæ ‡
            v: åƒç´ yåæ ‡
            depth_mm: æ·±åº¦å€¼ï¼ˆæ¯«ç±³ï¼‰
        
        Returns:
            point_3d: 3Dç‚¹åæ ‡ (X, Y, Z) å•ä½ï¼šç±³ï¼ˆç›¸æœºåæ ‡ç³»ï¼‰
        """
        # è½¬æ¢æ·±åº¦å•ä½ä¸ºç±³
        z_m = depth_mm / 1000.0
        
        # ä½¿ç”¨ç›¸æœºå†…å‚å°†åƒç´ åæ ‡è½¬æ¢ä¸º3Dåæ ‡
        # X = (u - cx) / fx * z
        # Y = (v - cy) / fy * z
        # Z = z
        X = (u - self.cx) / self.fx * z_m
        Y = (v - self.cy) / self.fy * z_m
        Z = z_m
        
        return np.array([X, Y, Z], dtype=np.float32)
    
    def scale_bbox(self, x1, y1, x2, y2, image_height, image_width):
        """
        ä»ä¸­å¿ƒç‚¹ç¼©æ”¾è¾¹ç•Œæ¡†
        
        Args:
            x1, y1, x2, y2: åŸå§‹è¾¹ç•Œæ¡†åæ ‡
            image_height: å›¾åƒé«˜åº¦
            image_width: å›¾åƒå®½åº¦
        
        Returns:
            scaled_x1, scaled_y1, scaled_x2, scaled_y2: ç¼©æ”¾åçš„è¾¹ç•Œæ¡†åæ ‡
        """
        if self.bbox_scale == 1.0:
            return x1, y1, x2, y2
        
        # è®¡ç®—ä¸­å¿ƒç‚¹å’Œå°ºå¯¸
        center_x = (x1 + x2) / 2.0
        center_y = (y1 + y2) / 2.0
        width = x2 - x1
        height = y2 - y1
        
        # ç¼©æ”¾å°ºå¯¸
        new_width = width * self.bbox_scale
        new_height = height * self.bbox_scale
        
        # è®¡ç®—æ–°çš„è¾¹ç•Œæ¡†åæ ‡
        scaled_x1 = int(center_x - new_width / 2.0)
        scaled_y1 = int(center_y - new_height / 2.0)
        scaled_x2 = int(center_x + new_width / 2.0)
        scaled_y2 = int(center_y + new_height / 2.0)
        
        # è£å‰ªåˆ°å›¾åƒèŒƒå›´å†…
        scaled_x1 = max(0, min(scaled_x1, image_width - 1))
        scaled_y1 = max(0, min(scaled_y1, image_height - 1))
        scaled_x2 = max(0, min(scaled_x2, image_width - 1))
        scaled_y2 = max(0, min(scaled_y2, image_height - 1))
        
        # ç¡®ä¿ x2 > x1 å’Œ y2 > y1
        if scaled_x2 <= scaled_x1:
            scaled_x2 = scaled_x1 + 1
        if scaled_y2 <= scaled_y1:
            scaled_y2 = scaled_y1 + 1
        
        if self.debug and self.bbox_scale != 1.0:
            print(f"[ç¼©æ”¾] åŸå§‹bbox: ({x1}, {y1}, {x2}, {y2}), ç¼©æ”¾å: ({scaled_x1}, {scaled_y1}, {scaled_x2}, {scaled_y2}), ç¼©æ”¾å› å­: {self.bbox_scale}")
        
        return scaled_x1, scaled_y1, scaled_x2, scaled_y2
    
    def erode_mask_vertical(self, mask):
        """
        å¯¹maskè¿›è¡Œä¸Šä¸‹æ–¹å‘çš„è…èš€ï¼Œå»é™¤è¾¹ç•ŒåŒºåŸŸä»¥è·å¾—æ›´ç²¾ç¡®çš„è´¨å¿ƒ
        
        Args:
            mask: äºŒå€¼maskï¼ˆnumpyæ•°ç»„ï¼Œ0/255æ ¼å¼ï¼‰
        
        Returns:
            eroded_mask: è…èš€åçš„mask
        """
        if not self.erode_bbox:
            return mask
        
        try:
            # è½¬æ¢ä¸ºäºŒå€¼æ ¼å¼ï¼ˆ0/1ï¼‰
            mask_bool = (mask > 0).astype(np.uint8)
            
            # æ‰¾åˆ°maskçš„æœ‰æ•ˆåŒºåŸŸï¼ˆéé›¶åŒºåŸŸï¼‰
            ys, xs = np.where(mask_bool > 0)
            if ys.size == 0 or xs.size == 0:
                return mask
            
            y_min = int(ys.min())
            y_max = int(ys.max())
            height = y_max - y_min + 1
            
            # è®¡ç®—ä¸Šä¸‹å„è…èš€çš„åƒç´ æ•°
            erode_pixels = int(height * self.erode_ratio)
            
            if erode_pixels > 0 and height > erode_pixels * 2:
                # åˆ›å»ºè…èš€åçš„mask
                eroded_mask = np.zeros_like(mask_bool)
                
                # åªä¿ç•™ä¸­é—´éƒ¨åˆ†ï¼ˆå»é™¤ä¸Šä¸‹å„10%ï¼‰
                y_start = y_min + erode_pixels
                y_end = y_max - erode_pixels + 1
                
                # å¤åˆ¶ä¸­é—´éƒ¨åˆ†
                eroded_mask[y_start:y_end, :] = mask_bool[y_start:y_end, :]
                
                # è½¬æ¢å›0/255æ ¼å¼
                eroded_mask = eroded_mask.astype(np.uint8) * 255
                
                if self.debug:
                    print(f"[è…èš€] åŸå§‹é«˜åº¦: {height}, è…èš€åƒç´ : {erode_pixels}, ä¿ç•™é«˜åº¦: {y_end - y_start}")
                
                return eroded_mask
            else:
                # å¦‚æœmaskå¤ªå°ï¼Œä¸è¿›è¡Œè…èš€
                if self.debug:
                    print(f"[è…èš€] Maskå¤ªå°ï¼ˆé«˜åº¦={height}ï¼‰ï¼Œè·³è¿‡è…èš€")
                return mask
                
        except Exception as e:
            print(f"[è…èš€] è…èš€maskå¤±è´¥: {e}ï¼Œè¿”å›åŸå§‹mask")
            return mask

    def _rpy_to_rotation_matrix(self, rx, ry, rz):
        # ä¿ç•™å…¼å®¹æ–¹æ³•ä½†å§”æ‰˜åˆ° utilï¼ˆå¦‚åç»­ç›´æ¥è°ƒç”¨ utilï¼Œå¯åˆ é™¤æ­¤æ–¹æ³•ï¼‰
        from util import rpy_to_rotation_matrix
        return rpy_to_rotation_matrix(rx, ry, rz)

    def _tool_offset_to_base(self, delta_tool_xyz_mm, tcp_rpy):
        # ä¿ç•™å…¼å®¹æ–¹æ³•ä½†å§”æ‰˜åˆ° util
        dx, dy, dz = util_tool_offset_to_base(delta_tool_xyz_mm, tcp_rpy)
        return [dx, dy, dz]

    def calculate_pointcloud_bbox(self, points):
        from point_cloud_utils import calculate_pointcloud_bbox
        return calculate_pointcloud_bbox(points)

    def calculate_surface_normal(self, points, method='pca'):
        from point_cloud_utils import calculate_surface_normal
        return calculate_surface_normal(points, method)

    def _simple_plane_fitting(self, points, centroid):
        from point_cloud_utils import _simple_plane_fitting
        return _simple_plane_fitting(points, centroid)

    def _nearest_neighbors_normal(self, points, centroid, k=20):
        from point_cloud_utils import _nearest_neighbors_normal
        return _nearest_neighbors_normal(points, centroid, k)

    def normal_to_rpy(self, normal_vector, current_rpy=None):
        from pose import normal_to_rpy as pose_normal_to_rpy
        return pose_normal_to_rpy(normal_vector, current_rpy)

    def _smooth_rpy_transition(self, current_rpy, target_rpy, max_change=0.1):
        from pose import smooth_rpy_transition
        return smooth_rpy_transition(current_rpy, target_rpy, max_change)

    def estimate_fish_weight(self, points_gripper, volume_factor: float = 1.0) -> float:
        from util import estimate_fish_weight
        return estimate_fish_weight(points_gripper, volume_factor)

    def calculate_grasp_pose_with_normal(self, points_gripper, current_tcp):
        from pose import calculate_grasp_pose_with_normal as pose_calculate_grasp_pose_with_normal
        return pose_calculate_grasp_pose_with_normal(points_gripper, current_tcp)
    
    
    def show_preview(self, color_image, depth_image, mask, detection_vis=None, landmark_vis=None):
        """
        åœ¨ä¸€ä¸ªçª—å£ä¸­æ˜¾ç¤º2x2ç½‘æ ¼ï¼šRGBã€æ£€æµ‹ã€åˆ†å‰²å’Œå…³é”®ç‚¹é¢„æµ‹ç»“æœ
        """
 
        # è°ƒæ•´å›¾åƒå¤§å°
        display_size = (640, 480)
        
        # 1. RGBå›¾åƒ
        rgb_display = cv2.resize(color_image, display_size)
        
        # 2. æ£€æµ‹å¯è§†åŒ–ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼Œä½¿ç”¨RGBå›¾åƒï¼‰
        if detection_vis is not None:
            detection_display = cv2.resize(detection_vis, display_size)
        else:
            detection_display = rgb_display.copy()
        
        # 3. åˆ†å‰²å¯è§†åŒ–
        if mask is not None:
            # å°†æ©ç è½¬æ¢ä¸ºå½©è‰²å›¾åƒ
            mask_colored = np.zeros_like(color_image)
            mask_colored[mask > 0] = [0, 255, 0]  # ç»¿è‰²æ©ç 
            # å åŠ åˆ°åŸå›¾ä¸Š
            segmentation_vis = cv2.addWeighted(color_image, 0.7, mask_colored, 0.3, 0)
        else:
            segmentation_vis = color_image.copy()
        seg_display = cv2.resize(segmentation_vis, display_size)
        
        # 4. å…³é”®ç‚¹é¢„æµ‹å¯è§†åŒ–ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼Œä½¿ç”¨RGBå›¾åƒï¼‰
        if landmark_vis is not None:
            landmark_display = cv2.resize(landmark_vis, display_size)
        else:
            landmark_display = rgb_display.copy()
        
        # åˆ›å»º2x2ç½‘æ ¼
        # ç¬¬ä¸€è¡Œï¼šRGB | æ£€æµ‹
        top_row = np.hstack((rgb_display, detection_display))
        # ç¬¬äºŒè¡Œï¼šåˆ†å‰² | å…³é”®ç‚¹
        bottom_row = np.hstack((seg_display, landmark_display))
        # å‚ç›´æ‹¼æ¥
        combined = np.vstack((top_row, bottom_row))
        
        # æ·»åŠ æ ‡ç­¾
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.8
        font_thickness = 2
        text_color = (0, 255, 0)
        
        # ç¬¬ä¸€è¡Œæ ‡ç­¾
        cv2.putText(combined, "RGB", (15, 45), font, font_scale, text_color, font_thickness)
        cv2.putText(combined, "Detection", (615, 45), font, font_scale, text_color, font_thickness)
        
        # ç¬¬äºŒè¡Œæ ‡ç­¾
        cv2.putText(combined, "Segmentation", (15, 495), font, font_scale, text_color, font_thickness)
        cv2.putText(combined, "Landmarks", (615, 495), font, font_scale, text_color, font_thickness)
        
        # æ·»åŠ å¸§è®¡æ•°
        cv2.putText(combined, f"Frame: {self.frame_count}", (10, combined.shape[0] - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        cv2.imshow('Real-time Processing (2x2 Grid)', combined)

    
    def run_realtime(self, max_frames=None, show_preview=True):
        """
        è¿è¡Œå®æ—¶å¤„ç†
        
        Args:
            max_frames: æœ€å¤§å¸§æ•° (Noneè¡¨ç¤ºæ— é™)
            show_preview: æ˜¯å¦æ˜¾ç¤ºé¢„è§ˆçª—å£
        """
        print("å¼€å§‹å®æ—¶å¤„ç†...")
        print("æŒ‰ 'q' é”®åœæ­¢")
        print("æŒ‰ç©ºæ ¼é”®æš‚åœ/ç»§ç»­")
        if self.fish_tracker is not None:
            print("æŒ‰ 'r' é”®é‡ç½®å®¹å™¨")
            print("æŒ‰ 's' é”®æ˜¾ç¤ºçŠ¶æ€")
            print("æŒ‰ 'e' é”®å¯¼å‡ºæ•°æ®")
        
        # æš‚åœæ ‡å¿—
        paused = False
        
        # éªŒè¯ç›¸æœºè¿æ¥
        print("[è°ƒè¯•] å¼€å§‹éªŒè¯ç›¸æœºè¿æ¥...")
        if not self.validate_camera_connection():
            print("âŒ ç›¸æœºè¿æ¥éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸æœºè¿æ¥åé‡è¯•")
            return
        print("[è°ƒè¯•] ç›¸æœºè¿æ¥éªŒè¯é€šè¿‡")

        try:
            if self.robot_config and os.path.exists(self.robot_config):
                with open(self.robot_config, 'r', encoding='utf-8') as f:
                    robot_conf = json.load(f)
                init = robot_conf.get('initial_pose', {})
                pose_type = init.get('type', 'joint')
                mode = int(init.get('mode', 0))
                blocking = bool(init.get('blocking', True))
                speed = float(init.get('speed', 1))
                if pose_type == 'joint':
                    vals_deg = init.get('values_deg')
                    vals_rad = init.get('values_rad')
                    if vals_rad is not None:
                        joints = [float(x) for x in vals_rad]
                    elif vals_deg is not None:
                        joints = [float(x) * np.pi / 180.0 for x in vals_deg]
                    else:
                        joints = None
                    if joints is not None and len(joints) == 6:
                        ret = self.robot.joint_move(joints, mode, blocking, speed)
                        time.sleep(0.2)
                elif pose_type == 'tcp':
                    tcp = init.get('values_mmrad')  # [x,y,z,rx,ry,rz]
                    if tcp is not None and len(tcp) == 6:
                        ret = self.robot.linear_move(tcp, mode, blocking, speed)
                        time.sleep(0.2)
        except Exception as e:
            print(f"åŠ è½½/ç§»åŠ¨åˆå§‹ä½å§¿å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

        try:
            tcp_result = self.robot.get_tcp_position()
            joint_result = self.robot.get_joint_position()
            original_joint = []
            if isinstance(tcp_result, tuple) and len(tcp_result) == 2:
                tcp_ok, original_tcp = tcp_result
                original_joint =  joint_result[1]
            else:
                # å¦‚æœåªè¿”å›ä¸€ä¸ªå€¼ï¼Œå‡è®¾å®ƒæ˜¯ä½ç½®ä¿¡æ¯
                original_tcp = tcp_result
                tcp_ok = True
            print(f"[è°ƒè¯•] TCPä½ç½®è·å–æˆåŠŸ: {original_tcp}")
            print(f"[è°ƒè¯•] Jointä½ç½®è·å–æˆåŠŸ: {original_joint}")
        except Exception as e:
            print(f"[é”™è¯¯] è·å–TCPä½ç½®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return  # å¦‚æœæ— æ³•è·å–TCPä½ç½®ï¼Œæå‰è¿”å›

        print("[è°ƒè¯•] å¼€å§‹åŠ è½½é±¼è·¯å¾„é…ç½®...")
        fish_count = 0 # count the number of fish in the container
        rows = -1
        cols = -1
        fish_path_json = {}
        
        # å§‹ç»ˆä» fish_grid_params.json åŠ è½½é…ç½®
        grid_params_path = "configs/fish_grid_params.json"
        print(f"[è°ƒè¯•] ä» fish_grid_params.json åŠ è½½é…ç½®...")
        
        try:
            with open(grid_params_path, 'r', encoding='utf-8') as f:
                grid_params = json.load(f)
            
            # ä»é…ç½®æ–‡ä»¶è·å–è·¯å¾„æ–‡ä»¶è·¯å¾„
            fish_paths_path = grid_params.get('output', {}).get('waypoints_json_path')
            if not fish_paths_path:
                raise ValueError("fish_grid_params.json ä¸­æœªæ‰¾åˆ° 'output.waypoints_json_path' å­—æ®µ")
            print(f"[è°ƒè¯•] ä» fish_grid_params.json è¯»å–è·¯å¾„: {fish_paths_path}")
            
            # è·å–ç½‘æ ¼é…ç½®
            grid = grid_params.get('grid', {})
            rows = int(grid.get('rows', 0))
            cols = int(grid.get('cols', 0))
            print(f"[è°ƒè¯•] ç½‘æ ¼é…ç½®: rows={rows}, cols={cols}")
            
            # åŠ è½½è·¯å¾„æ–‡ä»¶
            print(f"[è°ƒè¯•] è¯»å–é±¼è·¯å¾„æ–‡ä»¶: {fish_paths_path}")
            with open(fish_paths_path, 'r', encoding='utf-8') as f:
                fish_path_json = json.load(f)
            print(f"[è°ƒè¯•] æˆåŠŸåŠ è½½é±¼è·¯å¾„é…ç½®ï¼ŒåŒ…å« {len(fish_path_json)} ä¸ªä½ç½®")
            
            # å¦‚æœ rows æˆ– cols æ— æ•ˆï¼Œå°è¯•ä»è·¯å¾„æ–‡ä»¶æ¨æ–­
            if rows <= 0 or cols <= 0:
                total_positions = len(fish_path_json)
                if rows <= 0 and cols > 0:
                    rows = max(1, total_positions // cols)
                elif cols <= 0 and rows > 0:
                    cols = max(1, total_positions // rows)
                else:
                    # å¦‚æœéƒ½ä¸çŸ¥é“ï¼Œå‡è®¾æ˜¯å•åˆ—
                    rows = total_positions
                    cols = 1
                print(f"[è°ƒè¯•] ä»è·¯å¾„æ–‡ä»¶æ¨æ–­ rows:{rows}, cols:{cols}")
            
        except FileNotFoundError:
            print(f"[é”™è¯¯] é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {grid_params_path}")
            print(f"[é”™è¯¯] è¯·ç¡®ä¿ {grid_params_path} æ–‡ä»¶å­˜åœ¨")
            import traceback
            traceback.print_exc()
        except KeyError as e:
            print(f"[é”™è¯¯] fish_grid_params.json ä¸­ç¼ºå°‘å¿…è¦çš„å­—æ®µ: {e}")
            import traceback
            traceback.print_exc()
        except Exception as e:
            print(f"[é”™è¯¯] åŠ è½½é…ç½®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
        try:
            while True:
                # å¤„ç†é”®ç›˜è¾“å…¥ï¼ˆåŒ…æ‹¬æš‚åœçŠ¶æ€ä¸‹çš„è¾“å…¥ï¼‰
                if show_preview:
                    key = cv2.waitKey(1) & 0xFF
                    if key == ord(' '):  # ç©ºæ ¼é”®
                        paused = not paused
                        if paused:
                            print("â¸ï¸  å·²æš‚åœ - æŒ‰ç©ºæ ¼é”®ç»§ç»­")
                        else:
                            print("â–¶ï¸  å·²ç»§ç»­")
                    elif key == ord('q'):
                        print("ç”¨æˆ·æŒ‰ 'q' é”®åœæ­¢")
                        break
                    elif key == ord('r') and self.fish_tracker is not None:
                        print("ç”¨æˆ·æŒ‰ 'r' é”®é‡ç½®å®¹å™¨")
                        self.fish_tracker.reset_container(confirm=True)
                    elif key == ord('s') and self.fish_tracker is not None:
                        print("ç”¨æˆ·æŒ‰ 's' é”®æ˜¾ç¤ºçŠ¶æ€")
                        self.fish_tracker.print_status()
                    elif key == ord('e') and self.fish_tracker is not None:
                        print("ç”¨æˆ·æŒ‰ 'e' é”®å¯¼å‡ºæ•°æ®")
                        self.fish_tracker.export_data()
                
                # å¦‚æœæš‚åœï¼Œè·³è¿‡å¤„ç†ä½†ç»§ç»­ç›‘å¬é”®ç›˜
                if paused:
                    continue
                
                # æ•´ä¸ªå¾ªç¯è®¡æ—¶å¼€å§‹
                cycle_start = time.time()
                
                # æ•è·å¸§ï¼ˆä½¿ç”¨é‡è¯•æœºåˆ¶ï¼‰
                color_image, depth_image, success = self.capture_frames() #self.capture_frames_with_retry(max_retries=3, timeout_ms=10000)
                if not success:
                    print("âš ï¸  è·³è¿‡æ­¤å¸§ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€å¸§...")
                    continue
                
                self.frame_count = self.frame_count + 1
                        # è¿™é‡Œå¯ä»¥æ·»åŠ é‡æ–°è¿æ¥é€»è¾‘
                if self.frame_count < 10 :
                    print(f"è·³è¿‡å‰10å¸§ï¼Œç­‰å¾…ç›¸æœºç¨³å®š...")
                    if self.frame_count == 9 and show_preview:
                        self.show_preview(color_image, depth_image, None, None, None)
                        cv2.waitKey(1)
                    continue

                # æ£€æµ‹ + åˆ†å‰² + è½ç›˜ï¼ˆæœ€è¿‘ç›®æ ‡é€‰æ‹©ï¼‰
                mask_vis, base_name = self.detect_and_segment_and_dump_all(color_image, depth_image)
                
                # ä¿å­˜RGBå’Œæ·±åº¦å›¾åƒï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹ï¼‰
                if self.debug and base_name is not None:
                    # ä¿å­˜RGBå›¾åƒ
                    rgb_path = os.path.join(self.rgb_dir, f"{base_name}.png")
                    cv2.imwrite(rgb_path, color_image)
                    
                    # ä¿å­˜æ·±åº¦å›¾åƒï¼ˆåŸå§‹16ä½ï¼‰
                    depth_path = os.path.join(self.depth_dir, f"{base_name}.png")
                    cv2.imwrite(depth_path, depth_image.astype(np.uint16))
                    
                    # ä¿å­˜å¯è§†åŒ–æ·±åº¦å›¾åƒï¼ˆ8ä½å½©è‰²ï¼‰
                    valid_depth = depth_image > 0
                    if valid_depth.any():
                        depth_min = depth_image[valid_depth].min()
                        depth_max = depth_image[valid_depth].max()
                        depth_normalized = np.zeros_like(depth_image, dtype=np.uint8)
                        if depth_max > depth_min:
                            depth_normalized[valid_depth] = ((depth_image[valid_depth] - depth_min) / (depth_max - depth_min) * 255).astype(np.uint8)
                        depth_colormap = cv2.applyColorMap(depth_normalized, cv2.COLORMAP_JET)
                        depth_vis_path = os.path.join(self.depth_dir, f"{base_name}_visualization.png")
                        cv2.imwrite(depth_vis_path, depth_colormap)

                # ç”Ÿæˆæ£€æµ‹å¯è§†åŒ–
                detection_vis = None
                if mask_vis is not None:
                    # é‡æ–°è¿è¡Œæ£€æµ‹ä»¥è·å–è¾¹ç•Œæ¡†å¯è§†åŒ–
                    #if getattr(self, 'use_yolo', False):
                    boxes = self.detect_yolo(color_image, self.yolo_weights, conf=0.6, iou=0.45, imgsz=640)
                  
                    if boxes:
                        detection_vis = color_image.copy()
                        # ç»˜åˆ¶æ‰€æœ‰æ£€æµ‹æ¡†
                        for i, box in enumerate(boxes):
                            if len(box) >= 4:
                                x1, y1, x2, y2 = box[:4]
                                color = (0, 255, 0) if i == 0 else (0, 255, 255)  # ç¬¬ä¸€ä¸ªæ¡†ç”¨ç»¿è‰²ï¼Œå…¶ä»–ç”¨é»„è‰²
                                thickness = 3 if i == 0 else 2
                                cv2.rectangle(detection_vis, (x1, y1), (x2, y2), color, thickness)
                                if len(box) >= 5:
                                    confidence = box[4]
                                    cv2.putText(detection_vis, f"{confidence:.2f}", (x1, y1-10), 
                                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
                
                # ç”Ÿæˆå…³é”®ç‚¹/æ–¹å‘å¯è§†åŒ–
                landmark_vis = None
                if (self.grasp_point_mode == "ai" and self.landmark_detector is not None and 
                    mask_vis is not None):
                    try:
                        # æ ¹æ®æ©ç è®¡ç®—å¤–æ¥çŸ©å½¢ï¼Œå¾—åˆ°é±¼çš„è£å‰ªåŒºåŸŸ
                        ys, xs = np.where(mask_vis > 0)
                        if ys.size > 0 and xs.size > 0:
                            x1, y1 = int(xs.min()), int(ys.min())
                            x2, y2 = int(xs.max())+1, int(ys.max())+1
                            crop_bgr = color_image[y1:y2, x1:x2]
                            crop_rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)
                            
                            # é¢„æµ‹å…³é”®ç‚¹
                            pred_landmarks, pred_visibility = self.landmark_detector.predict(crop_rgb)
                            
                            # å¯è§†åŒ–å…³é”®ç‚¹
                            landmark_vis_crop = self.landmark_detector.visualize_landmarks(crop_rgb, pred_landmarks, pred_visibility)
                            
                            # å°†è£å‰ªåŒºåŸŸçš„å¯è§†åŒ–ç»“æœæ”¾å›åŸå›¾
                            landmark_vis = color_image.copy()
                            landmark_vis_bgr = cv2.cvtColor(landmark_vis_crop, cv2.COLOR_RGB2BGR)
                            landmark_vis[y1:y2, x1:x2] = landmark_vis_bgr
                            
                            # ç»˜åˆ¶è¾¹ç•Œæ¡†
                            cv2.rectangle(landmark_vis, (x1, y1), (x2, y2), (255, 0, 0), 2)
                    except Exception as e:
                        print(f"[å¯è§†åŒ–] å…³é”®ç‚¹é¢„æµ‹å¯è§†åŒ–å¤±è´¥: {e}")

                # è‹¥æœªç”Ÿæˆå…³é”®ç‚¹å¯è§†åŒ–ï¼Œåˆ™å¯è§†åŒ–ä¸»æ–¹å‘ï¼ˆPCAï¼‰
                if landmark_vis is None and mask_vis is not None:
                    try:
                        landmark_vis = draw_principal_axis(color_image, mask_vis > 0)
                    except Exception:
                        landmark_vis = None
                
                # åº”ç”¨å‚ç›´è…èš€ï¼ˆå¦‚æœå¯ç”¨ï¼‰- å¯¹æœ€ç»ˆé€‰æ‹©çš„maskè¿›è¡Œè…èš€
                mask_vis_eroded = mask_vis
                if mask_vis is not None and self.erode_bbox:
                    mask_vis_eroded = self.erode_mask_vertical(mask_vis.copy())
                
                # è®¡ç®—2Dè´¨å¿ƒï¼ˆç”¨äºåç»­3Dè½¬æ¢ï¼‰
                centroid_2d = None
                if mask_vis_eroded is not None:
                    try:
                        # è®¡ç®—æ©ç çš„è´¨å¿ƒï¼ˆä½¿ç”¨è…èš€åçš„maskï¼‰
                        ys, xs = np.where(mask_vis_eroded > 0)
                        if ys.size > 0 and xs.size > 0:
                            centroid_x = int(np.mean(xs))
                            centroid_y = int(np.mean(ys))
                            centroid_2d = (centroid_x, centroid_y)
                    except Exception as e:
                        print(f"[è®¡ç®—] è®¡ç®—2Dè´¨å¿ƒå¤±è´¥: {e}")
                
                # åœ¨landmark_visä¸Šç»˜åˆ¶è´¨å¿ƒåå­—æ ‡è®°
                if landmark_vis is not None and centroid_2d is not None:
                    try:
                        centroid_x, centroid_y = centroid_2d
                        # ç»˜åˆ¶åå­—æ ‡è®°
                        cross_size = 25
                        cross_thickness = 4
                        cross_color = (0, 255, 255)  # é»„è‰² (BGR)
                        
                        # æ°´å¹³çº¿
                        cv2.line(landmark_vis, 
                               (centroid_x - cross_size, centroid_y), 
                               (centroid_x + cross_size, centroid_y), 
                               cross_color, cross_thickness)
                        # å‚ç›´çº¿
                        cv2.line(landmark_vis, 
                               (centroid_x, centroid_y - cross_size), 
                               (centroid_x, centroid_y + cross_size), 
                               cross_color, cross_thickness)
                    except Exception as e:
                        print(f"[å¯è§†åŒ–] ç»˜åˆ¶è´¨å¿ƒæ ‡è®°å¤±è´¥: {e}")
                
                # æ˜¾ç¤ºé¢„è§ˆçª—å£
                self.show_preview(color_image, depth_image, mask_vis, detection_vis, landmark_vis)
                
                # ç¡®ä¿çª—å£æ˜¾ç¤ºå¹¶å¤„ç†æŒ‰é”®ï¼ˆéæš‚åœçŠ¶æ€ä¸‹çš„é¢å¤–æŒ‰é”®å¤„ç†ï¼‰
                if show_preview:
                    key = cv2.waitKey(1) & 0xFF
                    if key == ord(' '):  # ç©ºæ ¼é”®
                        paused = not paused
                        if paused:
                            print("â¸ï¸  å·²æš‚åœ - æŒ‰ç©ºæ ¼é”®ç»§ç»­")
                        else:
                            print("â–¶ï¸  å·²ç»§ç»­")
                    elif key == ord('q'):
                        print("ç”¨æˆ·æŒ‰ 'q' é”®åœæ­¢")
                        break
                    elif key == ord('r') and self.fish_tracker is not None:
                        print("ç”¨æˆ·æŒ‰ 'r' é”®é‡ç½®å®¹å™¨")
                        self.fish_tracker.reset_container(confirm=True)
                    elif key == ord('s') and self.fish_tracker is not None:
                        print("ç”¨æˆ·æŒ‰ 's' é”®æ˜¾ç¤ºçŠ¶æ€")
                        self.fish_tracker.print_status()
                    elif key == ord('e') and self.fish_tracker is not None:
                        print("ç”¨æˆ·æŒ‰ 'e' é”®å¯¼å‡ºæ•°æ®")
                        self.fish_tracker.export_data()

                # æ ¹æ®æ©ç ç”Ÿæˆ3Dç‚¹äº‘å¹¶ä¿å­˜ï¼ˆå¯é€‰åº”ç”¨æ‰‹çœ¼æ ‡å®šï¼‰
                # æ³¨æ„ï¼šç°åœ¨ä¸»è¦ç”¨äºå¯è§†åŒ–ï¼Œ3Dè´¨å¿ƒè®¡ç®—æ”¹ä¸ºä½¿ç”¨2Dè´¨å¿ƒ+æ·±åº¦
                points_gripper = None  # åˆå§‹åŒ–å˜é‡
                if mask_vis is not None and base_name is not None:
                    # ç‚¹äº‘ç”Ÿæˆè®¡æ—¶ï¼ˆä»…ç”¨äºå¯è§†åŒ–ï¼Œä¸ç”¨äºè´¨å¿ƒè®¡ç®—ï¼‰
                    pointcloud_start = time.time()
                    mask_bool = (mask_vis > 0)
                    points, colors = self.generate_pointcloud(color_image, depth_image, mask_bool)
                    pointcloud_time = time.time() - pointcloud_start
                    self.timers['pointcloud_generation'].append(pointcloud_time)
                    print(f"â±ï¸  pointcloud_generation: {pointcloud_time:.3f}s")
                    if len(points) > 0:
                        # åº”ç”¨æ‰‹çœ¼å˜æ¢ï¼šç›¸æœºâ†’å¤¹çˆª
                        points_gripper = self.apply_hand_eye_transform(points)
                        
                        # ä¿å­˜ç‚¹äº‘ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹ï¼‰
                        # if self.debug:
                        #     # ä¿å­˜ç›¸æœºåæ ‡ç³»ç‚¹äº‘
                        #     # cam_ply = os.path.join(self.pointcloud_dir, f"{base_name}_cam_pointcloud.ply")
                        #     # save_pointcloud_to_file(points, colors, cam_ply)
                        #     # ä¿å­˜å¤¹çˆªåæ ‡ç³»ç‚¹äº‘
                        #     grip_ply = os.path.join(self.pointcloud_dir, f"{base_name}_gripper_pointcloud.ply")
                        #     save_pointcloud_to_file(points_gripper, colors, grip_ply)
                
                # don't forget to transform the units, the point cloud is in meter, but robot
                # control would like to be in mm. 

                # ä½¿ç”¨2Dè´¨å¿ƒ+æ·±åº¦è®¡ç®—3Dè´¨å¿ƒï¼ˆåœ¨å¤¹çˆªåæ ‡ç³»ä¸­ï¼‰
                if centroid_2d is not None and mask_vis is not None:
                    # æŠ“å–ç‚¹è®¡ç®—è®¡æ—¶
                    grasp_calc_start = time.time()
                    
                    # è·å–å½“å‰æœºå™¨äººTCPä½ç½®
                    tcp_result = self.robot.get_tcp_position()
                    if isinstance(tcp_result, tuple) and len(tcp_result) == 2:
                        tcp_ok, current_tcp = tcp_result
                    else:
                        # å¦‚æœåªè¿”å›ä¸€ä¸ªå€¼ï¼Œå‡è®¾å®ƒæ˜¯ä½ç½®ä¿¡æ¯
                        current_tcp = tcp_result
                        tcp_ok = True
                    print(f"å½“å‰TCPä½ç½®: {current_tcp}")
                    
                    # æ£€æŸ¥å®¹å™¨æ˜¯å¦å·²æ»¡
                    if self.fish_tracker is not None and self.fish_tracker.is_container_full():
                        print("ğŸ“¦ å®¹å™¨å·²æ»¡ï¼åœæ­¢æŠ“å–æ–°é±¼ã€‚")
                        print("æŒ‰ 'r' é”®é‡ç½®å®¹å™¨ï¼ŒæŒ‰ 'q' é”®é€€å‡º")
                        # æ˜¾ç¤ºçŠ¶æ€
                        self.fish_tracker.print_status()
                        continue
                    
                    # è®¡ç®—æŠ“å–ç‚¹ï¼ˆä¼˜å…ˆAIï¼‰
                    relative_move = None
                    angle_rad = 0
                    alpha_1 = None  # angle between headâ†’body (image) and vertical (image y-axis)
                    # å…ˆç”¨æ©ç åŸºäºPCAä¼°è®¡ä¸€ä¸ª alpha_1 ä½œä¸ºé»˜è®¤å€¼
                    try:
                        if mask_vis is not None:
                            alpha_1 = estimate_body_angle_alpha1(mask_vis > 0)
                            print(f"[PCA] ä¼°è®¡ alpha_1(rad) = {alpha_1:.4f}, deg = {np.degrees(alpha_1):.2f}")
                    except Exception as e:
                        print(f"[PCA] ä¼°è®¡ alpha_1 å¤±è´¥: {e}")

                    if self.grasp_point_mode == "ai" and self.landmark_detector is not None and mask_vis is not None:
                        try:
                            # æ ¹æ®æ©ç è®¡ç®—å¤–æ¥çŸ©å½¢ï¼Œå¾—åˆ°é±¼çš„è£å‰ªåŒºåŸŸ
                            ys, xs = np.where(mask_vis > 0)
                            if ys.size > 0 and xs.size > 0:
                                x1, y1 = int(xs.min()), int(ys.min())
                                x2, y2 = int(xs.max())+1, int(ys.max())+1
                                crop_bgr = color_image[y1:y2, x1:x2]
                                crop_rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)
                                # é¢„æµ‹å±€éƒ¨åæ ‡ç³»ä¸‹çš„ä¸¤ä¸ªå…³é”®ç‚¹ï¼ˆ0=body_center, 1=head_centerï¼‰
                                pred_landmarks, pred_visibility = self.landmark_detector.predict(crop_rgb)
                                if pred_landmarks.shape[0] >= 2:
                                    body_xy_local = pred_landmarks[0]
                                    head_xy_local = pred_landmarks[1]
                                else:
                                    # å…¼å®¹åªæœ‰ä¸€ä¸ªç‚¹çš„æƒ…å†µ
                                    body_xy_local = pred_landmarks[0]
                                    head_xy_local = pred_landmarks[0]

                                # æ˜ å°„å›å…¨å›¾åæ ‡ï¼ˆåƒç´ ï¼‰
                                u_body = float(x1 + body_xy_local[0])
                                v_body = float(y1 + body_xy_local[1])
                                u_head = float(x1 + head_xy_local[0])
                                v_head = float(y1 + head_xy_local[1])

                                # æ·±åº¦ï¼ˆç±³ï¼‰
                                z_body_m = float(depth_image[int(round(v_body)), int(round(u_body))]) / 1000.0 if 0 <= int(round(v_body)) < depth_image.shape[0] and 0 <= int(round(u_body)) < depth_image.shape[1] else 0.0
                                z_head_m = float(depth_image[int(round(v_head)), int(round(u_head))]) / 1000.0 if 0 <= int(round(v_head)) < depth_image.shape[0] and 0 <= int(round(u_head)) < depth_image.shape[1] else 0.0
                                if z_body_m <= 0:
                                    print(f"æ— æ•ˆèº«ä½“ä¸­å¿ƒæ·±åº¦: {z_body_m}")
                                    raise ValueError("æ— æ•ˆæ·±åº¦")

                                # åæŠ•å½±åˆ°ç›¸æœºåæ ‡ç³»ï¼ˆèº«ä½“ä¸­å¿ƒï¼‰
                                Xb = (u_body - self.cx) / self.fx * z_body_m
                                Yb = (v_body - self.cy) / self.fy * z_body_m
                                point_cam_body = np.array([[Xb, Yb, z_body_m]], dtype=np.float32)

                                # åæŠ•å½±åˆ°ç›¸æœºåæ ‡ç³»ï¼ˆå¤´éƒ¨ä¸­å¿ƒï¼Œå¦‚æ— æ•ˆæ·±åº¦åˆ™æ²¿ç”¨èº«ä½“æ·±åº¦ï¼‰
                                if z_head_m <= 0:
                                    z_head_m = z_body_m
                                Xh = (u_head - self.cx) / self.fx * z_head_m
                                Yh = (v_head - self.cy) / self.fy * z_head_m
                                point_cam_head = np.array([[Xh, Yh, z_head_m]], dtype=np.float32)

                                # ç›¸æœºâ†’å¤¹çˆª
                                point_grip_body = self.apply_hand_eye_transform(point_cam_body)[0]
                                point_grip_head = self.apply_hand_eye_transform(point_cam_head)[0]
                                body_grip_mm = point_grip_body * 1000.0
                                head_grip_mm = point_grip_head * 1000.0

                                # æ–¹å‘å‘é‡ï¼ˆå›¾åƒåæ ‡ç³»ï¼Œå•ä½å‘é‡ï¼‰ headâ†’body
                                dir_img = np.array([u_body - u_head, v_body - v_head], dtype=np.float32)
                                norm_img = np.linalg.norm(dir_img) + 1e-6
                                dir_img_unit = (dir_img / norm_img).tolist()

                                # ä¸å›¾åƒç«–ç›´è½´(å³yè½´)çš„å¤¹è§’ï¼šä½¿ç”¨ atan2(vx, vy)
                                # è¿”å›[-pi, pi] çš„æœ‰ç¬¦å·è§’åº¦
                                alpha_1 = float(np.arctan2(dir_img[0], dir_img[1]))

                                # æ–¹å‘å‘é‡ï¼ˆå¤¹çˆªåæ ‡ç³»XYï¼Œå•ä½å‘é‡ï¼Œmmï¼‰
                                dir_grip_xy = np.array([head_grip_mm[0] - body_grip_mm[0], head_grip_mm[1] - body_grip_mm[1]], dtype=np.float32)
                                norm_grip = np.linalg.norm(dir_grip_xy) + 1e-6
                                dir_grip_xy_unit = (dir_grip_xy / norm_grip).tolist()

                                # å½“å‰æŠ“å–æŒ‰èº«ä½“ä¸­å¿ƒ
                                delta_tool_mm = [body_grip_mm[0], body_grip_mm[1], body_grip_mm[2]]
                                delta_base_xyz = self._tool_offset_to_base(delta_tool_mm, current_tcp[3:6])
                                z_offset = -delta_tool_mm[2] - 25

                                print(f"ğŸ¯ ä½¿ç”¨AIèº«ä½“ä¸­å¿ƒ: uv=({u_body:.1f},{v_body:.1f}) -> grip(mm)={body_grip_mm}")
                                print(f"ğŸ“ å¤´éƒ¨ä¸­å¿ƒ: uv=({u_head:.1f},{v_head:.1f}) -> grip(mm)={head_grip_mm}")
                                print(f"ğŸ§­ æ–¹å‘(åƒç´ xy,å•ä½å‘é‡) headâ†’body = {dir_img_unit}")
                                print(f"ğŸ“ ä¸å›¾åƒç«–ç›´è½´çš„å¤¹è§’ alpha_1(rad) = {alpha_1:.4f}, deg = {np.degrees(alpha_1):.2f}")
                                print(f"ğŸ§­ æ–¹å‘(å¤¹çˆªXY,å•ä½å‘é‡) bodyâ†’head = {dir_grip_xy_unit}")
                                # ä¸Xè½´(1,0,0)çš„å¤¹è§’ï¼ˆå¼§åº¦ï¼‰ï¼Œå¹¶è§„èŒƒåŒ–åˆ° [-pi/2, pi/2]
                                # è¿™æ ·æ— è®ºé±¼ä½“åŸå§‹æœå‘å¦‚ä½•ï¼Œéƒ½ä¼šè¢«æ˜ å°„åˆ°â€œæœå‘+XåŠå¹³é¢â€çš„ç­‰æ•ˆå§¿æ€ï¼Œä¾¿äºç»Ÿä¸€æ”¾ç½®æ–¹å‘
                                angle_rad = float(np.arctan2(dir_grip_xy_unit[1], dir_grip_xy_unit[0]))
                                if angle_rad > np.pi/2:
                                    angle_rad -= np.pi
                                elif angle_rad < -np.pi/2:
                                    angle_rad += np.pi
                                
                                relative_move = [delta_base_xyz[0], delta_base_xyz[1], z_offset, 0, 0, 0]

                                print(f"ğŸ§® æ–¹å‘ä¸Xè½´çš„å¤¹è§’(rad): {angle_rad:.4f}")
                            else:
                                print("[AI] æ©ç ä¸ºç©ºï¼Œå›é€€è´¨å¿ƒ")
                        except Exception as e:
                            print(f"[AI] é¢„æµ‹èº«ä½“ä¸­å¿ƒå¤±è´¥ï¼Œå›é€€è´¨å¿ƒ: {e}")

                    # è‹¥AIæœªç”Ÿæˆç§»åŠ¨ï¼Œä½¿ç”¨2Dè´¨å¿ƒ+æ·±åº¦æ–¹æ¡ˆ
                    if relative_move is None:
                        try:
                            # è·å–2Dè´¨å¿ƒä½ç½®çš„æ·±åº¦å€¼
                            centroid_x, centroid_y = centroid_2d
                            
                            # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
                            h, w = depth_image.shape
                            centroid_x = max(0, min(w - 1, centroid_x))
                            centroid_y = max(0, min(h - 1, centroid_y))
                            
                            # è·å–æ·±åº¦å€¼ï¼ˆæ¯«ç±³ï¼‰
                            depth_mm = depth_image[int(centroid_y), int(centroid_x)]
                            
                            if depth_mm > 0:
                                # å°†2Dè´¨å¿ƒ+æ·±åº¦è½¬æ¢ä¸º3Dç›¸æœºåæ ‡
                                centroid_camera = self.pixel_to_3d_camera(centroid_x, centroid_y, depth_mm)
                                print(f"2Dè´¨å¿ƒ: ({centroid_x}, {centroid_y}), æ·±åº¦: {depth_mm:.1f}mm")
                                print(f"ç›¸æœºåæ ‡ç³»3Dè´¨å¿ƒ: {centroid_camera}")
                                
                                # åº”ç”¨æ‰‹çœ¼æ ‡å®šè½¬æ¢åˆ°å¤¹çˆªåæ ‡ç³»
                                # éœ€è¦å°†å•ä¸ªç‚¹è½¬æ¢ä¸ºç‚¹æ•°ç»„æ ¼å¼
                                centroid_camera_array = centroid_camera.reshape(1, 3)
                                centroid_gripper_array = self.apply_hand_eye_transform(centroid_camera_array)
                                centroid_gripper = centroid_gripper_array[0]  # æå–å•ä¸ªç‚¹
                                
                                print(f"å¤¹çˆªåæ ‡ç³»3Dè´¨å¿ƒ: {centroid_gripper}")
                                
                                # è½¬æ¢ä¸ºæ¯«ç±³
                                center_gripper_mm = centroid_gripper * 1000.0
                            else:
                                print(f"âš ï¸ è­¦å‘Š: 2Dè´¨å¿ƒä½ç½®æ·±åº¦å€¼ä¸º0ï¼Œæ— æ³•è®¡ç®—3Dè´¨å¿ƒ")
                                # å¦‚æœæ·±åº¦æ— æ•ˆï¼Œå›é€€åˆ°ç‚¹äº‘è´¨å¿ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
                                if points_gripper is not None and len(points_gripper) > 0:
                                    centroid = np.mean(points_gripper, axis=0)
                                    print(f"å›é€€åˆ°ç‚¹äº‘è´¨å¿ƒ: {centroid}")
                                    center_gripper_mm = centroid * 1000.0
                                else:
                                    print("âš ï¸ é”™è¯¯: æ— æ³•è®¡ç®—3Dè´¨å¿ƒï¼Œè·³è¿‡æ­¤ç›®æ ‡")
                                    continue
                        except Exception as e:
                            print(f"âš ï¸ è®¡ç®—2Dè´¨å¿ƒåˆ°3Dè½¬æ¢å¤±è´¥: {e}")
                            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œå›é€€åˆ°ç‚¹äº‘è´¨å¿ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
                            if points_gripper is not None and len(points_gripper) > 0:
                                centroid = np.mean(points_gripper, axis=0)
                                print(f"å›é€€åˆ°ç‚¹äº‘è´¨å¿ƒ: {centroid}")
                                center_gripper_mm = centroid * 1000.0
                            else:
                                print("âš ï¸ é”™è¯¯: æ— æ³•è®¡ç®—3Dè´¨å¿ƒï¼Œè·³è¿‡æ­¤ç›®æ ‡")
                                continue
                        
                        delta_tool_mm = [center_gripper_mm[0], center_gripper_mm[1], center_gripper_mm[2]]
                        delta_base_xyz = self._tool_offset_to_base(delta_tool_mm, current_tcp[3:6])
                        z_offset = -delta_tool_mm[2]
                        relative_move = [delta_base_xyz[0], delta_base_xyz[1], z_offset -5, 0, 0, 0]
                        
                        # åœ¨è´¨å¿ƒæ¨¡å¼ä¸‹ï¼Œç¡®ä¿alpha_1çš„æ–¹å‘ä¸€è‡´æ€§
                        # PCAå¯èƒ½è¿”å›ä¸¤ä¸ªç›¸åçš„æ–¹å‘ï¼Œå¯¼è‡´è§’åº¦ä¸ä¸€è‡´ï¼ˆæœ‰æ—¶æ—‹è½¬90åº¦ï¼‰
                        # é¢„è§ˆå›¾åƒä¸­çš„ç®­å¤´æ˜¯æ­£ç¡®çš„ï¼Œè¯´æ˜(vx, vy)æ–¹å‘æ˜¯æ­£ç¡®çš„
                        # æˆ‘ä»¬éœ€è¦ç¡®ä¿ä½¿ç”¨çš„è§’åº¦ä¸é¢„è§ˆå›¾åƒä¸€è‡´
                        if alpha_1 is not None and mask_vis is not None:
                            try:
                                # é‡æ–°è®¡ç®—PCAï¼Œè·å–æ–¹å‘å‘é‡ï¼ˆä¸é¢„è§ˆå›¾åƒä½¿ç”¨ç›¸åŒçš„è®¡ç®—ï¼‰
                                alpha_1_raw, (vx, vy), _ = estimate_body_angle_alpha1(mask_vis > 0, return_details=True)
                                
                                # # ä½¿ç”¨ä¸é¢„è§ˆå›¾åƒç›¸åŒçš„æ–¹å‘å‘é‡é‡æ–°è®¡ç®—è§’åº¦
                                # # ç¡®ä¿è§’åº¦è®¡ç®—ä¸é¢„è§ˆå›¾åƒä¸€è‡´
                                # alpha_1_new = np.arctan2(vx, vy)
                                
                                # # å°†è§’åº¦è§„èŒƒåŒ–åˆ°[0, pi]èŒƒå›´å†…
                                # # alpha_1æ˜¯ç›¸å¯¹äºå›¾åƒyè½´çš„è§’åº¦ï¼ŒèŒƒå›´æ˜¯[-pi, pi]
                                # # æˆ‘ä»¬éœ€è¦å°†å…¶æ˜ å°„åˆ°[0, pi]èŒƒå›´
                                # # æ–¹æ³•ï¼šå¦‚æœè§’åº¦æ˜¯è´Ÿçš„ï¼ŒåŠ ä¸Š2*piï¼Œç„¶åå–æ¨¡åˆ°[0, 2*pi]ï¼Œæœ€åå¦‚æœè¶…è¿‡piï¼Œå–è¡¥è§’
                                # if alpha_1_new < 0:
                                #     alpha_1_new = alpha_1_new + 2 * np.pi
                                # # å¦‚æœè§’åº¦åœ¨[pi, 2*pi]èŒƒå›´å†…ï¼Œæ˜ å°„åˆ°[0, pi]ï¼ˆå–è¡¥è§’ï¼‰
                                # if alpha_1_new > np.pi:
                                #     alpha_1_new = 2 * np.pi - alpha_1_new
                                
                                # ç¡®ä¿è§’åº¦åœ¨[0, pi]èŒƒå›´å†…
                                alpha_1_new = (alpha_1_raw + np.pi) % np.pi 
                                
                                alpha_1 = alpha_1_new
                                print(f"[è´¨å¿ƒæ¨¡å¼] åŸå§‹alpha_1: {alpha_1_raw:.4f} rad ({np.degrees(alpha_1_raw):.2f} deg)")
                                print(f"[è´¨å¿ƒæ¨¡å¼] ä¿®æ­£åçš„alpha_1: {alpha_1:.4f} rad ({np.degrees(alpha_1):.2f} deg), vx={vx:.3f}, vy={vy:.3f}")
                            except Exception as e:
                                print(f"[è´¨å¿ƒæ¨¡å¼] ä¿®æ­£alpha_1å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å€¼")
                                import traceback
                                traceback.print_exc()
                    
                    grasp_calc_time = time.time() - grasp_calc_start
                    self.timers['grasp_calculation'].append(grasp_calc_time)
                    print(f"â±ï¸  grasp_calculation: {grasp_calc_time:.3f}s")
                    

                    # æ‰§è¡Œç›¸å¯¹ç§»åŠ¨
                    fish_count += 1
                    counter = rows * cols
                    if counter <= 0:
                        print(f"[é”™è¯¯] æ— æ•ˆçš„ç½‘æ ¼é…ç½®: rows={rows}, cols={cols}, counter={counter}")
                        counter = len(fish_path_json) if fish_path_json else 1
                        print(f"[è­¦å‘Š] ä½¿ç”¨è·¯å¾„æ–‡ä»¶æ•°é‡ä½œä¸ºè®¡æ•°å™¨: {counter}")
                    fish_count = ((fish_count - 1) % counter) + 1  # ç¡®ä¿ fish_count åœ¨ 1 åˆ° counter ä¹‹é—´
                    print(f"[è°ƒè¯•] fish_count={fish_count}, counter={counter}, rows={rows}, cols={cols}")

                    if fish_count == 1:
                        continue

                    if fish_count == 6:
                        continue

                    print("Step1 : å‡†å¤‡æŠ“å–")
                    print("ç›¸å¯¹ç§»åŠ¨é‡:", relative_move)
                    
                    # ä¼°ç®—é±¼é‡é‡ï¼ˆåœ¨æŠ“å–å‰ï¼‰
                    estimated_weight = 0.0
                    if self.fish_tracker is not None:
                        estimated_weight = self.estimate_fish_weight(points_gripper)
                        print(f"ğŸŸ ä¼°ç®—é±¼é‡é‡: {estimated_weight:.3f}kg")
                    
                    # æœºå™¨äººç§»åŠ¨è®¡æ—¶
                    robot_movement_start = time.time()
                    # delta_rad = float(alpha_1)
                    # delta_rad = np.clip(delta_rad, 0, np.pi)
                  
                    self.robot.joint_move([0,0,0,0,0,np.pi/2-alpha_1], 1, True, 1)
                    time.sleep(0.5)
                    
                    # catch fish
                    ret = self.robot.linear_move([relative_move[0], relative_move[1], 0, 0, 0, 0], 1, True, 100)
                    self.robot.set_digital_output(0, 0, 1)

                    ret = self.robot.linear_move([0, 0, relative_move[2], 0, 0, 0], 1, True, 100)

                    if fish_count == 5 or fish_count == 10:
                        relative_move[1] = float(relative_move[1]-50)

                    time.sleep(0.3)
                  
                    #ret = self.robot.linear_move(original_tcp, 0, True, 100)

                    # go back to original point
                    #self.robot.joint_move([0,0,0,0,0,-np.pi/2+alpha_1], 1, True, 1)
                    ret = self.robot.linear_move(original_tcp, 0, True, 100)

                    rotation_flag = 1 if alpha_1_raw > 0 else -1 
                    print("-----------------------------------------------------------")
                    print("rotation_flag : {}".format(rotation_flag))
                    ret = self.robot.joint_move([0,0,0,0,0,-np.pi/2 * rotation_flag],1, False,50)

                    time.sleep(0.2)
                    
                    # get target point1
                    fish_key = str(fish_count)
                    if fish_key not in fish_path_json:
                        print(f"[é”™è¯¯] è·¯å¾„æ–‡ä»¶ä¸­ä¸å­˜åœ¨é”® '{fish_key}'ï¼Œå¯ç”¨é”®: {list(fish_path_json.keys())}")
                        print(f"[é”™è¯¯] è·³è¿‡æ­¤æ¬¡æ”¾ç½®")
                        continue
                    xy_path = fish_path_json[fish_key]
                    joint_pos1 = [0, 0, 0, 0, 0, 0]
                    joint_pos1[0] = xy_path[0][0]
                    joint_pos1[1] = xy_path[0][1]
                    joint_pos1[2] = 0
                    joint_pos1[3] = 0
                    joint_pos1[4] = 0
                    joint_pos1[5] = 0
                    ret = self.robot.linear_move(joint_pos1, 1, True, 400)

                    joint_pos2 = [0, 0, 0, 0, 0, 0]
                    joint_pos2[0] = xy_path[1][0]
                    joint_pos2[1] = xy_path[1][1]
                    joint_pos2[2] = -200
                    joint_pos2[3] = 0
                    joint_pos2[4] = 0
                    joint_pos2[5] = 0
                    ret = self.robot.linear_move(joint_pos2, 1, True, 400)
                    self.robot.set_digital_output(0, 1, 1)
                    time.sleep(0.2)
                    self.robot.set_digital_output(0,0,0)
                    self.robot.set_digital_output(0,1,0)
                    ret = self.robot.linear_move([0, -joint_pos2[1], 200, 0, 0, 0], 1 , True, 400)
                    
                    self.robot.linear_move(original_tcp, 0, True, 200)
                    
                    time.sleep(0.3)
                    
                    robot_movement_time = time.time() - robot_movement_start
                    self.timers['robot_movement'].append(robot_movement_time)
                    print(f"â±ï¸  robot_movement: {robot_movement_time:.3f}s")
            
                else:
                    print("ç‚¹äº‘ä¸ºç©ºï¼Œè·³è¿‡æœºå™¨äººæ§åˆ¶")

                # æ•´ä¸ªå¾ªç¯è®¡æ—¶ç»“æŸ
                cycle_time = time.time() - cycle_start
                self.timers['total_cycle'].append(cycle_time)
                print(f"â±ï¸  total_cycle: {cycle_time:.3f}s")
                print("-" * 50)
 
                # self.robot.logout()
                # exit()
                

        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­å¤„ç†")
            self.robot.set_digital_output(0, 0, 0)
        except Exception as e:
            print(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        finally:
            self.cleanup()

    def cleanup(self):
        """
        æ¸…ç†èµ„æº
        """
        cv2.destroyAllWindows()
        if self.pipeline:
            self.pipeline.stop()
        
        # æ‰“å°æ—¶é—´ç»Ÿè®¡æ‘˜è¦
        self.print_timing_summary()
        
        print(f"å¤„ç†å®Œæˆï¼æ€»å…±å¤„ç†äº† {self.frame_count} å¸§")
        print(f"ç»“æœä¿å­˜åœ¨: {self.output_dir}")

def main():
    parser = argparse.ArgumentParser(description='å®æ—¶äººä½“åˆ†å‰²å’Œ3Dç‚¹äº‘ç”Ÿæˆ')
    parser.add_argument('--output_dir', type=str, default='realtime_output',
                      help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: realtime_output)')
    parser.add_argument('--device', type=str, default='cuda',
                      choices=['cpu', 'cuda'],
                      help='è¿è¡Œè®¾å¤‡ (é»˜è®¤: cuda)')
    parser.add_argument('--save_pointcloud', action='store_true',
                      help='ä¿å­˜3Dç‚¹äº‘')
    parser.add_argument('--max_frames', type=int, default=None,
                      help='æœ€å¤§å¤„ç†å¸§æ•° (é»˜è®¤: æ— é™)')
    parser.add_argument('--no_preview', action='store_true',
                      help='ä¸æ˜¾ç¤ºé¢„è§ˆçª—å£')
    parser.add_argument('--intrinsics_file', type=str, default=None,
                      help='ç›¸æœºå†…å‚JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--hand_eye_file', type=str, default=None,
                      help='æ‰‹çœ¼æ ‡å®š4x4é½æ¬¡çŸ©é˜µçš„.npyæ–‡ä»¶è·¯å¾„ï¼ˆç›¸æœºâ†’å¤¹çˆªï¼‰')
    parser.add_argument('--bbox_selection', type=str, default='highest_confidence',
                      choices=['smallest', 'largest', 'highest_confidence'],
                      help='è¾¹ç•Œæ¡†é€‰æ‹©ç­–ç•¥: smallest(é€‰æ‹©é¢ç§¯æœ€å°çš„é±¼) æˆ– largest(é€‰æ‹©é¢ç§¯æœ€å¤§çš„é±¼) (é»˜è®¤: largest)')
    parser.add_argument('--debug', action='store_true',
                      help='å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜æ‰€æœ‰ä¸­é—´æ–‡ä»¶ï¼ˆRGBã€æ·±åº¦ã€æ£€æµ‹ã€åˆ†å‰²ã€ç‚¹äº‘ï¼‰')
    parser.add_argument('--use_yolo', action='store_true',
                      help='ä½¿ç”¨YOLOä½œä¸ºæ£€æµ‹å™¨ï¼ˆæ›¿ä»£Grounding DINOï¼‰')
    parser.add_argument('--yolo_weights', type=str, default=None,
                      help='YOLOæƒé‡æ–‡ä»¶(.pt)è·¯å¾„ï¼ˆä¸ --use_yolo æ­é…ä½¿ç”¨ï¼‰')
    parser.add_argument('--det_gray', action='store_true',
                      help='ä»…åœ¨æ£€æµ‹é˜¶æ®µä½¿ç”¨ç°åº¦å›¾åƒï¼ˆSAMä¸æŠ“å–ä¿æŒRGBï¼‰')
    parser.add_argument('--grasp_point_mode', type=str, default='centroid',
                      choices=['centroid', 'ai'],
                      help='æŠ“å–ç‚¹æ¨¡å¼: centroid(ç‚¹äº‘è´¨å¿ƒ) æˆ– ai(ä½¿ç”¨AIèº«ä½“ä¸­å¿ƒ)')
    parser.add_argument('--landmark_model_path', type=str, default=None,
                      help='AIèº«ä½“ä¸­å¿ƒæ¨¡å‹è·¯å¾„ (.pth)ï¼Œå½“ grasp_point_mode=ai æ—¶å¿…éœ€')
    parser.add_argument('--enable_weight_tracking', action='store_true',
                      help='å¯ç”¨é±¼é‡é‡è·Ÿè¸ªåŠŸèƒ½')
    parser.add_argument('--max_container_weight', type=float, default=12.5,
                      help='å®¹å™¨æœ€å¤§é‡é‡ï¼ˆkgï¼‰ï¼Œé»˜è®¤12.5kg')
    parser.add_argument('--camera_calib_json', type=str, default=None,
                      help='æ‰‹çœ¼æ ‡å®šJSONæ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å« hand_eye.R å’Œ hand_eye.t')
    parser.add_argument('--robot_config', type=str, default='configs/robot.json',
                      help='æœºå™¨äººé…ç½®æ–‡ä»¶ï¼ŒåŒ…å«åˆå§‹ä½å§¿ (é»˜è®¤: configs/robot.json)')
    parser.add_argument('--erode_bbox', action='store_true',
                      help='å¯¹æ£€æµ‹åˆ°çš„maskè¿›è¡Œä¸Šä¸‹æ–¹å‘è…èš€ï¼Œå»é™¤è¾¹ç•ŒåŒºåŸŸä»¥è·å¾—æ›´ç²¾ç¡®çš„è´¨å¿ƒè®¡ç®—')
    parser.add_argument('--erode_ratio', type=float, default=0.1,
                      help='è…èš€æ¯”ä¾‹ï¼Œä¸Šä¸‹å„è…èš€çš„æ¯”ä¾‹ï¼ˆé»˜è®¤0.1ï¼Œå³10%%ï¼‰')
    parser.add_argument('--bbox_scale', type=float, default=1.0,
                      help='è¾¹ç•Œæ¡†ç¼©æ”¾å› å­ï¼ˆé»˜è®¤1.0ï¼Œå³ä¸ç¼©æ”¾ï¼›>1.0æ”¾å¤§ï¼Œ<1.0ç¼©å°ï¼‰')
    parser.add_argument('--seg_model', type=str, default='sam',
                      choices=['sam', 'yolov8_seg'],
                      help='åˆ†å‰²æ¨¡å‹ç±»å‹: sam(é»˜è®¤) æˆ– yolov8_seg')
    parser.add_argument('--yolo_seg_weights', type=str, default=None,
                      help='YOLOv8åˆ†å‰²æ¨¡å‹æƒé‡è·¯å¾„(.pt)ï¼Œå½“ --seg_model=yolov8_seg æ—¶å¿…éœ€')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºå¤„ç†å™¨
        processor = RealtimeSegmentation3D(
            output_dir=args.output_dir,
            device=args.device,
            save_pointcloud=args.save_pointcloud,
            intrinsics_file=args.intrinsics_file,
            hand_eye_file=args.hand_eye_file,
            bbox_selection=args.bbox_selection,
            debug=args.debug,
            use_yolo=args.use_yolo,
            yolo_weights=args.yolo_weights,
            grasp_point_mode=args.grasp_point_mode,
            landmark_model_path=args.landmark_model_path,
            enable_weight_tracking=args.enable_weight_tracking,
            max_container_weight=args.max_container_weight,
            det_gray=args.det_gray,
            camera_calib_json=args.camera_calib_json,
            robot_config=args.robot_config,
            erode_bbox=args.erode_bbox,
            erode_ratio=args.erode_ratio,
            bbox_scale=args.bbox_scale,
            seg_model=args.seg_model,
            yolo_seg_weights=args.yolo_seg_weights
        )
        # è¿è¡Œå®æ—¶å¤„ç†
        processor.run_realtime(
            max_frames=args.max_frames,
            show_preview=not args.no_preview
        )
        
    except Exception as e:
        print(f"ç¨‹åºå‡ºé”™: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
