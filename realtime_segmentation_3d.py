#!/usr/bin/env python3
"""
å®æ—¶äººä½“åˆ†å‰²å’Œ3Dç‚¹äº‘ç”Ÿæˆè„šæœ¬

æ•´åˆç°æœ‰åŠŸèƒ½ï¼š
1. RealSenseç›¸æœºè¯»å–RGBå’Œæ·±åº¦æ•°æ®
2. SAM + Grounding DINOè¿›è¡Œäººä½“åˆ†å‰²
3. å°†æ©ç è½¬æ¢ä¸º3Dç‚¹äº‘

ä½¿ç”¨æ–¹æ³•:
    python3 realtime_segmentation_3d.py --output_dir output_data --save_pointcloud

ä¾èµ–:
    - ç°æœ‰çš„seg.py, mask_to_3d.py, realsense_capture.py
    - pyrealsense2, opencv-python, numpy, torch
    - segment_anything, transformers, open3d
"""

import argparse
import os
import sys
import time
import numpy as np
import cv2
import torch
from datetime import datetime
from tqdm import tqdm
from PIL import Image

# å¯¼å…¥ç°æœ‰æ¨¡å—çš„åŠŸèƒ½
from seg import init_models# process_image_cv2
from mask_to_3d import mask_to_3d_pointcloud, save_pointcloud, load_camera_intrinsics
from realsense_capture import setup_realsense, depth_to_pointcloud, save_pointcloud_to_file

# è¿½åŠ è‡ªå®šä¹‰æ¨¡å—æœç´¢è·¯å¾„ï¼ˆæ‰‹çœ¼æ ‡å®šç›®å½•ï¼‰
_extra_paths = [
    "/home/ai/AI_perception/hand_eye_calibrate",
]
for _p in _extra_paths:
    try:
        if os.path.isdir(_p) and _p not in sys.path:
            sys.path.insert(0, _p)
    except Exception:
        pass

class RealtimeSegmentation3D:
    def __init__(self, output_dir, device="cpu", save_pointcloud=True, intrinsics_file=None, hand_eye_file=None, bbox_selection="smallest", debug=False):
        """
        åˆå§‹åŒ–å®æ—¶åˆ†å‰²å’Œ3Dç‚¹äº‘ç”Ÿæˆå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            device: è¿è¡Œè®¾å¤‡ (cpu/cuda)
            save_pointcloud: æ˜¯å¦ä¿å­˜3Dç‚¹äº‘
            intrinsics_file: ç›¸æœºå†…å‚JSONæ–‡ä»¶è·¯å¾„
            hand_eye_file: æ‰‹çœ¼æ ‡å®š4x4é½æ¬¡çŸ©é˜µçš„.npyæ–‡ä»¶è·¯å¾„ï¼ˆç›¸æœºâ†’å¤¹çˆªï¼‰
            bbox_selection: è¾¹ç•Œæ¡†é€‰æ‹©ç­–ç•¥ ("smallest" æˆ– "largest")
            debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆä¿å­˜æ‰€æœ‰ä¸­é—´æ–‡ä»¶ï¼‰
        """
        self.output_dir = output_dir
        self.device = device
        self.save_pointcloud = save_pointcloud
        self.bbox_selection = bbox_selection
        self.debug = debug
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹åˆ›å»ºï¼‰
        if self.debug:
            self.rgb_dir = os.path.join(output_dir, "rgb")
            self.depth_dir = os.path.join(output_dir, "depth")
            self.mask_dir = os.path.join(output_dir, "masks")
            self.pointcloud_dir = os.path.join(output_dir, "pointclouds")
            self.segmentation_dir = os.path.join(output_dir, "segmentation")
            self.detection_dir = os.path.join(output_dir, "detection")
            
            os.makedirs(self.rgb_dir, exist_ok=True)
            os.makedirs(self.depth_dir, exist_ok=True)
            os.makedirs(self.mask_dir, exist_ok=True)
            if save_pointcloud:
                os.makedirs(self.pointcloud_dir, exist_ok=True)
            os.makedirs(self.segmentation_dir, exist_ok=True)
            os.makedirs(self.detection_dir, exist_ok=True)
            print("è°ƒè¯•æ¨¡å¼å·²å¯ç”¨ï¼Œå°†ä¿å­˜æ‰€æœ‰ä¸­é—´æ–‡ä»¶")
        else:
            print("æ­£å¸¸æ¨¡å¼ï¼Œä¸ä¿å­˜ä¸­é—´æ–‡ä»¶")
        
        # åˆå§‹åŒ–æ¨¡å‹
        print("æ­£åœ¨åˆå§‹åŒ–AIæ¨¡å‹...")
        self.sam_predictor, self.grounding_dino_model, self.processor = init_models(device)
        
        # åˆå§‹åŒ–RealSenseç›¸æœº
        print("æ­£åœ¨åˆå§‹åŒ–RealSenseç›¸æœº...")
        self.pipeline, self.config = setup_realsense()
        if self.pipeline is None:
            raise RuntimeError("æ— æ³•å¯åŠ¨RealSenseç›¸æœº")
        
        # è·å–ç›¸æœºå†…å‚å’Œç•¸å˜ç³»æ•°
        self.fx, self.fy, self.cx, self.cy, self.dist, self.mtx = load_camera_intrinsics(intrinsics_file)
        print(f"ä½¿ç”¨ç›¸æœºå†…å‚: fx={self.fx}, fy={self.fy}, cx={self.cx}, cy={self.cy}")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç•¸å˜æ ¡æ­£
        if np.any(self.dist != 0):
            print("æ£€æµ‹åˆ°ç•¸å˜ç³»æ•°ï¼Œå°†è¿›è¡Œå®æ—¶å›¾åƒç•¸å˜æ ¡æ­£")
            print(f"ç•¸å˜ç³»æ•°: k1={self.dist[0]:.6f}, k2={self.dist[1]:.6f}, k3={self.dist[4]:.6f}")
        else:
            print("æœªæ£€æµ‹åˆ°ç•¸å˜ç³»æ•°ï¼Œè·³è¿‡ç•¸å˜æ ¡æ­£")
            self.mtx = None
            self.dist = None
        
        # åˆ›å»ºå¯¹é½å¯¹è±¡
        import pyrealsense2 as rs
        self.align = rs.align(rs.stream.color)
        
        # å¸§è®¡æ•°å™¨
        self.frame_count = 0
        self.start_time = time.time()
        
        # è®¡æ—¶å™¨
        self.timers = {
            'detection': [],
            'segmentation': [],
            'pointcloud_generation': [],
            'grasp_calculation': [],
            'robot_movement': [],
            'total_cycle': []
        }
        
        # åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µï¼ˆå¯é€‰ï¼‰
        self.hand_eye_transform = None  # 4x4 é½æ¬¡çŸ©é˜µï¼Œç›¸æœºåæ ‡â†’å¤¹çˆªåæ ‡
        if hand_eye_file is not None and os.path.exists(hand_eye_file):
            try:
                mat = np.load(hand_eye_file)
                if mat.shape == (4, 4):
                    self.hand_eye_transform = mat.astype(np.float32)
                    print("å·²åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µ (ç›¸æœºâ†’å¤¹çˆª):")
                    print(self.hand_eye_transform)
                else:
                    print(f"hand_eye_file æ ¼å¼ä¸æ­£ç¡®ï¼ŒæœŸæœ›(4,4)ï¼Œå®é™…{mat.shape}ï¼Œå¿½ç•¥ã€‚")
            except Exception as e:
                print(f"åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µå¤±è´¥: {e}")
        # è‹¥æœªåŠ è½½åˆ°ï¼Œåˆ™ä½¿ç”¨ç¡¬ç¼–ç çš„Rã€tï¼ˆç›¸æœºâ†’å¤¹çˆªï¼‰
        if self.hand_eye_transform is None:
            R_default = np.array([
                [-0.99791369, -0.06094636, -0.02130291],
                [ 0.06027516, -0.99770511,  0.03084494],
                [-0.02313391,  0.02949655,  0.99929714]
            ], dtype=np.float32)
            t_default = np.array([[0.04], [0.065], [-0.22081495]], dtype=np.float32)
            self.hand_eye_transform = np.eye(4, dtype=np.float32)
            self.hand_eye_transform[:3, :3] = R_default
            self.hand_eye_transform[:3, 3:4] = t_default
            print("ä½¿ç”¨ç¡¬ç¼–ç æ‰‹çœ¼æ ‡å®šçŸ©é˜µ (ç›¸æœºâ†’å¤¹çˆª):")
            print(self.hand_eye_transform)
        
        print("åˆå§‹åŒ–å®Œæˆï¼")


        import jkrc 
        self.robot = jkrc.RC("192.168.80.116")
        self.robot.login()   
    
    def time_step(self, step_name):
        """è®¡æ—¶å™¨è£…é¥°å™¨ï¼Œç”¨äºæµ‹é‡å„ä¸ªæ­¥éª¤çš„æ—¶é—´"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                start_time = time.time()
                result = func(*args, **kwargs)
                end_time = time.time()
                elapsed = end_time - start_time
                self.timers[step_name].append(elapsed)
                print(f"â±ï¸  {step_name}: {elapsed:.3f}s")
                return result
            return wrapper
        return decorator
    
    def print_timing_summary(self):
        """æ‰“å°æ—¶é—´ç»Ÿè®¡æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æ—¶é—´ç»Ÿè®¡æ‘˜è¦")
        print("="*60)
        
        for step_name, times in self.timers.items():
            if times:
                avg_time = np.mean(times)
                min_time = np.min(times)
                max_time = np.max(times)
                total_time = np.sum(times)
                print(f"{step_name:20s}: å¹³å‡={avg_time:.3f}s, æœ€å°={min_time:.3f}s, æœ€å¤§={max_time:.3f}s, æ€»è®¡={total_time:.3f}s")
            else:
                print(f"{step_name:20s}: æ— æ•°æ®")
        
        print("="*60)
    
    def capture_frames(self):
        """
        æ•è·RGBå’Œæ·±åº¦å¸§
        
        Returns:
            color_image: RGBå›¾åƒ
            depth_image: æ·±åº¦å›¾åƒ (æ¯«ç±³)
            success: æ˜¯å¦æˆåŠŸ
        """
        try:
            # ç­‰å¾…æ–°çš„å¸§
            frames = self.pipeline.wait_for_frames()
            
            # å¯¹é½æ·±åº¦å¸§åˆ°RGBå¸§
            aligned_frames = self.align.process(frames)
            
            # è·å–å¯¹é½åçš„å¸§
            color_frame = aligned_frames.get_color_frame()
            depth_frame = aligned_frames.get_depth_frame()
            
            if not color_frame or not depth_frame:
                return None, None, False
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆRealSenseé…ç½®ä¸ºbgr8ï¼Œå› æ­¤è¿™é‡Œç›´æ¥å¾—åˆ°BGRæ ¼å¼ï¼Œé€‚ç”¨äºOpenCVï¼‰
            color_image = np.asanyarray(color_frame.get_data())

            # è·å–æ·±åº¦æ•°æ®
            height, width = depth_frame.get_height(), depth_frame.get_width()
            depth_image = np.zeros((height, width), dtype=np.uint16)
            
            for y in range(height):
                for x in range(width):
                    dist = depth_frame.get_distance(x, y)
                    if dist > 0:
                        depth_image[y, x] = int(dist * 1000)  # è½¬æ¢ä¸ºæ¯«ç±³
            
            # å¦‚æœå¯ç”¨äº†ç•¸å˜æ ¡æ­£ï¼Œæ ¡æ­£å›¾åƒ
            # if self.mtx is not None and self.dist is not None:
            #     color_image = cv2.undistort(color_image, self.mtx, self.dist)
            #     # # æ·±åº¦å›¾åƒéœ€è¦è½¬æ¢ä¸ºfloat32ç±»å‹è¿›è¡Œç•¸å˜æ ¡æ­£
            #     # depth_image_float = depth_image.astype(np.float32)
            #     # depth_image_undistorted = cv2.undistort(depth_image_float, self.mtx, self.dist)
            #     # depth_image = depth_image_undistorted.astype(np.uint16)
            
            return color_image, depth_image, True
            
        except Exception as e:
            print(f"æ•è·å¸§æ—¶å‡ºé”™: {e}")
            return None, None, False
    
    def detect_and_segment_and_dump(self, color_image):
        """
        æœ¬åœ°å®Œæˆæ£€æµ‹->è½ç›˜->åˆ†å‰²->è½ç›˜ï¼Œè¿”å›ç”¨äºæ˜¾ç¤ºçš„å•é€šé“uint8æ©ç ï¼ˆ0/255ï¼‰ã€‚
        åªé€‰æ‹©ä¸€æ¡é±¼è¿›è¡Œåˆ†å‰²ï¼Œæ— æ£€æµ‹æ—¶è¿”å›Noneã€‚
        """
        # æ£€æµ‹ï¼ˆåªé€‰æ‹©ä¸€æ¡é±¼ï¼‰
        detection_start = time.time()
        boxes = self._detect_boxes(color_image)
        detection_time = time.time() - detection_start
        self.timers['detection'].append(detection_time)
        print(f"â±ï¸  detection: {detection_time:.3f}s")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        base_name = f"frame_{self.frame_count:06d}_{timestamp}"
        
        # ä¿å­˜æ£€æµ‹å¯è§†åŒ–ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹ï¼‰
        if self.debug:
            det_vis = color_image.copy()
            if len(boxes) > 0:
                # åªæ ‡è®°é€‰ä¸­çš„é±¼ï¼ˆç»¿è‰²æ¡†ï¼‰
                x1, y1, x2, y2 = boxes[0]
                cv2.rectangle(det_vis, (x1, y1), (x2, y2), (0, 255, 0), 3)  # ç»¿è‰²ç²—æ¡†è¡¨ç¤ºé€‰ä¸­çš„é±¼
                cv2.putText(det_vis, "SELECTED", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
                # ä¿å­˜é€‰ä¸­çš„é±¼çš„è£å‰ªå›¾åƒ
                crop = color_image[y1:y2, x1:x2]
                if crop.size > 0:
                    cv2.imwrite(os.path.join(self.detection_dir, f"{base_name}_selected_fish.png"), crop)
                
                cv2.imwrite(os.path.join(self.detection_dir, f"{base_name}_dino_detection.png"), det_vis)

        if not boxes:
            print("æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼Œè·³è¿‡åˆ†å‰²ã€‚")
            return None, None

        # åˆ†å‰²ï¼ˆSAMï¼‰- åªå¤„ç†é€‰ä¸­çš„ä¸€æ¡é±¼
        segmentation_start = time.time()
        try:
            image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)
            self.sam_predictor.set_image(image_rgb)
            
            # åªä½¿ç”¨é€‰ä¸­çš„è¾¹ç•Œæ¡†
            x1, y1, x2, y2 = boxes[0]
            boxes_tensor = torch.tensor([[x1, y1, x2, y2]], device=self.device)
            transformed_boxes = self.sam_predictor.transform.apply_boxes_torch(boxes_tensor, image_rgb.shape[:2])

            # ä½¿ç”¨multimask_output=Falseç¡®ä¿åªè¿”å›ä¸€ä¸ªæ©ç 
            masks, scores, logits = self.sam_predictor.predict_torch(
                point_coords=None,
                point_labels=None,
                boxes=transformed_boxes,
                multimask_output=False  # åªè¿”å›ä¸€ä¸ªæœ€ä½³æ©ç 
            )

            # å¤„ç†é€‰ä¸­çš„é±¼çš„æ©ç  - ç¡®ä¿åªä½¿ç”¨ç¬¬ä¸€ä¸ªæ©ç 
            if masks.shape[0] > 0 and masks.shape[1] > 0:
                # åªå–ç¬¬ä¸€ä¸ªæ©ç ï¼ˆç´¢å¼•[0][0]ï¼‰
                m_bool = masks[0][0].cpu().numpy().astype(np.uint8)
                mask_np = m_bool * 255
                
                # è¿›ä¸€æ­¥é™åˆ¶æ©ç åªåœ¨è¾¹ç•Œæ¡†åŒºåŸŸå†…
                # åˆ›å»ºä¸€ä¸ªå…¨é›¶æ©ç 
                restricted_mask = np.zeros_like(mask_np)
                # åªåœ¨è¾¹ç•Œæ¡†åŒºåŸŸå†…åº”ç”¨æ©ç 
                restricted_mask[y1:y2, x1:x2] = mask_np[y1:y2, x1:x2]
                mask_np = restricted_mask
                
                # ä¿å­˜æ©ç ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹ï¼‰
                if self.debug:
                    mask_path = os.path.join(self.segmentation_dir, f"{base_name}_selected_fish_mask.png")
                    cv2.imwrite(mask_path, mask_np)
                    
                    # ä¿å­˜è£å‰ªæ©ç 
                    mask_crop = mask_np[y1:y2, x1:x2]
                    if mask_crop.size > 0:
                        mask_crop_path = os.path.join(self.segmentation_dir, f"{base_name}_selected_fish_mask_crop.png")
                        cv2.imwrite(mask_crop_path, mask_crop)
                    
                    # ä¿å­˜è£å‰ªå¯è§†åŒ–
                    crop = color_image[y1:y2, x1:x2]
                    if crop.size > 0 and mask_crop.size > 0:
                        overlay = np.zeros_like(crop)
                        overlay[mask_crop > 0] = [0, 255, 0]
                        vis_crop = cv2.addWeighted(crop, 1.0, overlay, 0.4, 0)
                        vis_crop_path = os.path.join(self.segmentation_dir, f"{base_name}_selected_fish_vis.png")
                        cv2.imwrite(vis_crop_path, vis_crop)
                    
                    # ä¿å­˜æ•´ä½“å¯è§†åŒ–
                    colored = np.zeros_like(color_image)
                    colored[mask_np > 0] = [0, 255, 0]
                    vis = cv2.addWeighted(color_image, 1.0, colored, 0.4, 0)
                    vis_path = os.path.join(self.segmentation_dir, f"{base_name}_selected_fish_overlay.png")
                    cv2.imwrite(vis_path, vis)
                
                segmentation_time = time.time() - segmentation_start
                self.timers['segmentation'].append(segmentation_time)
                print(f"â±ï¸  segmentation: {segmentation_time:.3f}s")
                
                print(f"æˆåŠŸåˆ†å‰²é€‰ä¸­çš„é±¼ï¼Œæ©ç ç‚¹æ•°: {np.sum(mask_np > 0)}")
                print(f"æ©ç é™åˆ¶åœ¨è¾¹ç•Œæ¡†å†…: ({x1}, {y1}) åˆ° ({x2}, {y2})")
                return mask_np, base_name
            else:
                segmentation_time = time.time() - segmentation_start
                self.timers['segmentation'].append(segmentation_time)
                print(f"â±ï¸  segmentation: {segmentation_time:.3f}s")
                print("åˆ†å‰²å¤±è´¥ï¼Œæœªç”Ÿæˆæ©ç ")
                return None, None
                
        except Exception as e:
            segmentation_time = time.time() - segmentation_start
            self.timers['segmentation'].append(segmentation_time)
            print(f"â±ï¸  segmentation: {segmentation_time:.3f}s")
            print(f"åˆ†å‰²æ—¶å‡ºé”™: {e}")
            return None, None

    def _detect_boxes(self, color_image):
        """
        ä½¿ç”¨ä¸ seg.py ç›¸åŒçš„æ–¹å¼è¿›è¡Œæ£€æµ‹ï¼Œè¿”å›bboxåˆ—è¡¨
        åªé€‰æ‹©ä¸€æ¡é±¼è¿›è¡Œåˆ†å‰²å’ŒæŠ“å–
        """
        # è½¬æ¢ä¸ºPILå›¾åƒï¼ˆä¸ seg.py ä¸€è‡´ï¼‰
        image_pil = Image.fromarray(cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB))
        text_prompt = "fish. crab. marine animal"
        inputs = self.processor(images=image_pil, text=text_prompt, return_tensors="pt").to(self.device)
        with torch.no_grad():
            outputs = self.grounding_dino_model(**inputs)
        h, w = color_image.shape[0], color_image.shape[1]
        results = self.processor.post_process_grounded_object_detection(
            outputs,
            inputs.input_ids,
            text_threshold=0.3,
            # ä¸ seg.py ç›¸åŒçš„å°ºå¯¸ä¼ å…¥æ–¹å¼
            target_sizes=[image_pil.size[::-1]]
        )
        result = results[0]
        boxes = []
        print("\næ£€æµ‹ç»“æœè¯¦æƒ…:")
        print(f"æ£€æµ‹åˆ°çš„ç›®æ ‡æ•°é‡: {len(result['boxes'])}")
        if len(result["boxes"]) == 0:
            return boxes
        
        # è¿‡æ»¤è¾¹ç•Œæ¡†ï¼šé¢ç§¯å¿…é¡»å¤§äº1000åƒç´ 
        valid_boxes = []
        for box in result["boxes"]:
            x1, y1, x2, y2 = [int(c) for c in box.tolist()]
            x1 = max(0, min(x1, w - 1))
            y1 = max(0, min(y1, h - 1))
            x2 = max(0, min(x2, w - 1))
            y2 = max(0, min(y2, h - 1))
            
            # è®¡ç®—è¾¹ç•Œæ¡†é¢ç§¯
            area = (x2 - x1) * (y2 - y1)
            if area > 1000:  # é¢ç§¯è¿‡æ»¤
                valid_boxes.append(((x1, y1, x2, y2), area))
        
        if valid_boxes:
            # æ ¹æ®é€‰æ‹©ç­–ç•¥é€‰æ‹©è¾¹ç•Œæ¡†
            if self.bbox_selection == "smallest":
                selected_box = min(valid_boxes, key=lambda x: x[1])
                selection_type = "é¢ç§¯æœ€å°çš„"
            elif self.bbox_selection == "largest":
                selected_box = max(valid_boxes, key=lambda x: x[1])
                selection_type = "é¢ç§¯æœ€å¤§çš„"
            else:
                # é»˜è®¤é€‰æ‹©æœ€å°çš„
                selected_box = min(valid_boxes, key=lambda x: x[1])
                selection_type = "é¢ç§¯æœ€å°çš„"
                print(f"è­¦å‘Š: æœªçŸ¥çš„é€‰æ‹©ç­–ç•¥ '{self.bbox_selection}'ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥ 'smallest'")
            
            boxes.append(selected_box[0])
            print(f"æ£€æµ‹åˆ° {len(valid_boxes)} æ¡é±¼ï¼Œé€‰æ‹©{selection_type}è¿›è¡ŒæŠ“å–ï¼Œé¢ç§¯: {selected_box[1]} åƒç´ ")
            print(f"é€‰æ‹©çš„é±¼ä½ç½®: {selected_box[0]}")
        else:
            print("æ²¡æœ‰æ»¡è¶³é¢ç§¯è¦æ±‚çš„è¾¹ç•Œæ¡†")
        
        return boxes
    
    def dump_detections(self, color_image):
        """
        å°†æ£€æµ‹åˆ°çš„ç›®æ ‡è£å‰ªå¹¶ä¿å­˜åˆ° detection/ ç›®å½•
        """
        boxes = self._detect_boxes(color_image)
        if not boxes:
            return 0
        base_ts = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        saved = 0
        for idx, (x1, y1, x2, y2) in enumerate(boxes):
            crop = color_image[y1:y2, x1:x2]
            if crop.size == 0:
                continue
            filename = f"frame_{self.frame_count:06d}_{base_ts}_det_{idx}.png"
            path = os.path.join(self.detection_dir, filename)
            cv2.imwrite(path, crop)
            saved += 1
        if saved:
            print(f"å·²ä¿å­˜ {saved} ä¸ªæ£€æµ‹è£å‰ªåˆ°: {self.detection_dir}")
        return saved

    
    def generate_pointcloud(self, color_image, depth_image, mask):
        """
        ä»æ©ç ç”Ÿæˆ3Dç‚¹äº‘
        
        Args:
            color_image: RGBå›¾åƒ
            depth_image: æ·±åº¦å›¾åƒ (æ¯«ç±³)
            mask: åˆ†å‰²æ©ç 
            
        Returns:
            points: 3Dç‚¹åæ ‡
            colors: RGBé¢œè‰²
        """
        try:
            # è½¬æ¢æ·±åº¦å›¾åƒå•ä½ä¸ºç±³
            depth_image_meters = depth_image.astype(np.float32) / 1000.0
            
            # è½¬æ¢ä¸ºRGBæ ¼å¼
            color_image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)
            
            # ä½¿ç”¨mask_to_3d_pointcloudå‡½æ•°ï¼ˆæ”¯æŒç•¸å˜æ ¡æ­£ï¼‰
            points, colors = mask_to_3d_pointcloud(
                color_image_rgb, 
                depth_image_meters, 
                mask, 
                self.fx, self.fy, self.cx, self.cy,
                self.mtx, self.dist
            )
            
            return points, colors
            
        except Exception as e:
            print(f"ç”Ÿæˆç‚¹äº‘æ—¶å‡ºé”™: {e}")
            return np.array([]), np.array([])

    def apply_hand_eye_transform(self, points):
        """
        å°†ç‚¹äº‘ä»ç›¸æœºç³»è½¬æ¢åˆ°å¤¹çˆªç³»ï¼Œä½¿ç”¨ self.hand_eye_transform (4x4)ã€‚
        æ—‹è½¬çŸ©é˜µï¼š
            [[-0.99462885  0.07149648  0.07484454]
            [-0.06962775 -0.9971997   0.02728984]
            [ 0.07658608  0.021932    0.99682173]]
            å¹³ç§»å‘é‡ï¼š
            [[ 0.0247092 ]
            [ 0.09912939]
            [-0.25357213]]
        """
        if self.hand_eye_transform is None or points.size == 0:
            return points
        ones = np.ones((points.shape[0], 1), dtype=np.float32)
        homo = np.hstack([points.astype(np.float32), ones])  # (N,4)
        transformed = (self.hand_eye_transform @ homo.T).T  # (N,4)
        return transformed[:, :3]

    def _rpy_to_rotation_matrix(self, rx, ry, rz):
        """
        å°†æœ«ç«¯çš„ RPY (rx, ry, rz) è½¬ä¸ºæ—‹è½¬çŸ©é˜µ R (åŸºåº§â†’æœ«ç«¯)ã€‚
        é‡‡ç”¨å¸¸è§çš„å¤–æ—‹é¡ºåº R = Rz @ Ry @ Rxã€‚
        """
        sx, cx = np.sin(rx), np.cos(rx)
        sy, cy = np.sin(ry), np.cos(ry)
        sz, cz = np.sin(rz), np.cos(rz)

        Rx = np.array([[1, 0, 0],
                       [0, cx, -sx],
                       [0, sx,  cx]], dtype=np.float32)
        Ry = np.array([[ cy, 0, sy],
                       [  0, 1,  0],
                       [-sy, 0, cy]], dtype=np.float32)
        Rz = np.array([[cz, -sz, 0],
                       [sz,  cz, 0],
                       [ 0,   0, 1]], dtype=np.float32)

        return (Rz @ Ry @ Rx).astype(np.float32)

    def _tool_offset_to_base(self, delta_tool_xyz_mm, tcp_rpy):
        """
        å°†å¤¹çˆª(å·¥å…·)åæ ‡ç³»ä¸‹çš„ä½ç§»(mm)è½¬æ¢åˆ°åŸºåæ ‡ç³»ä¸‹çš„ä½ç§»(mm)ã€‚
        delta_tool_xyz_mm: [dx, dy, dz] in tool frame
        tcp_rpy: [rx, ry, rz] in radians
        è¿”å›: [dx_base, dy_base, dz_base]
        """
        rx, ry, rz = tcp_rpy
        R_base_tool = self._rpy_to_rotation_matrix(rx, ry, rz)
        delta_tool = np.asarray(delta_tool_xyz_mm, dtype=np.float32).reshape(3, 1)
        delta_base = (R_base_tool @ delta_tool).reshape(3)
        return delta_base.tolist()

    def calculate_pointcloud_bbox(self, points):
        """
        è®¡ç®—ç‚¹äº‘çš„è¾¹ç•Œæ¡†ä¿¡æ¯ï¼Œç”¨äºé«˜åº¦å’Œå§¿æ€ä¼°è®¡
        
        Args:
            points: ç‚¹äº‘åæ ‡ (N, 3)
            
        Returns:
            bbox_info: å­—å…¸åŒ…å«ä¸­å¿ƒç‚¹ã€å°ºå¯¸ã€è¾¹ç•Œæ¡†ç­‰
        """
        if points.size == 0:
            return None
            
        # è®¡ç®—è¾¹ç•Œæ¡†
        min_coords = np.min(points, axis=0)  # [min_x, min_y, min_z]
        max_coords = np.max(points, axis=0)  # [max_x, max_y, max_z]
        
        # è®¡ç®—ä¸­å¿ƒç‚¹
        center = (min_coords + max_coords) / 2.0  # [center_x, center_y, center_z]
        
        # è®¡ç®—å°ºå¯¸
        dimensions = max_coords - min_coords  # [width, height, depth]
        
        # è®¡ç®—é«˜åº¦ï¼ˆzæ–¹å‘ï¼‰
        height = dimensions[2]  # zæ–¹å‘çš„é«˜åº¦
        
        # è®¡ç®—8ä¸ªè§’ç‚¹
        corners = []
        for x in [min_coords[0], max_coords[0]]:
            for y in [min_coords[1], max_coords[1]]:
                for z in [min_coords[2], max_coords[2]]:
                    corners.append([x, y, z])
        corners = np.array(corners)
        
        # è®¡ç®—ç‚¹äº‘çš„ä¸»æ–¹å‘ï¼ˆPCAï¼‰
        try:
            from sklearn.decomposition import PCA
            pca = PCA(n_components=3)
            pca.fit(points)
            principal_axes = pca.components_  # ä¸»æ–¹å‘å‘é‡
            explained_variance = pca.explained_variance_ratio_  # è§£é‡Šæ–¹å·®æ¯”ä¾‹
        except ImportError:
            print("sklearnæœªå®‰è£…ï¼Œè·³è¿‡PCAå§¿æ€ä¼°è®¡")
            principal_axes = np.eye(3)
            explained_variance = [1.0, 0.0, 0.0]
        
        bbox_info = {
            'center': center,
            'dimensions': dimensions,
            'height': height,
            'min_coords': min_coords,
            'max_coords': max_coords,
            'corners': corners,
            'principal_axes': principal_axes,
            'explained_variance': explained_variance,
            'num_points': len(points)
        }
        
        return bbox_info
    
    def save_results(self, color_image, depth_image, mask, points, colors):
        """
        ä¿å­˜æ‰€æœ‰ç»“æœ
        
        Args:
            color_image: RGBå›¾åƒ
            depth_image: æ·±åº¦å›¾åƒ
            mask: åˆ†å‰²æ©ç 
            points: 3Dç‚¹åæ ‡
            colors: RGBé¢œè‰²
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        base_name = f"frame_{self.frame_count:06d}_{timestamp}"
        
        # ä¿å­˜RGBå›¾åƒ
        rgb_path = os.path.join(self.rgb_dir, f"{base_name}.png")
        cv2.imwrite(rgb_path, color_image)
        
        # ä¿å­˜æ·±åº¦å›¾åƒ
        depth_path = os.path.join(self.depth_dir, f"{base_name}.png")
        cv2.imwrite(depth_path, depth_image.astype(np.uint16))
        
        # ä¿å­˜æ©ç 
        if mask is not None:
            mask_path = os.path.join(self.mask_dir, f"{base_name}_mask.png")
            cv2.imwrite(mask_path, mask.astype(np.uint8) * 255)
            
            # åˆ›å»ºå¯è§†åŒ–ç»“æœ
            colored_mask = np.zeros_like(color_image)
            colored_mask[mask] = [0, 255, 0]  # ç»¿è‰²æ©ç 
            alpha = 0.5
            visualization = cv2.addWeighted(color_image, 1, colored_mask, alpha, 0)
            vis_path = os.path.join(self.segmentation_dir, f"{base_name}_vis.png")
            cv2.imwrite(vis_path, visualization)
        
        # ä¿å­˜ç‚¹äº‘
        if self.save_pointcloud and len(points) > 0:
            pointcloud_path = os.path.join(self.pointcloud_dir, f"{base_name}_pointcloud.ply")
            save_pointcloud_to_file(points, colors, pointcloud_path)
        
        print(f"å·²ä¿å­˜ç¬¬ {self.frame_count} å¸§ç»“æœ")
    
    def show_preview(self, color_image, depth_image, mask):
        """
        åœ¨ä¸€ä¸ªçª—å£ä¸­æ˜¾ç¤ºRGBã€æ·±åº¦å›¾å’Œåˆ†å‰²ç»“æœ
        """
        # åˆ›å»ºæ·±åº¦å¯è§†åŒ–
        valid_depth = depth_image > 0
        if valid_depth.any():
            depth_min = depth_image[valid_depth].min()
            depth_max = depth_image[valid_depth].max()
            depth_normalized = np.zeros_like(depth_image, dtype=np.uint8)
            if depth_max > depth_min:
                depth_normalized[valid_depth] = ((depth_image[valid_depth] - depth_min) / (depth_max - depth_min) * 255).astype(np.uint8)
            depth_colormap = cv2.applyColorMap(depth_normalized, cv2.COLORMAP_JET)
        else:
            depth_colormap = np.zeros((depth_image.shape[0], depth_image.shape[1], 3), dtype=np.uint8)
        
        # åˆ›å»ºåˆ†å‰²å¯è§†åŒ–
        if mask is not None:
            # å°†æ©ç è½¬æ¢ä¸ºå½©è‰²å›¾åƒ
            mask_colored = np.zeros_like(color_image)
            mask_colored[mask > 0] = [0, 255, 0]  # ç»¿è‰²æ©ç 
            # å åŠ åˆ°åŸå›¾ä¸Š
            segmentation_vis = cv2.addWeighted(color_image, 0.7, mask_colored, 0.3, 0)
        else:
            segmentation_vis = color_image.copy()
        
        # è°ƒæ•´å›¾åƒå¤§å°
        display_size = (400, 300)
        color_display = cv2.resize(color_image, display_size)
        depth_display = cv2.resize(depth_colormap, display_size)
        seg_display = cv2.resize(segmentation_vis, display_size)
        
        # æ°´å¹³æ‹¼æ¥ä¸‰ä¸ªå›¾åƒ
        combined = np.hstack((color_display, depth_display, seg_display))
        
        # æ·»åŠ æ ‡ç­¾
        cv2.putText(combined, "RGB", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        cv2.putText(combined, "Depth", (410, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        cv2.putText(combined, "Segmentation", (810, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        cv2.putText(combined, f"Frame: {self.frame_count}", (10, combined.shape[0] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        cv2.imshow('RGB | Depth | Segmentation', combined)

    
    def run_realtime(self, max_frames=None, show_preview=True):
        """
        è¿è¡Œå®æ—¶å¤„ç†
        
        Args:
            max_frames: æœ€å¤§å¸§æ•° (Noneè¡¨ç¤ºæ— é™)
            show_preview: æ˜¯å¦æ˜¾ç¤ºé¢„è§ˆçª—å£
        """
        print("å¼€å§‹å®æ—¶å¤„ç†...")
        print("æŒ‰ 'q' é”®åœæ­¢")
        

        tcp_result = self.robot.get_tcp_position()
        if isinstance(tcp_result, tuple) and len(tcp_result) == 2:
            tcp_ok, original_tcp = tcp_result
        else:
            # å¦‚æœåªè¿”å›ä¸€ä¸ªå€¼ï¼Œå‡è®¾å®ƒæ˜¯ä½ç½®ä¿¡æ¯
            original_tcp = tcp_result
            tcp_ok = True

        try:
            while True:
                # æ•´ä¸ªå¾ªç¯è®¡æ—¶å¼€å§‹
                cycle_start = time.time()
                
                # æ•è·å¸§
                color_image, depth_image, success = self.capture_frames()
                if not success:
                    continue
                
                # è·³è¿‡å‰3å¸§ï¼Œè®©ç›¸æœºç¨³å®š
                if self.frame_count < 10:
                    print(f"è·³è¿‡ç¬¬ {self.frame_count + 1} å¸§ï¼Œç­‰å¾…ç›¸æœºç¨³å®š...")
                    self.frame_count += 1
                    continue

                # æ£€æµ‹ + åˆ†å‰² + è½ç›˜
                mask_vis, base_name = self.detect_and_segment_and_dump(color_image)
                
                # ä¿å­˜RGBå’Œæ·±åº¦å›¾åƒï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹ï¼‰
                if self.debug and base_name is not None:
                    # ä¿å­˜RGBå›¾åƒ
                    rgb_path = os.path.join(self.rgb_dir, f"{base_name}.png")
                    cv2.imwrite(rgb_path, color_image)
                    
                    # ä¿å­˜æ·±åº¦å›¾åƒï¼ˆåŸå§‹16ä½ï¼‰
                    depth_path = os.path.join(self.depth_dir, f"{base_name}.png")
                    cv2.imwrite(depth_path, depth_image.astype(np.uint16))
                    
                    # ä¿å­˜å¯è§†åŒ–æ·±åº¦å›¾åƒï¼ˆ8ä½å½©è‰²ï¼‰
                    valid_depth = depth_image > 0
                    if valid_depth.any():
                        depth_min = depth_image[valid_depth].min()
                        depth_max = depth_image[valid_depth].max()
                        depth_normalized = np.zeros_like(depth_image, dtype=np.uint8)
                        if depth_max > depth_min:
                            depth_normalized[valid_depth] = ((depth_image[valid_depth] - depth_min) / (depth_max - depth_min) * 255).astype(np.uint8)
                        depth_colormap = cv2.applyColorMap(depth_normalized, cv2.COLORMAP_JET)
                        depth_vis_path = os.path.join(self.depth_dir, f"{base_name}_visualization.png")
                        cv2.imwrite(depth_vis_path, depth_colormap)

                # æ˜¾ç¤ºé¢„è§ˆçª—å£
                self.show_preview(color_image, depth_image, mask_vis)
                
                # ç¡®ä¿çª—å£æ˜¾ç¤ºå¹¶å¤„ç†æŒ‰é”®
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("ç”¨æˆ·æŒ‰ 'q' é”®åœæ­¢")
                    break

                # æ ¹æ®æ©ç ç”Ÿæˆ3Dç‚¹äº‘å¹¶ä¿å­˜ï¼ˆå¯é€‰åº”ç”¨æ‰‹çœ¼æ ‡å®šï¼‰
                points_gripper = None  # åˆå§‹åŒ–å˜é‡

                #import pdb; pdb.set_trace()
                if mask_vis is not None and base_name is not None:
                    # ç‚¹äº‘ç”Ÿæˆè®¡æ—¶
                    pointcloud_start = time.time()
                    mask_bool = (mask_vis > 0)
                    points, colors = self.generate_pointcloud(color_image, depth_image, mask_bool)
                    pointcloud_time = time.time() - pointcloud_start
                    self.timers['pointcloud_generation'].append(pointcloud_time)
                    print(f"â±ï¸  pointcloud_generation: {pointcloud_time:.3f}s")

                    #import pdb; pdb.set_trace()
                    if len(points) > 0:
                        # åº”ç”¨æ‰‹çœ¼å˜æ¢ï¼šç›¸æœºâ†’å¤¹çˆª
                        points_gripper = self.apply_hand_eye_transform(points)
                        
                        # ä¿å­˜ç‚¹äº‘ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹ï¼‰
                        if self.debug:
                            # ä¿å­˜ç›¸æœºåæ ‡ç³»ç‚¹äº‘
                            cam_ply = os.path.join(self.pointcloud_dir, f"{base_name}_cam_pointcloud.ply")
                            save_pointcloud_to_file(points, colors, cam_ply)
                            # ä¿å­˜å¤¹çˆªåæ ‡ç³»ç‚¹äº‘
                            grip_ply = os.path.join(self.pointcloud_dir, f"{base_name}_gripper_pointcloud.ply")
                            save_pointcloud_to_file(points_gripper, colors, grip_ply)
                
                # don't forget to transform the units, the point cloud is in meter, but robot
                # control would like to be in mm. 

                # è®¡ç®—ç‚¹äº‘è´¨å¿ƒï¼ˆåœ¨å¤¹çˆªåæ ‡ç³»ä¸­ï¼‰
                if points_gripper is not None and len(points_gripper) > 0:
                    # æŠ“å–ç‚¹è®¡ç®—è®¡æ—¶
                    grasp_calc_start = time.time()
                    
                    # è®¡ç®—ç‚¹äº‘è´¨å¿ƒ
                    centroid = np.mean(points_gripper, axis=0)
                    print(f"å¤¹çˆªåæ ‡ç³»ç‚¹äº‘è´¨å¿ƒ: {centroid}")
                    
                    # ç¡¬ç¼–ç é«˜åº¦ä¸º0.05m
                    hardcoded_height = 0.05  # 5cm
                    print(f"ä½¿ç”¨ç¡¬ç¼–ç é«˜åº¦: {hardcoded_height:.3f}m")

                    # è·å–å½“å‰æœºå™¨äººTCPä½ç½®
                    tcp_result = self.robot.get_tcp_position()
                    if isinstance(tcp_result, tuple) and len(tcp_result) == 2:
                        tcp_ok, current_tcp = tcp_result
                    else:
                        # å¦‚æœåªè¿”å›ä¸€ä¸ªå€¼ï¼Œå‡è®¾å®ƒæ˜¯ä½ç½®ä¿¡æ¯
                        current_tcp = tcp_result
                        tcp_ok = True
                    print(f"å½“å‰TCPä½ç½®: {current_tcp}")
                    
                    #import pdb; pdb.set_trace()
                    # å¤¹çˆªåæ ‡ç³»ä¸­çš„ç›®æ ‡ä¸­å¿ƒç‚¹ï¼ˆè½¬æ¢ä¸ºæ¯«ç±³ï¼‰
                    center_gripper_mm = centroid * 1000
                    #import pdb; pdb.set_trace()
                    # è®¡ç®—ç›¸å¯¹ç§»åŠ¨ï¼šä»å½“å‰TCPä½ç½®ç§»åŠ¨åˆ°å¤¹çˆªåæ ‡ç³»ä¸­çš„ç›®æ ‡ä½ç½®
                    # æ³¨æ„ï¼šå¤¹çˆªåæ ‡ç³»ä¸­çš„æ­£zæ–¹å‘å¯èƒ½éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                    # å°†å·¥å…·ç³»(å¤¹çˆª)ä½ç§»è½¬æ¢ä¸ºåŸºåº§ç³»ä½ç§»ï¼Œä»¥é¿å…x/yæ–¹å‘è¯¯å·®
                    # å·¥å…·ç³»ä½ç§»ï¼šè®©TCPä»å½“å‰åˆ°è¾¾å¯¹è±¡ä¸­å¿ƒï¼ˆå¿½ç•¥å§¿æ€å˜åŒ–ï¼‰
                    delta_tool_mm = [center_gripper_mm[0] , center_gripper_mm[1], hardcoded_height* 1000]
                    # current_tcp: [x(mm), y(mm), z(mm), rx(rad), ry(rad), rz(rad)]
                    delta_base_xyz = self._tool_offset_to_base(delta_tool_mm, current_tcp[3:6])
                    # è°ƒæ•´Zï¼šä½¿ç”¨å½“å‰zä¸æœŸæœ›é«˜åº¦å·®ï¼ˆæ­£å€¼å‘ä¸Š/å‘ä¸‹ä¾æœºå™¨äººå®šä¹‰ï¼Œå¯æŒ‰å®é™…è°ƒè¯•ï¼‰
                    z_offset = -(current_tcp[2] - hardcoded_height * 1000) + 220
                    relative_move = [delta_base_xyz[0] +0,delta_base_xyz[1] +0, z_offset, 0, 0, 0]
                    
                    grasp_calc_time = time.time() - grasp_calc_start
                    self.timers['grasp_calculation'].append(grasp_calc_time)
                    print(f"â±ï¸  grasp_calculation: {grasp_calc_time:.3f}s")
                    
                    print("Step1 : å‡†å¤‡æŠ“å–")
                    print("å¤¹çˆªåæ ‡ç³»ç›®æ ‡ä¸­å¿ƒ:", center_gripper_mm)
                    print("ç›¸å¯¹ç§»åŠ¨é‡:", relative_move)
                    
                    # æœºå™¨äººç§»åŠ¨è®¡æ—¶
                    robot_movement_start = time.time()
                    
                    # æ‰§è¡Œç›¸å¯¹ç§»åŠ¨
                    #import pdb; pdb.set_trace()
                    #self.robot.set_digital_output(0, 0, 1)
                    self.robot.linear_move(relative_move, 1, True, 400)
                    
                    
                    self.robot.linear_move(original_tcp, 0 , True, 400)
                    #self.robot.set_digital_output(0, 0, 0)
                    #self.robot.logout()
                    #exit()
                    
                    robot_movement_time = time.time() - robot_movement_start
                    self.timers['robot_movement'].append(robot_movement_time)
                    print(f"â±ï¸  robot_movement: {robot_movement_time:.3f}s")

                else:
                    print("ç‚¹äº‘ä¸ºç©ºï¼Œè·³è¿‡æœºå™¨äººæ§åˆ¶")

                # æ•´ä¸ªå¾ªç¯è®¡æ—¶ç»“æŸ
                cycle_time = time.time() - cycle_start
                self.timers['total_cycle'].append(cycle_time)
                print(f"â±ï¸  total_cycle: {cycle_time:.3f}s")
                print("-" * 50)

        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­å¤„ç†")
        except Exception as e:
            print(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        finally:
            self.cleanup()

    def cleanup(self):
        """
        æ¸…ç†èµ„æº
        """
        cv2.destroyAllWindows()
        if self.pipeline:
            self.pipeline.stop()
        
        # æ‰“å°æ—¶é—´ç»Ÿè®¡æ‘˜è¦
        self.print_timing_summary()
        
        print(f"å¤„ç†å®Œæˆï¼æ€»å…±å¤„ç†äº† {self.frame_count} å¸§")
        print(f"ç»“æœä¿å­˜åœ¨: {self.output_dir}")

def main():
    parser = argparse.ArgumentParser(description='å®æ—¶äººä½“åˆ†å‰²å’Œ3Dç‚¹äº‘ç”Ÿæˆ')
    parser.add_argument('--output_dir', type=str, default='realtime_output',
                      help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: realtime_output)')
    parser.add_argument('--device', type=str, default='cuda',
                      choices=['cpu', 'cuda'],
                      help='è¿è¡Œè®¾å¤‡ (é»˜è®¤: cuda)')
    parser.add_argument('--save_pointcloud', action='store_true',
                      help='ä¿å­˜3Dç‚¹äº‘')
    parser.add_argument('--max_frames', type=int, default=None,
                      help='æœ€å¤§å¤„ç†å¸§æ•° (é»˜è®¤: æ— é™)')
    parser.add_argument('--no_preview', action='store_true',
                      help='ä¸æ˜¾ç¤ºé¢„è§ˆçª—å£')
    parser.add_argument('--intrinsics_file', type=str, default=None,
                      help='ç›¸æœºå†…å‚JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--hand_eye_file', type=str, default=None,
                      help='æ‰‹çœ¼æ ‡å®š4x4é½æ¬¡çŸ©é˜µçš„.npyæ–‡ä»¶è·¯å¾„ï¼ˆç›¸æœºâ†’å¤¹çˆªï¼‰')
    parser.add_argument('--bbox_selection', type=str, default='largest',
                      choices=['smallest', 'largest'],
                      help='è¾¹ç•Œæ¡†é€‰æ‹©ç­–ç•¥: smallest(é€‰æ‹©é¢ç§¯æœ€å°çš„é±¼) æˆ– largest(é€‰æ‹©é¢ç§¯æœ€å¤§çš„é±¼) (é»˜è®¤: largest)')
    parser.add_argument('--debug', action='store_true',
                      help='å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜æ‰€æœ‰ä¸­é—´æ–‡ä»¶ï¼ˆRGBã€æ·±åº¦ã€æ£€æµ‹ã€åˆ†å‰²ã€ç‚¹äº‘ï¼‰')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºå¤„ç†å™¨
        processor = RealtimeSegmentation3D(
            output_dir=args.output_dir,
            device=args.device,
            save_pointcloud=args.save_pointcloud,
            intrinsics_file=args.intrinsics_file,
            hand_eye_file=args.hand_eye_file,
            bbox_selection=args.bbox_selection,
            debug=args.debug
        )
        
        # è¿è¡Œå®æ—¶å¤„ç†
        processor.run_realtime(
            max_frames=args.max_frames,
            show_preview=not args.no_preview
        )
        
    except Exception as e:
        print(f"ç¨‹åºå‡ºé”™: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
