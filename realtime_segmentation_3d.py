#!/usr/bin/env python3
"""
å®æ—¶äººä½“åˆ†å‰²å’Œ3Dç‚¹äº‘ç”Ÿæˆè„šæœ¬

æ•´åˆç°æœ‰åŠŸèƒ½ï¼š
1. RealSenseç›¸æœºè¯»å–RGBå’Œæ·±åº¦æ•°æ®
2. SAM + Grounding DINOè¿›è¡Œäººä½“åˆ†å‰²
3. å°†æ©ç è½¬æ¢ä¸º3Dç‚¹äº‘

ä½¿ç”¨æ–¹æ³•:
    python3 realtime_segmentation_3d.py --output_dir output_data --save_pointcloud

ä¾èµ–:
    - ç°æœ‰çš„seg.py, mask_to_3d.py, realsense_capture.py
    - pyrealsense2, opencv-python, numpy, torch
    - segment_anything, transformers, open3d



    add a training mechasim that grasp only on the fish with masked out other region 
"""

import argparse
import os
import sys
import time
import math
import numpy as np
import cv2
import torch
from datetime import datetime
from tqdm import tqdm
from PIL import Image

# å¯¼å…¥ç°æœ‰æ¨¡å—çš„åŠŸèƒ½
from seg import init_models# process_image_cv2
from mask_to_3d import mask_to_3d_pointcloud, save_pointcloud, load_camera_intrinsics
from realsense_capture import setup_realsense, depth_to_pointcloud, save_pointcloud_to_file
# Landmark detector for AI-based grasp point estimation
try:
    # ä¼˜å…ˆä»¥åŒ…å½¢å¼å¯¼å…¥
    from landmarks.fish_landmark_detector import FishLandmarkDetector
except Exception:
    # å…¼å®¹ç›´æ¥åœ¨å·¥ä½œåŒºæ ¹ç›®å½•è¿è¡Œ
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'landmarks'))
    try:
        from fish_landmark_detector import FishLandmarkDetector
    except Exception:
        FishLandmarkDetector = None

# è¿½åŠ è‡ªå®šä¹‰æ¨¡å—æœç´¢è·¯å¾„ï¼ˆæ‰‹çœ¼æ ‡å®šç›®å½•ï¼‰
_extra_paths = [
    "/home/ai/AI_perception/hand_eye_calibrate",
]
for _p in _extra_paths:
    try:
        if os.path.isdir(_p) and _p not in sys.path:
            sys.path.insert(0, _p)
    except Exception:
        pass

# å¯¼å…¥é±¼å®¹å™¨è·Ÿè¸ªå™¨
try:
    from FishContainerTracker import FishContainerTracker
except ImportError:
    print("[è­¦å‘Š] æ— æ³•å¯¼å…¥ FishContainerTrackerï¼Œå°†è·³è¿‡é‡é‡è·Ÿè¸ªåŠŸèƒ½")
    FishContainerTracker = None

# å¯¼å…¥ä½ç½®æ±‚è§£å™¨
try:
    from PositionSolver import PositionSolver, ContainerConfig
except ImportError:
    print("[è­¦å‘Š] æ— æ³•å¯¼å…¥ PositionSolverï¼Œå°†è·³è¿‡ä½ç½®é¢„æµ‹åŠŸèƒ½")
    PositionSolver = None
    ContainerConfig = None

class RealtimeSegmentation3D:
    def __init__(self, output_dir, device="cpu", save_pointcloud=True, intrinsics_file=None, hand_eye_file=None, bbox_selection="highest_confidence", debug=False, use_yolo=False, yolo_weights=None,
                 grasp_point_mode: str = "centroid", landmark_model_path: str = None, enable_weight_tracking: bool = True, max_container_weight: float = 12.5):
        """
        åˆå§‹åŒ–å®æ—¶åˆ†å‰²å’Œ3Dç‚¹äº‘ç”Ÿæˆå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            device: è¿è¡Œè®¾å¤‡ (cpu/cuda)
            save_pointcloud: æ˜¯å¦ä¿å­˜3Dç‚¹äº‘
            intrinsics_file: ç›¸æœºå†…å‚JSONæ–‡ä»¶è·¯å¾„
            hand_eye_file: æ‰‹çœ¼æ ‡å®š4x4é½æ¬¡çŸ©é˜µçš„.npyæ–‡ä»¶è·¯å¾„ï¼ˆç›¸æœºâ†’å¤¹çˆªï¼‰
            bbox_selection: è¾¹ç•Œæ¡†é€‰æ‹©ç­–ç•¥ ("smallest" æˆ– "largest" æˆ– "highest_confidence")
            debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆä¿å­˜æ‰€æœ‰ä¸­é—´æ–‡ä»¶ï¼‰
            use_yolo: æ˜¯å¦ä½¿ç”¨YOLOä½œä¸ºæ£€æµ‹å™¨
            yolo_weights: YOLOæƒé‡è·¯å¾„ï¼ˆ.ptï¼‰
            grasp_point_mode: æŠ“å–ç‚¹æ¨¡å¼ ("centroid" æˆ– "ai")
            landmark_model_path: AIå…³é”®ç‚¹æ¨¡å‹è·¯å¾„
            enable_weight_tracking: æ˜¯å¦å¯ç”¨é‡é‡è·Ÿè¸ª
            max_container_weight: å®¹å™¨æœ€å¤§é‡é‡ï¼ˆkgï¼‰
        """
        self.output_dir = output_dir
        self.device = device
        self.save_pointcloud = save_pointcloud
        self.bbox_selection = bbox_selection
        self.debug = debug
        self.use_yolo = use_yolo
        self.yolo_weights = yolo_weights
        # æŠ“å–ç‚¹æ¨¡å¼ï¼šcentroid æˆ– ai
        self.grasp_point_mode = grasp_point_mode
        self.landmark_model_path = landmark_model_path
        # é‡é‡è·Ÿè¸ªç›¸å…³
        self.enable_weight_tracking = enable_weight_tracking
        self.max_container_weight = max_container_weight
        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹åˆ›å»ºï¼‰
        if self.debug:
            self.rgb_dir = os.path.join(output_dir, "rgb")
            self.depth_dir = os.path.join(output_dir, "depth")
            self.mask_dir = os.path.join(output_dir, "masks")
            self.pointcloud_dir = os.path.join(output_dir, "pointclouds")
            self.segmentation_dir = os.path.join(output_dir, "segmentation")
            self.detection_dir = os.path.join(output_dir, "detection")
            
            os.makedirs(self.rgb_dir, exist_ok=True)
            os.makedirs(self.depth_dir, exist_ok=True)
            os.makedirs(self.mask_dir, exist_ok=True)
            if save_pointcloud:
                os.makedirs(self.pointcloud_dir, exist_ok=True)
            os.makedirs(self.segmentation_dir, exist_ok=True)
            os.makedirs(self.detection_dir, exist_ok=True)
            print("è°ƒè¯•æ¨¡å¼å·²å¯ç”¨ï¼Œå°†ä¿å­˜æ‰€æœ‰ä¸­é—´æ–‡ä»¶")
        else:
            print("æ­£å¸¸æ¨¡å¼ï¼Œä¸ä¿å­˜ä¸­é—´æ–‡ä»¶")
        
        # åˆå§‹åŒ–æ¨¡å‹
        print("æ­£åœ¨åˆå§‹åŒ–AIæ¨¡å‹...")
        self.sam_predictor, self.grounding_dino_model, self.processor = init_models(device)
        
        if self.use_yolo:
            if not self.yolo_weights or not os.path.exists(self.yolo_weights):
                print(f"[è­¦å‘Š] å·²å¯ç”¨YOLOæ£€æµ‹ï¼Œä½†æœªæ‰¾åˆ°æƒé‡: {self.yolo_weights}ï¼Œå°†å›é€€Grounding DINO")
                self.use_yolo = False
        
        # åˆå§‹åŒ–RealSenseç›¸æœº
        print("æ­£åœ¨åˆå§‹åŒ–RealSenseç›¸æœº...")
        self.pipeline, self.config = setup_realsense()
        if self.pipeline is None:
            raise RuntimeError("æ— æ³•å¯åŠ¨RealSenseç›¸æœº")
        
        # è·å–ç›¸æœºå†…å‚å’Œç•¸å˜ç³»æ•°
        self.fx, self.fy, self.cx, self.cy, self.dist, self.mtx = load_camera_intrinsics(intrinsics_file)
        print(f"ä½¿ç”¨ç›¸æœºå†…å‚: fx={self.fx}, fy={self.fy}, cx={self.cx}, cy={self.cy}")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç•¸å˜æ ¡æ­£
        if np.any(self.dist != 0):
            print("æ£€æµ‹åˆ°ç•¸å˜ç³»æ•°ï¼Œå°†è¿›è¡Œå®æ—¶å›¾åƒç•¸å˜æ ¡æ­£")
            print(f"ç•¸å˜ç³»æ•°: k1={self.dist[0]:.6f}, k2={self.dist[1]:.6f}, k3={self.dist[4]:.6f}")
        else:
            print("æœªæ£€æµ‹åˆ°ç•¸å˜ç³»æ•°ï¼Œè·³è¿‡ç•¸å˜æ ¡æ­£")
            self.mtx = None
            self.dist = None
        
        # åˆ›å»ºå¯¹é½å¯¹è±¡
        import pyrealsense2 as rs
        self.align = rs.align(rs.stream.color)
        
        # å¸§è®¡æ•°å™¨
        self.frame_count = 0
        self.start_time = time.time()
        
        # è®¡æ—¶å™¨
        self.timers = {
            'detection': [],
            'segmentation': [],
            'pointcloud_generation': [],
            'grasp_calculation': [],
            'robot_movement': [],
            'total_cycle': []
        }
        
        # åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µï¼ˆå¯é€‰ï¼‰
        self.hand_eye_transform = None  # 4x4 é½æ¬¡çŸ©é˜µï¼Œç›¸æœºåæ ‡â†’å¤¹çˆªåæ ‡
        if hand_eye_file is not None and os.path.exists(hand_eye_file):
            try:
                mat = np.load(hand_eye_file)
                if mat.shape == (4, 4):
                    self.hand_eye_transform = mat.astype(np.float32)
                    print("å·²åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µ (ç›¸æœºâ†’å¤¹çˆª):")
                    print(self.hand_eye_transform)
                else:
                    print(f"hand_eye_file æ ¼å¼ä¸æ­£ç¡®ï¼ŒæœŸæœ›(4,4)ï¼Œå®é™…{mat.shape}ï¼Œå¿½ç•¥ã€‚")
            except Exception as e:
                print(f"åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µå¤±è´¥: {e}")
        # è‹¥æœªåŠ è½½åˆ°ï¼Œåˆ™ä½¿ç”¨ç¡¬ç¼–ç çš„Rã€tï¼ˆç›¸æœºâ†’å¤¹çˆªï¼‰
        if self.hand_eye_transform is None:
            R_default = np.array([
                [-0.99791369, -0.06094636, -0.02130291],
                [ 0.06027516, -0.99770511,  0.03084494],
                [-0.02313391,  0.02949655,  0.99929714]
            ], dtype=np.float32)
            t_default = np.array([[0.04], [0.113], [-0.22081495]], dtype=np.float32)
            self.hand_eye_transform = np.eye(4, dtype=np.float32)
            self.hand_eye_transform[:3, :3] = R_default
            self.hand_eye_transform[:3, 3:4] = t_default
            print("ä½¿ç”¨ç¡¬ç¼–ç æ‰‹çœ¼æ ‡å®šçŸ©é˜µ (ç›¸æœºâ†’å¤¹çˆª):")
            print(self.hand_eye_transform)
        
        print("åˆå§‹åŒ–å®Œæˆï¼")

        # åˆå§‹åŒ–AIå…³é”®ç‚¹æ£€æµ‹å™¨ï¼ˆå¯é€‰ï¼‰
        self.landmark_detector = None
        if self.grasp_point_mode == "ai":
            if FishLandmarkDetector is None:
                print("[è­¦å‘Š] æ— æ³•å¯¼å…¥ FishLandmarkDetectorï¼Œå°†å›é€€ä¸ºè´¨å¿ƒæ¨¡å¼")
                self.grasp_point_mode = "centroid"
            else:
                try:
                    if landmark_model_path is None:
                        print("[è­¦å‘Š] æœªæä¾› landmark_model_pathï¼Œå°†å›é€€ä¸ºè´¨å¿ƒæ¨¡å¼")
                        self.grasp_point_mode = "centroid"
                    else:
                        self.landmark_detector = FishLandmarkDetector(model_path=landmark_model_path, device=('cuda' if self.device=='cuda' and torch.cuda.is_available() else 'cpu'))
                        print(f"å·²åŠ è½½AIå…³é”®ç‚¹æ¨¡å‹: {landmark_model_path}")
                except Exception as e:
                    print(f"[è­¦å‘Š] å…³é”®ç‚¹æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†å›é€€ä¸ºè´¨å¿ƒæ¨¡å¼")
                    self.grasp_point_mode = "centroid"

        # åˆå§‹åŒ–é±¼å®¹å™¨è·Ÿè¸ªå™¨ï¼ˆå¯é€‰ï¼‰
        self.fish_tracker = None
        if self.enable_weight_tracking and FishContainerTracker is not None:
            try:
                self.fish_tracker = FishContainerTracker(
                    max_weight_kg=self.max_container_weight,
                    data_file=os.path.join(self.output_dir, "fish_tracking_data.json")
                )
                print(f"å·²å¯ç”¨é±¼å®¹å™¨è·Ÿè¸ªå™¨ï¼Œæœ€å¤§å®¹é‡: {self.max_container_weight}kg")
            except Exception as e:
                print(f"[è­¦å‘Š] é±¼å®¹å™¨è·Ÿè¸ªå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.fish_tracker = None
        else:
            print("é±¼å®¹å™¨è·Ÿè¸ªå™¨æœªå¯ç”¨")

        # åˆå§‹åŒ–ä½ç½®æ±‚è§£å™¨ï¼ˆå¯é€‰ï¼‰
        self.position_solver = None
        if self.enable_weight_tracking and PositionSolver is not None:
            try:
                # é…ç½®å®¹å™¨å‚æ•°ï¼ˆæ ¹æ®å®é™…å®¹å™¨å°ºå¯¸è°ƒæ•´ï¼‰
                container_config = ContainerConfig(
                    width_mm=300.0,      # å®¹å™¨å®½åº¦
                    height_mm=200.0,     # å®¹å™¨é«˜åº¦
                    depth_mm=150.0,      # å®¹å™¨æ·±åº¦
                    grid_spacing_mm=30.0, # ç½‘æ ¼é—´è·
                    margin_mm=20.0,      # è¾¹è·
                    base_height_mm=0.0   # åŸºç¡€é«˜åº¦
                )
                self.position_solver = PositionSolver(container_config)
                print("å·²å¯ç”¨ä½ç½®æ±‚è§£å™¨")
            except Exception as e:
                print(f"[è­¦å‘Š] ä½ç½®æ±‚è§£å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.position_solver = None
        else:
            print("ä½ç½®æ±‚è§£å™¨æœªå¯ç”¨")


        import jkrc 
        self.robot = jkrc.RC("192.168.80.116")
        self.robot.login()   
        self.robot.set_digital_output(0, 0, 0)

    
    def time_step(self, step_name):
        """è®¡æ—¶å™¨è£…é¥°å™¨ï¼Œç”¨äºæµ‹é‡å„ä¸ªæ­¥éª¤çš„æ—¶é—´"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                start_time = time.time()
                result = func(*args, **kwargs)
                end_time = time.time()
                elapsed = end_time - start_time
                self.timers[step_name].append(elapsed)
                print(f"â±ï¸  {step_name}: {elapsed:.3f}s")
                return result
            return wrapper
        return decorator
    
    def print_timing_summary(self):
        """æ‰“å°æ—¶é—´ç»Ÿè®¡æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æ—¶é—´ç»Ÿè®¡æ‘˜è¦")
        print("="*60)
        
        for step_name, times in self.timers.items():
            if times:
                avg_time = np.mean(times)
                min_time = np.min(times)
                max_time = np.max(times)
                total_time = np.sum(times)
                print(f"{step_name:20s}: å¹³å‡={avg_time:.3f}s, æœ€å°={min_time:.3f}s, æœ€å¤§={max_time:.3f}s, æ€»è®¡={total_time:.3f}s")
            else:
                print(f"{step_name:20s}: æ— æ•°æ®")
        
        print("="*60)
    
    def capture_frames(self):
        """
        æ•è·RGBå’Œæ·±åº¦å¸§
        
        Returns:
            color_image: RGBå›¾åƒ
            depth_image: æ·±åº¦å›¾åƒ (æ¯«ç±³)
            success: æ˜¯å¦æˆåŠŸ
        """
        try:
            # ç­‰å¾…æ–°çš„å¸§
            frames = self.pipeline.wait_for_frames()
            
            # å¯¹é½æ·±åº¦å¸§åˆ°RGBå¸§
            aligned_frames = self.align.process(frames)
            
            # è·å–å¯¹é½åçš„å¸§
            color_frame = aligned_frames.get_color_frame()
            depth_frame = aligned_frames.get_depth_frame()
            
            if not color_frame or not depth_frame:
                return None, None, False
            
            # è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼ˆRealSenseé…ç½®ä¸ºbgr8ï¼Œå› æ­¤è¿™é‡Œç›´æ¥å¾—åˆ°BGRæ ¼å¼ï¼Œé€‚ç”¨äºOpenCVï¼‰
            color_image = np.asanyarray(color_frame.get_data())

            # è·å–æ·±åº¦æ•°æ®
            height, width = depth_frame.get_height(), depth_frame.get_width()
            depth_image = np.zeros((height, width), dtype=np.uint16)
            
            for y in range(height):
                for x in range(width):
                    dist = depth_frame.get_distance(x, y)
                    if dist > 0:
                        depth_image[y, x] = int(dist * 1000)  # è½¬æ¢ä¸ºæ¯«ç±³
            
            # å¦‚æœå¯ç”¨äº†ç•¸å˜æ ¡æ­£ï¼Œæ ¡æ­£å›¾åƒ
            # if self.mtx is not None and self.dist is not None:
            #     color_image = cv2.undistort(color_image, self.mtx, self.dist)
            #     # # æ·±åº¦å›¾åƒéœ€è¦è½¬æ¢ä¸ºfloat32ç±»å‹è¿›è¡Œç•¸å˜æ ¡æ­£
            #     # depth_image_float = depth_image.astype(np.float32)
            #     # depth_image_undistorted = cv2.undistort(depth_image_float, self.mtx, self.dist)
            #     # depth_image = depth_image_undistorted.astype(np.uint16)
            
            return color_image, depth_image, True
            
        except Exception as e:
            print(f"æ•è·å¸§æ—¶å‡ºé”™: {e}")
            return None, None, False
    
    # write seperate function to do detection and segmentation togther but segmentation is done for all the objects, 
    # and then we can select the  grasp object based on the distance of camera,.
    def detect_and_segment_and_dump_all(self, color_image, depth_image):
        """
        æ£€æµ‹æ‰€æœ‰å€™é€‰é±¼ï¼Œåˆ†åˆ«åˆ†å‰²å¹¶è®¡ç®—ç‚¹äº‘è´¨å¿ƒæ·±åº¦ï¼Œé€‰å–ç¦»ç›¸æœºæœ€è¿‘çš„ä¸€ä¸ªã€‚

        Args:
            color_image: BGR å›¾åƒ (H,W,3)
            depth_image: æ·±åº¦å›¾ (æ¯«ç±³, uint16)

        Returns:
            mask_np: é€‰ä¸­é±¼çš„å•é€šé“uint8æ©ç ï¼ˆ0/255ï¼‰ï¼Œè‹¥å¤±è´¥è¿”å›None
            base_name: æ–‡ä»¶åŸºåå­—ç¬¦ä¸²ï¼Œè‹¥å¤±è´¥è¿”å›None
        """
        # 1) æ£€æµ‹æ‰€æœ‰å€™é€‰æ¡†
        detection_start = time.time()
        if getattr(self, 'use_yolo', False):
            # YOLO è·¯å¾„ï¼šdetect_yolo å·²è¿”å›æ‰€æœ‰æ»¡è¶³æ¡ä»¶çš„æ¡† (x1,y1,x2,y2,conf)
            boxes = self.detect_yolo(color_image, self.yolo_weights, conf=0.25, iou=0.45, imgsz=640)
        else:
            # GroundingDINO è·¯å¾„ï¼šå¤ç”¨ _detect_boxes çš„å®ç°é€»è¾‘ä½†æ”¶é›†å…¨éƒ¨æœ‰æ•ˆæ¡†
            image_pil = Image.fromarray(cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB))
            text_prompt = "fish. crab. marine animal"
            inputs = self.processor(images=image_pil, text=text_prompt, return_tensors="pt").to(self.device)
            with torch.no_grad():
                outputs = self.grounding_dino_model(**inputs)
            H, W = color_image.shape[0], color_image.shape[1]
            results = self.processor.post_process_grounded_object_detection(
                outputs,
                inputs.input_ids,
                text_threshold=0.3,
                target_sizes=[image_pil.size[::-1]]
            )
            result = results[0]
            boxes = []
            if len(result.get("boxes", [])) > 0:
                for box in result["boxes"]:
                    x1, y1, x2, y2 = [int(c) for c in box.tolist()]
                    x1 = max(0, min(x1, W - 1))
                    y1 = max(0, min(y1, H - 1))
                    x2 = max(0, min(x2, W - 1))
                    y2 = max(0, min(y2, H - 1))
                    area = max(0, x2 - x1) * max(0, y2 - y1)
                    if area > 1000:
                        # ä¸ºäº†ç»Ÿä¸€ï¼Œä¸ YOLO ä¸€æ ·é™„ä¸Šä¸€ä¸ªä¼ªç½®ä¿¡åº¦ 1.0
                        boxes.append((x1, y1, x2, y2, 1.0))
        detection_time = time.time() - detection_start
        self.timers['detection'].append(detection_time)
        print(f"â±ï¸  detection(all): {detection_time:.3f}s  å€™é€‰æ•°: {len(boxes) if boxes else 0}")

        if not boxes:
            print("æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼Œè·³è¿‡åˆ†å‰²ã€‚")
            return None, None

        # 2) é€ä¸ªå€™é€‰æ¡†è¿›è¡Œåˆ†å‰²ï¼Œå¹¶è®¡ç®—ç‚¹äº‘è´¨å¿ƒæ·±åº¦
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        base_name = f"frame_{self.frame_count:06d}_{timestamp}"

        image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)
        self.sam_predictor.set_image(image_rgb)

        best_idx = -1
        best_depth_m = float('inf')
        best_mask = None

        segmentation_start = time.time()
        for i, b in enumerate(boxes):
            x1, y1, x2, y2 = b[:4]
            boxes_tensor = torch.tensor([[x1, y1, x2, y2]], device=self.device)
            transformed_boxes = self.sam_predictor.transform.apply_boxes_torch(boxes_tensor, image_rgb.shape[:2])

            try:
                masks, scores, logits = self.sam_predictor.predict_torch(
                    point_coords=None,
                    point_labels=None,
                    boxes=transformed_boxes,
                    multimask_output=False
                )
            except Exception as e:
                print(f"[åˆ†å‰²] å€™é€‰æ¡† {i} é¢„æµ‹å¤±è´¥: {e}")
                continue

            if masks.shape[0] == 0 or masks.shape[1] == 0:
                print(f"[åˆ†å‰²] å€™é€‰æ¡† {i} æœªç”Ÿæˆæ©ç ")
                continue

            m_bool = masks[0][0].detach().cpu().numpy().astype(np.uint8)
            mask_np = m_bool * 255
            # é™åˆ¶åœ¨ bbox å†…
            restricted_mask = np.zeros_like(mask_np, dtype=np.uint8)
            restricted_mask[y1:y2, x1:x2] = mask_np[y1:y2, x1:x2]
            mask_np = restricted_mask

            # è®¡ç®—ç‚¹äº‘å¹¶æ±‚è´¨å¿ƒæ·±åº¦ï¼ˆç›¸æœºåæ ‡ç³»ï¼Œå•ä½ç±³ï¼‰
            mask_bool = (mask_np > 0)
            if not np.any(mask_bool):
                print(f"[åˆ†å‰²] å€™é€‰æ¡† {i} æ©ç ä¸ºç©ºï¼Œè·³è¿‡")
                continue

            points, colors = self.generate_pointcloud(color_image, depth_image, mask_bool)
            if points is None or len(points) == 0:
                print(f"[ç‚¹äº‘] å€™é€‰æ¡† {i} ç‚¹äº‘ä¸ºç©ºï¼Œè·³è¿‡")
                continue

            centroid = np.mean(points, axis=0)  # (x,y,z) in meters (cam frame)
            depth_m = float(centroid[2])
            print(f"å€™é€‰æ¡† {i} è´¨å¿ƒæ·±åº¦: {depth_m:.4f} m  bbox=({x1},{y1},{x2},{y2})")

            # è®°å½•è°ƒè¯•è¾“å‡º
            if self.debug:
                cv2.imwrite(os.path.join(self.segmentation_dir, f"{base_name}_cand{i}_mask.png"), mask_np)
                det_vis = color_image.copy()
                cv2.rectangle(det_vis, (x1, y1), (x2, y2), (0, 255, 255), 2)
                cv2.putText(det_vis, f"cand {i}", (x1, max(0, y1-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)
                cv2.imwrite(os.path.join(self.detection_dir, f"{base_name}_cand{i}_box.png"), det_vis)

            if depth_m < best_depth_m:
                best_depth_m = depth_m
                best_mask = mask_np
                best_idx = i

        segmentation_time = time.time() - segmentation_start
        self.timers['segmentation'].append(segmentation_time)
        print(f"â±ï¸  segmentation(all): {segmentation_time:.3f}s")

        if best_idx == -1 or best_mask is None:
            print("åˆ†å‰²/ç‚¹äº‘å‡å¤±è´¥ï¼Œæœªé€‰å‡ºå€™é€‰")
            return None, None

        print(f"é€‰æ‹©æœ€è¿‘å€™é€‰: idx={best_idx}, æ·±åº¦={best_depth_m:.4f} m")

        if best_depth_m > 0.8:
            print(f"æ·±åº¦è¶…è¿‡0.8mï¼Œè·³è¿‡")
            return None, None

        # å¯è§†åŒ–æœ€ç»ˆé€‰æ‹©
        if self.debug:
            colored = np.zeros_like(color_image)
            colored[best_mask > 0] = [0, 255, 0]
            vis = cv2.addWeighted(color_image, 1.0, colored, 0.4, 0)
            vis_path = os.path.join(self.segmentation_dir, f"{base_name}_closest_overlay.png")
            cv2.imwrite(vis_path, vis)

        return best_mask, base_name



    def detect_and_segment_and_dump(self, color_image):
        """
        æœ¬åœ°å®Œæˆæ£€æµ‹->åˆ†å‰² è¿”å›ç”¨äºæ˜¾ç¤ºçš„å•é€šé“uint8æ©ç ï¼ˆ0/255ï¼‰ã€‚
        based on confidence score,åªé€‰æ‹©ä¸€æ¡é±¼è¿›è¡Œåˆ†å‰²ï¼Œæ— æ£€æµ‹æ—¶è¿”å›Noneã€‚
        """
        # æ£€æµ‹ï¼ˆåªé€‰æ‹©ä¸€æ¡é±¼ï¼‰
        detection_start = time.time()
        if getattr(self, 'use_yolo', False):
            boxes = self.detect_yolo(color_image, self.yolo_weights, conf=0.25, iou=0.45, imgsz=640)
        else:
            boxes = self._detect_boxes(color_image)
        detection_time = time.time() - detection_start
        self.timers['detection'].append(detection_time)
        print(f"â±ï¸  detection: {detection_time:.3f}s")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        base_name = f"frame_{self.frame_count:06d}_{timestamp}"
        
        # ä¿å­˜æ£€æµ‹å¯è§†åŒ–ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹ï¼‰
        if self.debug:
            det_vis = color_image.copy()
            if len(boxes) > 0:
                # åªæ ‡è®°é€‰ä¸­çš„é±¼ï¼ˆç»¿è‰²æ¡†ï¼‰
                x1, y1, x2, y2, confidence = boxes[0]
                cv2.rectangle(det_vis, (x1, y1), (x2, y2), (0, 255, 0), 3)  # ç»¿è‰²ç²—æ¡†è¡¨ç¤ºé€‰ä¸­çš„é±¼
                cv2.putText(det_vis, f"SELECTED (conf: {confidence:.2f})", (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                
                # ä¿å­˜é€‰ä¸­çš„é±¼çš„è£å‰ªå›¾åƒ
                crop = color_image[y1:y2, x1:x2]
                if crop.size > 0:
                    cv2.imwrite(os.path.join(self.detection_dir, f"{base_name}_selected_fish.png"), crop)
                
                cv2.imwrite(os.path.join(self.detection_dir, f"{base_name}_dino_detection.png"), det_vis)

        if not boxes:
            print("æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼Œè·³è¿‡åˆ†å‰²ã€‚")
            return None, None

        # åˆ†å‰²ï¼ˆSAMï¼‰- åªå¤„ç†é€‰ä¸­çš„ä¸€æ¡é±¼
        segmentation_start = time.time()
        try:
            image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)
            self.sam_predictor.set_image(image_rgb)
            
            # åªä½¿ç”¨é€‰ä¸­çš„è¾¹ç•Œæ¡†
            x1, y1, x2, y2, confidence = boxes[0]
            boxes_tensor = torch.tensor([[x1, y1, x2, y2]], device=self.device)
            transformed_boxes = self.sam_predictor.transform.apply_boxes_torch(boxes_tensor, image_rgb.shape[:2])

            # ä½¿ç”¨multimask_output=Falseç¡®ä¿åªè¿”å›ä¸€ä¸ªæ©ç 
            masks, scores, logits = self.sam_predictor.predict_torch(
                point_coords=None,
                point_labels=None,
                boxes=transformed_boxes,
                multimask_output=False  # åªè¿”å›ä¸€ä¸ªæœ€ä½³æ©ç 
            )

            # å¤„ç†é€‰ä¸­çš„é±¼çš„æ©ç  - ç¡®ä¿åªä½¿ç”¨ç¬¬ä¸€ä¸ªæ©ç 
            if masks.shape[0] > 0 and masks.shape[1] > 0:
                # åªå–ç¬¬ä¸€ä¸ªæ©ç ï¼ˆç´¢å¼•[0][0]ï¼‰
                m_bool = masks[0][0].cpu().numpy().astype(np.uint8)
                mask_np = m_bool * 255
                
                # è¿›ä¸€æ­¥é™åˆ¶æ©ç åªåœ¨è¾¹ç•Œæ¡†åŒºåŸŸå†…
                # åˆ›å»ºä¸€ä¸ªå…¨é›¶æ©ç 
                restricted_mask = np.zeros_like(mask_np)
                # åªåœ¨è¾¹ç•Œæ¡†åŒºåŸŸå†…åº”ç”¨æ©ç 
                restricted_mask[y1:y2, x1:x2] = mask_np[y1:y2, x1:x2]
                mask_np = restricted_mask
                
                # ä¿å­˜æ©ç ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹ï¼‰
                if self.debug:
                    mask_path = os.path.join(self.segmentation_dir, f"{base_name}_selected_fish_mask.png")
                    cv2.imwrite(mask_path, mask_np)
                    
                    # ä¿å­˜è£å‰ªæ©ç 
                    mask_crop = mask_np[y1:y2, x1:x2]
                    if mask_crop.size > 0:
                        mask_crop_path = os.path.join(self.segmentation_dir, f"{base_name}_selected_fish_mask_crop.png")
                        cv2.imwrite(mask_crop_path, mask_crop)
                    
                    # ä¿å­˜è£å‰ªå¯è§†åŒ–
                    crop = color_image[y1:y2, x1:x2]
                    if crop.size > 0 and mask_crop.size > 0:
                        overlay = np.zeros_like(crop)
                        overlay[mask_crop > 0] = [0, 255, 0]
                        vis_crop = cv2.addWeighted(crop, 1.0, overlay, 0.4, 0)
                        vis_crop_path = os.path.join(self.segmentation_dir, f"{base_name}_selected_fish_vis.png")
                        cv2.imwrite(vis_crop_path, vis_crop)
                    
                    # ä¿å­˜æ•´ä½“å¯è§†åŒ–
                    colored = np.zeros_like(color_image)
                    colored[mask_np > 0] = [0, 255, 0]
                    vis = cv2.addWeighted(color_image, 1.0, colored, 0.4, 0)
                    vis_path = os.path.join(self.segmentation_dir, f"{base_name}_selected_fish_overlay.png")
                    cv2.imwrite(vis_path, vis)
                
                segmentation_time = time.time() - segmentation_start
                self.timers['segmentation'].append(segmentation_time)
                print(f"â±ï¸  segmentation: {segmentation_time:.3f}s")
                
                print(f"æˆåŠŸåˆ†å‰²é€‰ä¸­çš„é±¼ï¼Œæ©ç ç‚¹æ•°: {np.sum(mask_np > 0)}")
                print(f"æ©ç é™åˆ¶åœ¨è¾¹ç•Œæ¡†å†…: ({x1}, {y1}) åˆ° ({x2}, {y2})")
                return mask_np, base_name
            else:
                segmentation_time = time.time() - segmentation_start
                self.timers['segmentation'].append(segmentation_time)
                print(f"â±ï¸  segmentation: {segmentation_time:.3f}s")
                print("åˆ†å‰²å¤±è´¥ï¼Œæœªç”Ÿæˆæ©ç ")
                return None, None
                
        except Exception as e:
            segmentation_time = time.time() - segmentation_start
            self.timers['segmentation'].append(segmentation_time)
            print(f"â±ï¸  segmentation: {segmentation_time:.3f}s")
            print(f"åˆ†å‰²æ—¶å‡ºé”™: {e}")
            return None, None

    def _detect_boxes(self, color_image):
        """
        ä½¿ç”¨ä¸ seg.py ç›¸åŒçš„æ–¹å¼è¿›è¡Œæ£€æµ‹ï¼Œè¿”å›bboxåˆ—è¡¨
        åªé€‰æ‹©ä¸€æ¡é±¼è¿›è¡Œåˆ†å‰²å’ŒæŠ“å–
        """
        # è½¬æ¢ä¸ºPILå›¾åƒï¼ˆä¸ seg.py ä¸€è‡´ï¼‰
        image_pil = Image.fromarray(cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB))
        text_prompt = "fish. crab. marine animal"
        inputs = self.processor(images=image_pil, text=text_prompt, return_tensors="pt").to(self.device)
        with torch.no_grad():
            outputs = self.grounding_dino_model(**inputs)
        h, w = color_image.shape[0], color_image.shape[1]
        results = self.processor.post_process_grounded_object_detection(
            outputs,
            inputs.input_ids,
            text_threshold=0.3,
            # ä¸ seg.py ç›¸åŒçš„å°ºå¯¸ä¼ å…¥æ–¹å¼
            target_sizes=[image_pil.size[::-1]]
        )
        result = results[0]
        boxes = []
        print("\næ£€æµ‹ç»“æœè¯¦æƒ…:")
        print(f"æ£€æµ‹åˆ°çš„ç›®æ ‡æ•°é‡: {len(result['boxes'])}")
        if len(result["boxes"]) == 0:
            return boxes
        
        # è¿‡æ»¤è¾¹ç•Œæ¡†ï¼šé¢ç§¯å¿…é¡»å¤§äº1000åƒç´ 
        valid_boxes = []
        for box in result["boxes"]:
            x1, y1, x2, y2 = [int(c) for c in box.tolist()]
            x1 = max(0, min(x1, w - 1))
            y1 = max(0, min(y1, h - 1))
            x2 = max(0, min(x2, w - 1))
            y2 = max(0, min(y2, h - 1))
            
            # è®¡ç®—è¾¹ç•Œæ¡†é¢ç§¯
            area = (x2 - x1) * (y2 - y1)
            if area > 1000:  # é¢ç§¯è¿‡æ»¤
                valid_boxes.append(((x1, y1, x2, y2), area))
        
        if valid_boxes:
            # æ ¹æ®é€‰æ‹©ç­–ç•¥é€‰æ‹©è¾¹ç•Œæ¡†
            if self.bbox_selection == "smallest":
                selected_box = min(valid_boxes, key=lambda x: x[1])
                selection_type = "é¢ç§¯æœ€å°çš„"
            elif self.bbox_selection == "largest":
                selected_box = max(valid_boxes, key=lambda x: x[1])
                selection_type = "é¢ç§¯æœ€å¤§çš„"
            else:
                # é»˜è®¤é€‰æ‹©æœ€å°çš„
                selected_box = min(valid_boxes, key=lambda x: x[1])
                selection_type = "é¢ç§¯æœ€å°çš„"
                print(f"è­¦å‘Š: æœªçŸ¥çš„é€‰æ‹©ç­–ç•¥ '{self.bbox_selection}'ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥ 'smallest'")
            
            boxes.append(selected_box[0])
            print(f"æ£€æµ‹åˆ° {len(valid_boxes)} æ¡é±¼ï¼Œé€‰æ‹©{selection_type}è¿›è¡ŒæŠ“å–ï¼Œé¢ç§¯: {selected_box[1]} åƒç´ ")
            print(f"é€‰æ‹©çš„é±¼ä½ç½®: {selected_box[0]}")
        else:
            print("æ²¡æœ‰æ»¡è¶³é¢ç§¯è¦æ±‚çš„è¾¹ç•Œæ¡†")
        
        return boxes

    def detect_yolo(self, color_image, yolo_weights_path, conf=0.5, iou=0.45, imgsz=640, min_area=1000):
        """
        ä½¿ç”¨Ultralytics YOLOè¿›è¡Œé±¼çš„æ£€æµ‹ï¼Œè¿”å›æ‰€æœ‰æ£€æµ‹åˆ°çš„bboxã€‚
        
        Args:
            color_image: OpenCV BGRå›¾åƒ (H,W,3)
            yolo_weights_path: è®­ç»ƒå¥½çš„YOLOæƒé‡ .pt è·¯å¾„
            conf: ç½®ä¿¡åº¦é˜ˆå€¼
            iou: NMS IOU é˜ˆå€¼
            imgsz: æ¨ç†è¾“å…¥å°ºå¯¸
            min_area: è¿‡æ»¤æœ€å°é¢ç§¯ï¼ˆåƒç´ ï¼‰
        
        Returns:
            boxes: List[Tuple[x1, y1, x2, y2, confidence]] æ‰€æœ‰æ»¡è¶³æ¡ä»¶çš„bboxï¼›è‹¥æ— åˆ™è¿”å›ç©ºåˆ—è¡¨
        """
        try:
            from ultralytics import YOLO
        except Exception as e:
            print("[é”™è¯¯] æœªæ‰¾åˆ° ultralyticsï¼Œè¯·å…ˆ: pip install ultralytics")
            print(e)
            return []

        # åŠ è½½æ¨¡å‹ï¼ˆæ¯æ¬¡è°ƒç”¨åŠ è½½é¿å…ä¸å…¶ä»–ä¾èµ–å†²çªï¼›è‹¥é¢‘ç¹è°ƒç”¨å¯å¤–éƒ¨ç¼“å­˜æ¨¡å‹å®ä¾‹ï¼‰
        try:
            model = YOLO(yolo_weights_path)
        except Exception as e:
            print(f"[é”™è¯¯] åŠ è½½YOLOæƒé‡å¤±è´¥: {yolo_weights_path} -> {e}")
            return []

        # YOLOæ”¯æŒç›´æ¥ä¼ å…¥numpyå›¾åƒï¼›ç¡®ä¿ä¸ºRGB
        #image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)
        try:
            results = model.predict(
                source=[color_image],
                imgsz=imgsz,
                conf=conf,
                iou=iou,
                verbose=False,
                save=False
            )
        except Exception as e:
            print(f"[é”™è¯¯] YOLO æ¨ç†å¤±è´¥: {e}")
            return []

        if not results:
            return []

        res = results[0]
        boxes_np = None
        confidences = None
        try:
            # xyxy (N,4) å’Œ conf (N,)
            boxes_np = res.boxes.xyxy.cpu().numpy() if hasattr(res, 'boxes') and res.boxes is not None else None
            confidences = res.boxes.conf.cpu().numpy() if hasattr(res, 'boxes') and res.boxes is not None else None
        except Exception:
            boxes_np = None
            confidences = None

        if boxes_np is None or len(boxes_np) == 0:
            return []

        # è¿‡æ»¤é¢ç§¯ã€è£å‰ªåˆ°å›¾åƒèŒƒå›´ï¼ŒåŒæ—¶ä¿å­˜ç½®ä¿¡åº¦ä¿¡æ¯
        H, W = color_image.shape[0], color_image.shape[1]
        valid_boxes = []
        for i, xyxy in enumerate(boxes_np):
            x1, y1, x2, y2 = [int(round(v)) for v in xyxy[:4].tolist()]
            x1 = max(0, min(x1, W - 1))
            y1 = max(0, min(y1, H - 1))
            x2 = max(0, min(x2, W - 1))
            y2 = max(0, min(y2, H - 1))
            area = max(0, x2 - x1) * max(0, y2 - y1)
            if area > min_area:
                confidence = confidences[i] if confidences is not None else 0.0
                valid_boxes.append(((x1, y1, x2, y2), area, confidence))

        boxes = []
        if valid_boxes:
            # è¿”å›æ‰€æœ‰æ£€æµ‹åˆ°çš„bboxï¼ŒåŒ…å«ç½®ä¿¡åº¦ä¿¡æ¯
            for bbox_info in valid_boxes:
                bbox_coords, area, confidence = bbox_info
                # è¿”å›æ ¼å¼: (x1, y1, x2, y2, confidence)
                boxes.append((*bbox_coords, confidence))
            
            print(f"[YOLO] æ£€æµ‹åˆ° {len(valid_boxes)} ä¸ªå€™é€‰æ¡†ï¼Œå…¨éƒ¨è¿”å›ç”¨äºå¤„ç†")
            for i, (bbox_coords, area, confidence) in enumerate(valid_boxes):
                print(f"[YOLO] æ¡† {i+1}: {bbox_coords}, ç½®ä¿¡åº¦: {confidence:.3f}, é¢ç§¯: {area} åƒç´ ")
        else:
            print("[YOLO] æ²¡æœ‰æ»¡è¶³é¢ç§¯è¦æ±‚çš„è¾¹ç•Œæ¡†")

        return boxes
    
    def detect_yolo_all(self, color_image, yolo_weights_path, conf=0.5, iou=0.45, imgsz=640):
        """
        ä½¿ç”¨Ultralytics YOLOå¯¹å•å¸§è¿›è¡Œæ¨ç†ï¼Œè¿”å›æ‰€æœ‰æ£€æµ‹åˆ°çš„bboxï¼ˆä¸åšé¢ç§¯è¿‡æ»¤ä¸å•æ¡†é€‰æ‹©ï¼‰ã€‚
        è¿”å›ï¼šList[Tuple[int,int,int,int,float,int]] -> (x1,y1,x2,y2,conf,cls)
        """
        try:
            from ultralytics import YOLO
        except Exception as e:
            print("[é”™è¯¯] æœªæ‰¾åˆ° ultralyticsï¼Œè¯·å…ˆ: pip install ultralytics")
            print(e)
            return []

        # åŠ è½½æ¨¡å‹ï¼ˆç®€åŒ–ä¸ºæ¯æ¬¡åŠ è½½ï¼›å¦‚éœ€ä¼˜åŒ–å¯åœ¨å¤–éƒ¨ç¼“å­˜ï¼‰
        try:
            model = YOLO(yolo_weights_path)
        except Exception as e:
            print(f"[é”™è¯¯] åŠ è½½YOLOæƒé‡å¤±è´¥: {yolo_weights_path} -> {e}")
            return []

        # BGR -> RGB
        #image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)
        try:
            results = model.predict(
                source=[color_image],
                imgsz=imgsz,
                conf=conf,
                iou=iou,
                device=(0 if self.device == 'cuda' else 'cpu'),
                verbose=False,
                save=False,
            )
        except Exception as e:
            print(f"[é”™è¯¯] YOLO æ¨ç†å¤±è´¥: {e}")
            return []

        if not results:
            print("[YOLO] æ— æ£€æµ‹ç»“æœ")
            return []

        res = results[0]
        if not hasattr(res, 'boxes') or res.boxes is None or res.boxes.shape[0] == 0:
            print("[YOLO] boxes ä¸ºç©º")
            return []

        xyxy = res.boxes.xyxy.cpu().numpy()  # (N,4)
        conf_arr = res.boxes.conf.cpu().numpy() if hasattr(res.boxes, 'conf') else None
        cls_arr = res.boxes.cls.cpu().numpy() if hasattr(res.boxes, 'cls') else None

        all_boxes = []
        for i, b in enumerate(xyxy):
            x1, y1, x2, y2 = [int(round(v)) for v in b[:4].tolist()]
            conf_v = float(conf_arr[i]) if conf_arr is not None else 0.0
            cls_v = int(cls_arr[i]) if cls_arr is not None else -1
            all_boxes.append((x1, y1, x2, y2, conf_v, cls_v))

        print(f"[YOLO] æ£€æµ‹åˆ° {len(all_boxes)} ä¸ªæ¡†ï¼ˆconf>={conf}ï¼‰ï¼šå‰3ä¸ª: {all_boxes[:3]}")
        return all_boxes

    def dump_detections(self, color_image):
        """
        å°†æ£€æµ‹åˆ°çš„ç›®æ ‡è£å‰ªå¹¶ä¿å­˜åˆ° detection/ ç›®å½•
        """
        boxes = self._detect_boxes(color_image)
        if not boxes:
            return 0
        base_ts = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        saved = 0
        for idx, (x1, y1, x2, y2) in enumerate(boxes):
            crop = color_image[y1:y2, x1:x2]
            if crop.size == 0:
                continue
            filename = f"frame_{self.frame_count:06d}_{base_ts}_det_{idx}.png"
            path = os.path.join(self.detection_dir, filename)
            cv2.imwrite(path, crop)
            saved += 1
        if saved:
            print(f"å·²ä¿å­˜ {saved} ä¸ªæ£€æµ‹è£å‰ªåˆ°: {self.detection_dir}")
        return saved

    
    def generate_pointcloud(self, color_image, depth_image, mask):
        """
        ä»æ©ç ç”Ÿæˆ3Dç‚¹äº‘
        
        Args:
            color_image: RGBå›¾åƒ
            depth_image: æ·±åº¦å›¾åƒ (æ¯«ç±³)
            mask: åˆ†å‰²æ©ç 
            
        Returns:
            points: 3Dç‚¹åæ ‡
            colors: RGBé¢œè‰²
        """
        try:
            # è½¬æ¢æ·±åº¦å›¾åƒå•ä½ä¸ºç±³
            depth_image_meters = depth_image.astype(np.float32) / 1000.0
            
            # è½¬æ¢ä¸ºRGBæ ¼å¼
            color_image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)
            
            # ä½¿ç”¨mask_to_3d_pointcloudå‡½æ•°ï¼ˆæ”¯æŒç•¸å˜æ ¡æ­£ï¼‰
            points, colors = mask_to_3d_pointcloud(
                color_image_rgb, 
                depth_image_meters, 
                mask, 
                self.fx, self.fy, self.cx, self.cy,
                self.mtx, self.dist
            )
            
            return points, colors
            
        except Exception as e:
            print(f"ç”Ÿæˆç‚¹äº‘æ—¶å‡ºé”™: {e}")
            return np.array([]), np.array([])

    def apply_hand_eye_transform(self, points):
        """
        å°†ç‚¹äº‘ä»ç›¸æœºç³»è½¬æ¢åˆ°å¤¹çˆªç³»ï¼Œä½¿ç”¨ self.hand_eye_transform (4x4)ã€‚
        æ—‹è½¬çŸ©é˜µï¼š
            [[-0.99462885  0.07149648  0.07484454]
            [-0.06962775 -0.9971997   0.02728984]
            [ 0.07658608  0.021932    0.99682173]]
            å¹³ç§»å‘é‡ï¼š
            [[ 0.0247092 ]
            [ 0.09912939]
            [-0.25357213]]
        """
        if self.hand_eye_transform is None or points.size == 0:
            return points
        ones = np.ones((points.shape[0], 1), dtype=np.float32)
        homo = np.hstack([points.astype(np.float32), ones])  # (N,4)
        transformed = (self.hand_eye_transform @ homo.T).T  # (N,4)
        return transformed[:, :3]

    def _rpy_to_rotation_matrix(self, rx, ry, rz):
        """
        å°†æœ«ç«¯çš„ RPY (rx, ry, rz) è½¬ä¸ºæ—‹è½¬çŸ©é˜µ R (åŸºåº§â†’æœ«ç«¯)ã€‚
        é‡‡ç”¨å¸¸è§çš„å¤–æ—‹é¡ºåº R = Rz @ Ry @ Rxã€‚
        """
        sx, cx = np.sin(rx), np.cos(rx)
        sy, cy = np.sin(ry), np.cos(ry)
        sz, cz = np.sin(rz), np.cos(rz)

        Rx = np.array([[1, 0, 0],
                       [0, cx, -sx],
                       [0, sx,  cx]], dtype=np.float32)
        Ry = np.array([[ cy, 0, sy],
                       [  0, 1,  0],
                       [-sy, 0, cy]], dtype=np.float32)
        Rz = np.array([[cz, -sz, 0],
                       [sz,  cz, 0],
                       [ 0,   0, 1]], dtype=np.float32)

        return (Rz @ Ry @ Rx).astype(np.float32)

    def _tool_offset_to_base(self, delta_tool_xyz_mm, tcp_rpy):
        """
        å°†å¤¹çˆª(å·¥å…·)åæ ‡ç³»ä¸‹çš„ä½ç§»(mm)è½¬æ¢åˆ°åŸºåæ ‡ç³»ä¸‹çš„ä½ç§»(mm)ã€‚
        delta_tool_xyz_mm: [dx, dy, dz] in tool frame
        tcp_rpy: [rx, ry, rz] in radians
        è¿”å›: [dx_base, dy_base, dz_base]
        """
        rx, ry, rz = tcp_rpy
        R_base_tool = self._rpy_to_rotation_matrix(rx, ry, rz)
        delta_tool = np.asarray(delta_tool_xyz_mm, dtype=np.float32).reshape(3, 1)
        delta_base = (R_base_tool @ delta_tool).reshape(3)
        return delta_base.tolist()

    def calculate_pointcloud_bbox(self, points):
        """
        è®¡ç®—ç‚¹äº‘çš„è¾¹ç•Œæ¡†ä¿¡æ¯ï¼Œç”¨äºé«˜åº¦å’Œå§¿æ€ä¼°è®¡
        
        Args:
            points: ç‚¹äº‘åæ ‡ (N, 3)
            
        Returns:
            bbox_info: å­—å…¸åŒ…å«ä¸­å¿ƒç‚¹ã€å°ºå¯¸ã€è¾¹ç•Œæ¡†ç­‰
        """
        if points.size == 0:
            return None
            
        # è®¡ç®—è¾¹ç•Œæ¡†
        min_coords = np.min(points, axis=0)  # [min_x, min_y, min_z]
        max_coords = np.max(points, axis=0)  # [max_x, max_y, max_z]
        
        # è®¡ç®—ä¸­å¿ƒç‚¹
        center = (min_coords + max_coords) / 2.0  # [center_x, center_y, center_z]
        
        # è®¡ç®—å°ºå¯¸
        dimensions = max_coords - min_coords  # [width, height, depth]
        
        # è®¡ç®—é«˜åº¦ï¼ˆzæ–¹å‘ï¼‰
        height = dimensions[2]  # zæ–¹å‘çš„é«˜åº¦
        
        # è®¡ç®—8ä¸ªè§’ç‚¹
        corners = []
        for x in [min_coords[0], max_coords[0]]:
            for y in [min_coords[1], max_coords[1]]:
                for z in [min_coords[2], max_coords[2]]:
                    corners.append([x, y, z])
        corners = np.array(corners)
        
        # è®¡ç®—ç‚¹äº‘çš„ä¸»æ–¹å‘ï¼ˆPCAï¼‰
        try:
            from sklearn.decomposition import PCA
            pca = PCA(n_components=3)
            pca.fit(points)
            principal_axes = pca.components_  # ä¸»æ–¹å‘å‘é‡
            explained_variance = pca.explained_variance_ratio_  # è§£é‡Šæ–¹å·®æ¯”ä¾‹
        except ImportError:
            print("sklearnæœªå®‰è£…ï¼Œè·³è¿‡PCAå§¿æ€ä¼°è®¡")
            principal_axes = np.eye(3)
            explained_variance = [1.0, 0.0, 0.0]
        
        bbox_info = {
            'center': center,
            'dimensions': dimensions,
            'height': height,
            'min_coords': min_coords,
            'max_coords': max_coords,
            'corners': corners,
            'principal_axes': principal_axes,
            'explained_variance': explained_variance,
            'num_points': len(points)
        }
        
        return bbox_info

    def calculate_surface_normal(self, points, method='pca'):
        """
        è®¡ç®—ç‚¹äº‘è´¨å¿ƒå¤„çš„è¡¨é¢æ³•å‘é‡
        
        Args:
            points: ç‚¹äº‘åæ ‡ (N, 3)
            method: æ³•å‘é‡è®¡ç®—æ–¹æ³• ('pca', 'plane_fitting', 'nearest_neighbors')
            
        Returns:
            normal: æ³•å‘é‡ (3,) å•ä½å‘é‡
            centroid: è´¨å¿ƒåæ ‡ (3,)
        """
        if points.size == 0 or len(points) < 3:
            return np.array([0, 0, 1]), np.array([0, 0, 0])
        
        centroid = np.mean(points, axis=0)
        
        if method == 'pca':
            # ä½¿ç”¨PCAè®¡ç®—æ³•å‘é‡
            try:
                from sklearn.decomposition import PCA
                pca = PCA(n_components=3)
                pca.fit(points)
                # æœ€å°ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡å°±æ˜¯æ³•å‘é‡
                normal = pca.components_[2]  # ç¬¬ä¸‰ä¸ªä¸»æˆåˆ†ï¼ˆæœ€å°æ–¹å·®æ–¹å‘ï¼‰
            except ImportError:
                print("sklearnæœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•å¹³é¢æ‹Ÿåˆ")
                return self._simple_plane_fitting(points, centroid)
        
        elif method == 'plane_fitting':
            return self._simple_plane_fitting(points, centroid)
        
        elif method == 'nearest_neighbors':
            return self._nearest_neighbors_normal(points, centroid)
        
        else:
            raise ValueError(f"æœªçŸ¥çš„æ³•å‘é‡è®¡ç®—æ–¹æ³•: {method}")
        
        # ç¡®ä¿æ³•å‘é‡æŒ‡å‘æ­£ç¡®çš„æ–¹å‘ï¼ˆé€šå¸¸æŒ‡å‘ç›¸æœºæ–¹å‘ï¼‰
        # å¦‚æœæ³•å‘é‡çš„zåˆ†é‡ä¸ºè´Ÿï¼Œåˆ™ç¿»è½¬æ–¹å‘
        if normal[2] < 0:
            normal = -normal
        
        # å½’ä¸€åŒ–
        normal = normal / np.linalg.norm(normal)
        
        return normal, centroid

    def _simple_plane_fitting(self, points, centroid):
        """
        ä½¿ç”¨ç®€å•å¹³é¢æ‹Ÿåˆè®¡ç®—æ³•å‘é‡
        """
        # å°†ç‚¹äº‘ä¸­å¿ƒåŒ–
        centered_points = points - centroid
        
        # æ„å»ºåæ–¹å·®çŸ©é˜µ
        cov_matrix = np.cov(centered_points.T)
        
        # è®¡ç®—ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
        
        # æœ€å°ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡å°±æ˜¯æ³•å‘é‡
        normal = eigenvectors[:, 0]  # æœ€å°ç‰¹å¾å€¼å¯¹åº”çš„ç‰¹å¾å‘é‡
        
        # ç¡®ä¿æ³•å‘é‡æŒ‡å‘æ­£ç¡®çš„æ–¹å‘
        if normal[2] < 0:
            normal = -normal
        
        # å½’ä¸€åŒ–
        normal = normal / np.linalg.norm(normal)
        
        return normal, centroid

    def _nearest_neighbors_normal(self, points, centroid, k=20):
        """
        ä½¿ç”¨æœ€è¿‘é‚»æ–¹æ³•è®¡ç®—æ³•å‘é‡
        """
        # è®¡ç®—æ¯ä¸ªç‚¹åˆ°è´¨å¿ƒçš„è·ç¦»
        distances = np.linalg.norm(points - centroid, axis=1)
        
        # æ‰¾åˆ°æœ€è¿‘çš„kä¸ªç‚¹
        nearest_indices = np.argsort(distances)[:k]
        nearest_points = points[nearest_indices]
        
        # ä½¿ç”¨è¿™äº›æœ€è¿‘é‚»ç‚¹è¿›è¡Œå¹³é¢æ‹Ÿåˆ
        return self._simple_plane_fitting(nearest_points, centroid)

    def normal_to_rpy(self, normal_vector, current_rpy=None):
        """
        å°†æ³•å‘é‡è½¬æ¢ä¸ºæœºå™¨äººæœ«ç«¯å§¿æ€çš„RPYè§’åº¦
        
        Args:
            normal_vector: æ³•å‘é‡ (3,) å•ä½å‘é‡ï¼Œè¡¨ç¤ºæœŸæœ›çš„Zè½´æ–¹å‘
            current_rpy: å½“å‰RPYè§’åº¦ [rx, ry, rz] (å¯é€‰ï¼Œç”¨äºå¹³æ»‘è¿‡æ¸¡)
            
        Returns:
            target_rpy: ç›®æ ‡RPYè§’åº¦ [rx, ry, rz]
        """
        # æœŸæœ›çš„Zè½´æ–¹å‘ï¼ˆæ³•å‘é‡ï¼‰
        z_target = normal_vector / np.linalg.norm(normal_vector)
        
        # å®šä¹‰å‚è€ƒåæ ‡ç³»ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
        # è¿™é‡Œå‡è®¾Xè½´æŒ‡å‘æœºå™¨äººå‰æ–¹ï¼ŒYè½´æŒ‡å‘æœºå™¨äººå·¦ä¾§
        x_ref = np.array([1, 0, 0])  # å‚è€ƒXè½´
        y_ref = np.array([0, 1, 0])  # å‚è€ƒYè½´
        
        # è®¡ç®—æ–°çš„åæ ‡ç³»
        # Zè½´ = æ³•å‘é‡
        z_new = z_target
        
        # Xè½´ = å‚è€ƒXè½´åœ¨å‚ç›´äºZè½´çš„å¹³é¢ä¸Šçš„æŠ•å½±
        x_new = x_ref - np.dot(x_ref, z_new) * z_new
        x_new = x_new / np.linalg.norm(x_new)
        
        # Yè½´ = Zè½´ Ã— Xè½´
        y_new = np.cross(z_new, x_new)
        y_new = y_new / np.linalg.norm(y_new)
        
        # æ„å»ºæ—‹è½¬çŸ©é˜µ
        R = np.column_stack([x_new, y_new, z_new])
        
        # å°†æ—‹è½¬çŸ©é˜µè½¬æ¢ä¸ºRPYè§’åº¦
        # ä½¿ç”¨ZYXæ¬§æ‹‰è§’é¡ºåºï¼ˆRoll-Pitch-Yawï¼‰
        sy = np.sqrt(R[0, 0] * R[0, 0] + R[1, 0] * R[1, 0])
        
        singular = sy < 1e-6
        
        if not singular:
            rx = np.arctan2(R[2, 1], R[2, 2])  # Roll
            ry = np.arctan2(-R[2, 0], sy)      # Pitch
            rz = np.arctan2(R[1, 0], R[0, 0])  # Yaw
        else:
            rx = np.arctan2(-R[1, 2], R[1, 1])  # Roll
            ry = np.arctan2(-R[2, 0], sy)       # Pitch
            rz = 0                               # Yaw
        
        target_rpy = np.array([rx, ry, rz])
        
        # å¦‚æœæä¾›äº†å½“å‰RPYï¼Œè¿›è¡Œå¹³æ»‘è¿‡æ¸¡
        if current_rpy is not None:
            target_rpy = self._smooth_rpy_transition(current_rpy, target_rpy)
        
        return target_rpy

    def _smooth_rpy_transition(self, current_rpy, target_rpy, max_change=0.1):
        """
        å¹³æ»‘RPYè§’åº¦è¿‡æ¸¡ï¼Œé¿å…çªå˜
        
        Args:
            current_rpy: å½“å‰RPYè§’åº¦
            target_rpy: ç›®æ ‡RPYè§’åº¦
            max_change: å•æ¬¡æœ€å¤§å˜åŒ–é‡ï¼ˆå¼§åº¦ï¼‰
            
        Returns:
            smoothed_rpy: å¹³æ»‘åçš„RPYè§’åº¦
        """
        current_rpy = np.array(current_rpy)
        target_rpy = np.array(target_rpy)
        
        # è®¡ç®—è§’åº¦å·®
        diff = target_rpy - current_rpy
        
        # å¤„ç†è§’åº¦è·³è·ƒï¼ˆÂ±Ï€ï¼‰
        for i in range(3):
            if diff[i] > np.pi:
                diff[i] -= 2 * np.pi
            elif diff[i] < -np.pi:
                diff[i] += 2 * np.pi
        
        # é™åˆ¶å˜åŒ–é‡
        for i in range(3):
            if abs(diff[i]) > max_change:
                diff[i] = np.sign(diff[i]) * max_change
        
        # è®¡ç®—å¹³æ»‘åçš„è§’åº¦
        smoothed_rpy = current_rpy + diff
        
        return smoothed_rpy

    def estimate_fish_weight(self, points_gripper, volume_factor: float = 1.0) -> float:
        """
        æ ¹æ®ç‚¹äº‘ä¼°ç®—é±¼çš„é‡é‡
        
        Args:
            points_gripper: å¤¹çˆªåæ ‡ç³»ä¸­çš„ç‚¹äº‘ (N, 3)
            volume_factor: ä½“ç§¯åˆ°é‡é‡çš„è½¬æ¢å› å­ (kg/mÂ³)
            
        Returns:
            weight_kg: ä¼°ç®—çš„é±¼é‡é‡ï¼ˆåƒå…‹ï¼‰
        """
        if points_gripper.size == 0 or len(points_gripper) < 3:
            return 0.0
        
        # è®¡ç®—ç‚¹äº‘çš„è¾¹ç•Œæ¡†ä½“ç§¯
        min_coords = np.min(points_gripper, axis=0)
        max_coords = np.max(points_gripper, axis=0)
        dimensions = max_coords - min_coords
        
        # è®¡ç®—ä½“ç§¯ï¼ˆç«‹æ–¹ç±³ï¼‰
        volume_m3 = np.prod(dimensions)
        
        # åº”ç”¨å½¢çŠ¶å› å­ï¼ˆé±¼ä¸æ˜¯å®Œç¾çš„çŸ©å½¢ï¼‰
        shape_factor = 0.6  # ç»éªŒå€¼ï¼Œé±¼çš„å®é™…ä½“ç§¯çº¦ä¸ºè¾¹ç•Œæ¡†çš„60%
        effective_volume = volume_m3 * shape_factor
        
        # ä¼°ç®—é‡é‡ï¼ˆå‡è®¾é±¼çš„å¯†åº¦çº¦ä¸º1000 kg/mÂ³ï¼‰
        fish_density = 1000.0  # kg/mÂ³
        weight_kg = effective_volume * fish_density * volume_factor
        
        # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
        weight_kg = max(0.1, min(weight_kg, 2.0))  # 0.1kg åˆ° 2.0kg
        
        return weight_kg

    def calculate_grasp_pose_with_normal(self, points_gripper, current_tcp):
        """
        è®¡ç®—è€ƒè™‘æ³•å‘é‡çš„æŠ“å–å§¿æ€
        
        Args:
            points_gripper: å¤¹çˆªåæ ‡ç³»ä¸­çš„ç‚¹äº‘ (N, 3)
            current_tcp: å½“å‰TCPä½ç½® [x, y, z, rx, ry, rz]
            
        Returns:
            grasp_pose: æŠ“å–å§¿æ€ [x, y, z, rx, ry, rz]
            normal_info: æ³•å‘é‡ä¿¡æ¯å­—å…¸
        """
        if points_gripper.size == 0 or len(points_gripper) < 3:
            print("ç‚¹äº‘ç‚¹æ•°ä¸è¶³ï¼Œæ— æ³•è®¡ç®—æ³•å‘é‡")
            return current_tcp, None
        
        # è®¡ç®—è´¨å¿ƒå’Œæ³•å‘é‡
        normal, centroid = self.calculate_surface_normal(points_gripper, method='pca')
        
        print(f"è´¨å¿ƒåæ ‡: {centroid}")
        print(f"æ³•å‘é‡: {normal}")
        
        # å°†æ³•å‘é‡è½¬æ¢ä¸ºRPYè§’åº¦
        current_rpy = current_tcp[3:6]
        target_rpy = self.normal_to_rpy(normal, current_rpy)
        
        print(f"å½“å‰RPY: {np.degrees(current_rpy)} åº¦")
        print(f"ç›®æ ‡RPY: {np.degrees(target_rpy)} åº¦")
        

        delta_tool_mm = [centroid[0] * 1000, centroid[1] * 1000, centroid[2] * 1000]
        delta_base_xyz = self._tool_offset_to_base(delta_tool_mm, current_tcp[3:6])

        # æ„å»ºæŠ“å–å§¿æ€
        grasp_pose = np.array([
            delta_base_xyz[0],  # è½¬æ¢ä¸ºæ¯«ç±³
            delta_base_xyz[1],
            delta_base_xyz[2] -25 , # move a bit deeper  to make sure the gripper is attached with the object
            target_rpy[0],       # ä¿æŒå¼§åº¦
            target_rpy[1],
            target_rpy[2]
        ])
        
        # æ³•å‘é‡ä¿¡æ¯
        normal_info = {
            'centroid': centroid,
            'normal': normal,
            'current_rpy': current_rpy,
            'target_rpy': target_rpy,
            'rpy_change': target_rpy - current_rpy
        }
        
        return grasp_pose, normal_info
    
    def save_results(self, color_image, depth_image, mask, points, colors):
        """
        ä¿å­˜æ‰€æœ‰ç»“æœ
        
        Args:
            color_image: RGBå›¾åƒ
            depth_image: æ·±åº¦å›¾åƒ
            mask: åˆ†å‰²æ©ç 
            points: 3Dç‚¹åæ ‡
            colors: RGBé¢œè‰²
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        base_name = f"frame_{self.frame_count:06d}_{timestamp}"
        
        # ä¿å­˜RGBå›¾åƒ
        rgb_path = os.path.join(self.rgb_dir, f"{base_name}.png")
        cv2.imwrite(rgb_path, color_image)
        
        # ä¿å­˜æ·±åº¦å›¾åƒ
        depth_path = os.path.join(self.depth_dir, f"{base_name}.png")
        cv2.imwrite(depth_path, depth_image.astype(np.uint16))
        
        # ä¿å­˜æ©ç 
        if mask is not None:
            mask_path = os.path.join(self.mask_dir, f"{base_name}_mask.png")
            cv2.imwrite(mask_path, mask.astype(np.uint8) * 255)
            
            # åˆ›å»ºå¯è§†åŒ–ç»“æœ
            colored_mask = np.zeros_like(color_image)
            colored_mask[mask] = [0, 255, 0]  # ç»¿è‰²æ©ç 
            alpha = 0.5
            visualization = cv2.addWeighted(color_image, 1, colored_mask, alpha, 0)
            vis_path = os.path.join(self.segmentation_dir, f"{base_name}_vis.png")
            cv2.imwrite(vis_path, visualization)
        
        # ä¿å­˜ç‚¹äº‘
        if self.save_pointcloud and len(points) > 0:
            pointcloud_path = os.path.join(self.pointcloud_dir, f"{base_name}_pointcloud.ply")
            save_pointcloud_to_file(points, colors, pointcloud_path)
        
        print(f"å·²ä¿å­˜ç¬¬ {self.frame_count} å¸§ç»“æœ")
    
    def show_preview(self, color_image, depth_image, mask, detection_vis=None, landmark_vis=None):
        """
        åœ¨ä¸€ä¸ªçª—å£ä¸­æ˜¾ç¤º2x2ç½‘æ ¼ï¼šRGBã€æ£€æµ‹ã€åˆ†å‰²å’Œå…³é”®ç‚¹é¢„æµ‹ç»“æœ
        """
        # è°ƒæ•´å›¾åƒå¤§å°
        display_size = (600, 450)
        
        # 1. RGBå›¾åƒ
        rgb_display = cv2.resize(color_image, display_size)
        
        # 2. æ£€æµ‹å¯è§†åŒ–ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼Œä½¿ç”¨RGBå›¾åƒï¼‰
        if detection_vis is not None:
            detection_display = cv2.resize(detection_vis, display_size)
        else:
            detection_display = rgb_display.copy()
        
        # 3. åˆ†å‰²å¯è§†åŒ–
        if mask is not None:
            # å°†æ©ç è½¬æ¢ä¸ºå½©è‰²å›¾åƒ
            mask_colored = np.zeros_like(color_image)
            mask_colored[mask > 0] = [0, 255, 0]  # ç»¿è‰²æ©ç 
            # å åŠ åˆ°åŸå›¾ä¸Š
            segmentation_vis = cv2.addWeighted(color_image, 0.7, mask_colored, 0.3, 0)
        else:
            segmentation_vis = color_image.copy()
        seg_display = cv2.resize(segmentation_vis, display_size)
        
        # 4. å…³é”®ç‚¹é¢„æµ‹å¯è§†åŒ–ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼Œä½¿ç”¨RGBå›¾åƒï¼‰
        if landmark_vis is not None:
            landmark_display = cv2.resize(landmark_vis, display_size)
        else:
            landmark_display = rgb_display.copy()
        
        # åˆ›å»º2x2ç½‘æ ¼
        # ç¬¬ä¸€è¡Œï¼šRGB | æ£€æµ‹
        top_row = np.hstack((rgb_display, detection_display))
        # ç¬¬äºŒè¡Œï¼šåˆ†å‰² | å…³é”®ç‚¹
        bottom_row = np.hstack((seg_display, landmark_display))
        # å‚ç›´æ‹¼æ¥
        combined = np.vstack((top_row, bottom_row))
        
        # æ·»åŠ æ ‡ç­¾
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.8
        font_thickness = 2
        text_color = (0, 255, 0)
        
        # ç¬¬ä¸€è¡Œæ ‡ç­¾
        cv2.putText(combined, "RGB", (15, 45), font, font_scale, text_color, font_thickness)
        cv2.putText(combined, "Detection", (615, 45), font, font_scale, text_color, font_thickness)
        
        # ç¬¬äºŒè¡Œæ ‡ç­¾
        cv2.putText(combined, "Segmentation", (15, 495), font, font_scale, text_color, font_thickness)
        cv2.putText(combined, "Landmarks", (615, 495), font, font_scale, text_color, font_thickness)
        
        # æ·»åŠ å¸§è®¡æ•°
        cv2.putText(combined, f"Frame: {self.frame_count}", (10, combined.shape[0] - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        cv2.imshow('Real-time Processing (2x2 Grid)', combined)

    
    def run_realtime(self, max_frames=None, show_preview=True):
        """
        è¿è¡Œå®æ—¶å¤„ç†
        
        Args:
            max_frames: æœ€å¤§å¸§æ•° (Noneè¡¨ç¤ºæ— é™)
            show_preview: æ˜¯å¦æ˜¾ç¤ºé¢„è§ˆçª—å£
        """
        print("å¼€å§‹å®æ—¶å¤„ç†...")
        print("æŒ‰ 'q' é”®åœæ­¢")
        if self.fish_tracker is not None:
            print("æŒ‰ 'r' é”®é‡ç½®å®¹å™¨")
            print("æŒ‰ 's' é”®æ˜¾ç¤ºçŠ¶æ€")
            print("æŒ‰ 'e' é”®å¯¼å‡ºæ•°æ®")
        if self.position_solver is not None:
            print("æŒ‰ 'p' é”®æ˜¾ç¤ºæ”¾ç½®çŠ¶æ€")
            print("æŒ‰ 'v' é”®æ˜¾ç¤ºæ”¾ç½®å¯è§†åŒ–")
        

        tcp_result = self.robot.get_tcp_position()
        if isinstance(tcp_result, tuple) and len(tcp_result) == 2:
            tcp_ok, original_tcp = tcp_result
        else:
            # å¦‚æœåªè¿”å›ä¸€ä¸ªå€¼ï¼Œå‡è®¾å®ƒæ˜¯ä½ç½®ä¿¡æ¯
            original_tcp = tcp_result
            tcp_ok = True

        try:
            while True:
                # æ•´ä¸ªå¾ªç¯è®¡æ—¶å¼€å§‹
                cycle_start = time.time()
                
                # æ•è·å¸§
                color_image, depth_image, success = self.capture_frames()
                if not success:
                    continue
                
                self.frame_count = self.frame_count + 1
               
                if self.frame_count < 10 :
                    print(f"è·³è¿‡å‰10å¸§ï¼Œç­‰å¾…ç›¸æœºç¨³å®š...")
                    continue
                # æ£€æµ‹ + åˆ†å‰² + è½ç›˜ï¼ˆæœ€è¿‘ç›®æ ‡é€‰æ‹©ï¼‰
                mask_vis, base_name = self.detect_and_segment_and_dump_all(color_image, depth_image)
                
                # ä¿å­˜RGBå’Œæ·±åº¦å›¾åƒï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹ï¼‰
                if self.debug and base_name is not None:
                    # ä¿å­˜RGBå›¾åƒ
                    rgb_path = os.path.join(self.rgb_dir, f"{base_name}.png")
                    cv2.imwrite(rgb_path, color_image)
                    
                    # ä¿å­˜æ·±åº¦å›¾åƒï¼ˆåŸå§‹16ä½ï¼‰
                    depth_path = os.path.join(self.depth_dir, f"{base_name}.png")
                    cv2.imwrite(depth_path, depth_image.astype(np.uint16))
                    
                    # ä¿å­˜å¯è§†åŒ–æ·±åº¦å›¾åƒï¼ˆ8ä½å½©è‰²ï¼‰
                    valid_depth = depth_image > 0
                    if valid_depth.any():
                        depth_min = depth_image[valid_depth].min()
                        depth_max = depth_image[valid_depth].max()
                        depth_normalized = np.zeros_like(depth_image, dtype=np.uint8)
                        if depth_max > depth_min:
                            depth_normalized[valid_depth] = ((depth_image[valid_depth] - depth_min) / (depth_max - depth_min) * 255).astype(np.uint8)
                        depth_colormap = cv2.applyColorMap(depth_normalized, cv2.COLORMAP_JET)
                        depth_vis_path = os.path.join(self.depth_dir, f"{base_name}_visualization.png")
                        cv2.imwrite(depth_vis_path, depth_colormap)

                # ç”Ÿæˆæ£€æµ‹å¯è§†åŒ–
                detection_vis = None
                if mask_vis is not None:
                    # é‡æ–°è¿è¡Œæ£€æµ‹ä»¥è·å–è¾¹ç•Œæ¡†å¯è§†åŒ–
                    if getattr(self, 'use_yolo', False):
                        boxes = self.detect_yolo(color_image, self.yolo_weights, conf=0.25, iou=0.45, imgsz=640)
                    else:
                        boxes = self._detect_boxes(color_image)
                    
                    if boxes:
                        detection_vis = color_image.copy()
                        # ç»˜åˆ¶æ‰€æœ‰æ£€æµ‹æ¡†
                        for i, box in enumerate(boxes):
                            if len(box) >= 4:
                                x1, y1, x2, y2 = box[:4]
                                color = (0, 255, 0) if i == 0 else (0, 255, 255)  # ç¬¬ä¸€ä¸ªæ¡†ç”¨ç»¿è‰²ï¼Œå…¶ä»–ç”¨é»„è‰²
                                thickness = 3 if i == 0 else 2
                                cv2.rectangle(detection_vis, (x1, y1), (x2, y2), color, thickness)
                                if len(box) >= 5:
                                    confidence = box[4]
                                    cv2.putText(detection_vis, f"{confidence:.2f}", (x1, y1-10), 
                                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
                
                # ç”Ÿæˆå…³é”®ç‚¹é¢„æµ‹å¯è§†åŒ–
                landmark_vis = None
                if (self.grasp_point_mode == "ai" and self.landmark_detector is not None and 
                    mask_vis is not None):
                    try:
                        # æ ¹æ®æ©ç è®¡ç®—å¤–æ¥çŸ©å½¢ï¼Œå¾—åˆ°é±¼çš„è£å‰ªåŒºåŸŸ
                        ys, xs = np.where(mask_vis > 0)
                        if ys.size > 0 and xs.size > 0:
                            x1, y1 = int(xs.min()), int(ys.min())
                            x2, y2 = int(xs.max())+1, int(ys.max())+1
                            crop_bgr = color_image[y1:y2, x1:x2]
                            crop_rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)
                            
                            # é¢„æµ‹å…³é”®ç‚¹
                            pred_landmarks, pred_visibility = self.landmark_detector.predict(crop_rgb)
                            
                            # å¯è§†åŒ–å…³é”®ç‚¹
                            landmark_vis_crop = self.landmark_detector.visualize_landmarks(crop_rgb, pred_landmarks, pred_visibility)
                            
                            # å°†è£å‰ªåŒºåŸŸçš„å¯è§†åŒ–ç»“æœæ”¾å›åŸå›¾
                            landmark_vis = color_image.copy()
                            landmark_vis_bgr = cv2.cvtColor(landmark_vis_crop, cv2.COLOR_RGB2BGR)
                            landmark_vis[y1:y2, x1:x2] = landmark_vis_bgr
                            
                            # ç»˜åˆ¶è¾¹ç•Œæ¡†
                            cv2.rectangle(landmark_vis, (x1, y1), (x2, y2), (255, 0, 0), 2)
                    except Exception as e:
                        print(f"[å¯è§†åŒ–] å…³é”®ç‚¹é¢„æµ‹å¯è§†åŒ–å¤±è´¥: {e}")
                
                # æ˜¾ç¤ºé¢„è§ˆçª—å£
                self.show_preview(color_image, depth_image, mask_vis, detection_vis, landmark_vis)
                
                # ç¡®ä¿çª—å£æ˜¾ç¤ºå¹¶å¤„ç†æŒ‰é”®
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("ç”¨æˆ·æŒ‰ 'q' é”®åœæ­¢")
                    break
                elif key == ord('r') and self.fish_tracker is not None:
                    print("ç”¨æˆ·æŒ‰ 'r' é”®é‡ç½®å®¹å™¨")
                    self.fish_tracker.reset_container(confirm=True)
                elif key == ord('s') and self.fish_tracker is not None:
                    print("ç”¨æˆ·æŒ‰ 's' é”®æ˜¾ç¤ºçŠ¶æ€")
                    self.fish_tracker.print_status()
                elif key == ord('e') and self.fish_tracker is not None:
                    print("ç”¨æˆ·æŒ‰ 'e' é”®å¯¼å‡ºæ•°æ®")
                    self.fish_tracker.export_data()
                elif key == ord('p') and self.position_solver is not None:
                    print("ç”¨æˆ·æŒ‰ 'p' é”®æ˜¾ç¤ºæ”¾ç½®çŠ¶æ€")
                    self.position_solver.print_placement_status()
                elif key == ord('v') and self.position_solver is not None:
                    print("ç”¨æˆ·æŒ‰ 'v' é”®æ˜¾ç¤ºæ”¾ç½®å¯è§†åŒ–")
                    print(self.position_solver.visualize_placements())

                # æ ¹æ®æ©ç ç”Ÿæˆ3Dç‚¹äº‘å¹¶ä¿å­˜ï¼ˆå¯é€‰åº”ç”¨æ‰‹çœ¼æ ‡å®šï¼‰
                points_gripper = None  # åˆå§‹åŒ–å˜é‡

                #import pdb; pdb.set_trace()
                if mask_vis is not None and base_name is not None:
                    # ç‚¹äº‘ç”Ÿæˆè®¡æ—¶
                    pointcloud_start = time.time()
                    mask_bool = (mask_vis > 0)
                    points, colors = self.generate_pointcloud(color_image, depth_image, mask_bool)
                    pointcloud_time = time.time() - pointcloud_start
                    self.timers['pointcloud_generation'].append(pointcloud_time)
                    print(f"â±ï¸  pointcloud_generation: {pointcloud_time:.3f}s")

                    #import pdb; pdb.set_trace()
                    if len(points) > 0:
                        # åº”ç”¨æ‰‹çœ¼å˜æ¢ï¼šç›¸æœºâ†’å¤¹çˆª
                        points_gripper = self.apply_hand_eye_transform(points)
                        
                        # ä¿å­˜ç‚¹äº‘ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹ï¼‰
                        if self.debug:
                            # ä¿å­˜ç›¸æœºåæ ‡ç³»ç‚¹äº‘
                            cam_ply = os.path.join(self.pointcloud_dir, f"{base_name}_cam_pointcloud.ply")
                            save_pointcloud_to_file(points, colors, cam_ply)
                            # ä¿å­˜å¤¹çˆªåæ ‡ç³»ç‚¹äº‘
                            grip_ply = os.path.join(self.pointcloud_dir, f"{base_name}_gripper_pointcloud.ply")
                            save_pointcloud_to_file(points_gripper, colors, grip_ply)
                
                # don't forget to transform the units, the point cloud is in meter, but robot
                # control would like to be in mm. 

                # è®¡ç®—ç‚¹äº‘è´¨å¿ƒå’Œæ³•å‘é‡ï¼ˆåœ¨å¤¹çˆªåæ ‡ç³»ä¸­ï¼‰
                if points_gripper is not None and len(points_gripper) > 0:
                    # æŠ“å–ç‚¹è®¡ç®—è®¡æ—¶
                    grasp_calc_start = time.time()
                    
                    # è·å–å½“å‰æœºå™¨äººTCPä½ç½®
                    tcp_result = self.robot.get_tcp_position()
                    if isinstance(tcp_result, tuple) and len(tcp_result) == 2:
                        tcp_ok, current_tcp = tcp_result
                    else:
                        # å¦‚æœåªè¿”å›ä¸€ä¸ªå€¼ï¼Œå‡è®¾å®ƒæ˜¯ä½ç½®ä¿¡æ¯
                        current_tcp = tcp_result
                        tcp_ok = True
                    print(f"å½“å‰TCPä½ç½®: {current_tcp}")
                    
                    # æ£€æŸ¥å®¹å™¨æ˜¯å¦å·²æ»¡
                    if self.fish_tracker is not None and self.fish_tracker.is_container_full():
                        print("ğŸ“¦ å®¹å™¨å·²æ»¡ï¼åœæ­¢æŠ“å–æ–°é±¼ã€‚")
                        print("æŒ‰ 'r' é”®é‡ç½®å®¹å™¨ï¼ŒæŒ‰ 'q' é”®é€€å‡º")
                        # æ˜¾ç¤ºçŠ¶æ€
                        self.fish_tracker.print_status()
                        continue
                    
                    # è®¡ç®—æŠ“å–ç‚¹ï¼ˆä¼˜å…ˆAIï¼‰
                    relative_move = None
                    angle_rad = 0
                    if self.grasp_point_mode == "ai" and self.landmark_detector is not None and mask_vis is not None:
                        try:
                            # æ ¹æ®æ©ç è®¡ç®—å¤–æ¥çŸ©å½¢ï¼Œå¾—åˆ°é±¼çš„è£å‰ªåŒºåŸŸ
                            ys, xs = np.where(mask_vis > 0)
                            if ys.size > 0 and xs.size > 0:
                                x1, y1 = int(xs.min()), int(ys.min())
                                x2, y2 = int(xs.max())+1, int(ys.max())+1
                                crop_bgr = color_image[y1:y2, x1:x2]
                                crop_rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)
                                # é¢„æµ‹å±€éƒ¨åæ ‡ç³»ä¸‹çš„ä¸¤ä¸ªå…³é”®ç‚¹ï¼ˆ0=body_center, 1=head_centerï¼‰
                                pred_landmarks, pred_visibility = self.landmark_detector.predict(crop_rgb)
                                if pred_landmarks.shape[0] >= 2:
                                    body_xy_local = pred_landmarks[0]
                                    head_xy_local = pred_landmarks[1]
                                else:
                                    # å…¼å®¹åªæœ‰ä¸€ä¸ªç‚¹çš„æƒ…å†µ
                                    body_xy_local = pred_landmarks[0]
                                    head_xy_local = pred_landmarks[0]

                                # æ˜ å°„å›å…¨å›¾åæ ‡ï¼ˆåƒç´ ï¼‰
                                u_body = float(x1 + body_xy_local[0])
                                v_body = float(y1 + body_xy_local[1])
                                u_head = float(x1 + head_xy_local[0])
                                v_head = float(y1 + head_xy_local[1])

                                # æ·±åº¦ï¼ˆç±³ï¼‰
                                z_body_m = float(depth_image[int(round(v_body)), int(round(u_body))]) / 1000.0 if 0 <= int(round(v_body)) < depth_image.shape[0] and 0 <= int(round(u_body)) < depth_image.shape[1] else 0.0
                                z_head_m = float(depth_image[int(round(v_head)), int(round(u_head))]) / 1000.0 if 0 <= int(round(v_head)) < depth_image.shape[0] and 0 <= int(round(u_head)) < depth_image.shape[1] else 0.0
                                if z_body_m <= 0:
                                    print(f"æ— æ•ˆèº«ä½“ä¸­å¿ƒæ·±åº¦: {z_body_m}")
                                    raise ValueError("æ— æ•ˆæ·±åº¦")

                                # åæŠ•å½±åˆ°ç›¸æœºåæ ‡ç³»ï¼ˆèº«ä½“ä¸­å¿ƒï¼‰
                                Xb = (u_body - self.cx) / self.fx * z_body_m
                                Yb = (v_body - self.cy) / self.fy * z_body_m
                                point_cam_body = np.array([[Xb, Yb, z_body_m]], dtype=np.float32)

                                # åæŠ•å½±åˆ°ç›¸æœºåæ ‡ç³»ï¼ˆå¤´éƒ¨ä¸­å¿ƒï¼Œå¦‚æ— æ•ˆæ·±åº¦åˆ™æ²¿ç”¨èº«ä½“æ·±åº¦ï¼‰
                                if z_head_m <= 0:
                                    z_head_m = z_body_m
                                Xh = (u_head - self.cx) / self.fx * z_head_m
                                Yh = (v_head - self.cy) / self.fy * z_head_m
                                point_cam_head = np.array([[Xh, Yh, z_head_m]], dtype=np.float32)

                                # ç›¸æœºâ†’å¤¹çˆª
                                point_grip_body = self.apply_hand_eye_transform(point_cam_body)[0]
                                point_grip_head = self.apply_hand_eye_transform(point_cam_head)[0]
                                body_grip_mm = point_grip_body * 1000.0
                                head_grip_mm = point_grip_head * 1000.0

                                # æ–¹å‘å‘é‡ï¼ˆå›¾åƒåæ ‡ç³»ï¼Œå•ä½å‘é‡ï¼‰
                                dir_img = np.array([u_head - u_body, v_head - v_body], dtype=np.float32)
                                norm_img = np.linalg.norm(dir_img) + 1e-6
                                dir_img_unit = (dir_img / norm_img).tolist()

                                # æ–¹å‘å‘é‡ï¼ˆå¤¹çˆªåæ ‡ç³»XYï¼Œå•ä½å‘é‡ï¼Œmmï¼‰
                                dir_grip_xy = np.array([head_grip_mm[0] - body_grip_mm[0], head_grip_mm[1] - body_grip_mm[1]], dtype=np.float32)
                                norm_grip = np.linalg.norm(dir_grip_xy) + 1e-6
                                dir_grip_xy_unit = (dir_grip_xy / norm_grip).tolist()


            
                                # å½“å‰æŠ“å–æŒ‰èº«ä½“ä¸­å¿ƒ
                                delta_tool_mm = [body_grip_mm[0], body_grip_mm[1], body_grip_mm[2]]
                                delta_base_xyz = self._tool_offset_to_base(delta_tool_mm, current_tcp[3:6])
                                z_offset = -delta_tool_mm[2] - 25

                                print(f"ğŸ¯ ä½¿ç”¨AIèº«ä½“ä¸­å¿ƒ: uv=({u_body:.1f},{v_body:.1f}) -> grip(mm)={body_grip_mm}")
                                print(f"ğŸ“ å¤´éƒ¨ä¸­å¿ƒ: uv=({u_head:.1f},{v_head:.1f}) -> grip(mm)={head_grip_mm}")
                                print(f"ğŸ§­ æ–¹å‘(åƒç´ xy,å•ä½å‘é‡) bodyâ†’head = {dir_img_unit}")
                                print(f"ğŸ§­ æ–¹å‘(å¤¹çˆªXY,å•ä½å‘é‡) bodyâ†’head = {dir_grip_xy_unit}")
                                # ä¸Xè½´(1,0,0)çš„å¤¹è§’ï¼ˆå¼§åº¦ï¼‰ï¼Œå¹¶è§„èŒƒåŒ–åˆ° [-pi/2, pi/2]
                                # è¿™æ ·æ— è®ºé±¼ä½“åŸå§‹æœå‘å¦‚ä½•ï¼Œéƒ½ä¼šè¢«æ˜ å°„åˆ°â€œæœå‘+XåŠå¹³é¢â€çš„ç­‰æ•ˆå§¿æ€ï¼Œä¾¿äºç»Ÿä¸€æ”¾ç½®æ–¹å‘
                                angle_rad = float(np.arctan2(dir_grip_xy_unit[1], dir_grip_xy_unit[0]))
                                if angle_rad > np.pi/2:
                                    angle_rad -= np.pi
                                elif angle_rad < -np.pi/2:
                                    angle_rad += np.pi
                                

                                relative_move = [delta_base_xyz[0], delta_base_xyz[1], z_offset, 0, 0, 0]

                                print(f"ğŸ§® æ–¹å‘ä¸Xè½´çš„å¤¹è§’(rad): {angle_rad:.4f}")
                            else:
                                print("[AI] æ©ç ä¸ºç©ºï¼Œå›é€€è´¨å¿ƒ")
                        except Exception as e:
                            print(f"[AI] é¢„æµ‹èº«ä½“ä¸­å¿ƒå¤±è´¥ï¼Œå›é€€è´¨å¿ƒ: {e}")

                    # è‹¥AIæœªç”Ÿæˆç§»åŠ¨ï¼Œä½¿ç”¨è´¨å¿ƒç‚¹äº‘æ–¹æ¡ˆ
                    if relative_move is None:
                        # è´¨å¿ƒç‚¹ï¼ˆå¤¹çˆªç³»ï¼‰
                        centroid = np.mean(points_gripper, axis=0)
                        print(f"å¤¹çˆªåæ ‡ç³»ç‚¹äº‘è´¨å¿ƒ: {centroid}")
                        center_gripper_mm = centroid * 1000
                        delta_tool_mm = [center_gripper_mm[0], center_gripper_mm[1], center_gripper_mm[2]]
                        delta_base_xyz = self._tool_offset_to_base(delta_tool_mm, current_tcp[3:6])
                        z_offset = -delta_tool_mm[2] -25
                        relative_move = [delta_base_xyz[0], delta_base_xyz[1], z_offset, 0, 0, 0]
                    
                    grasp_calc_time = time.time() - grasp_calc_start
                    self.timers['grasp_calculation'].append(grasp_calc_time)
                    print(f"â±ï¸  grasp_calculation: {grasp_calc_time:.3f}s")
                    
                    print("Step1 : å‡†å¤‡æŠ“å–")
                    print("ç›¸å¯¹ç§»åŠ¨é‡:", relative_move)
                    
                    # ä¼°ç®—é±¼é‡é‡ï¼ˆåœ¨æŠ“å–å‰ï¼‰
                    estimated_weight = 0.0
                    if self.fish_tracker is not None:
                        estimated_weight = self.estimate_fish_weight(points_gripper)
                        print(f"ğŸŸ ä¼°ç®—é±¼é‡é‡: {estimated_weight:.3f}kg")
                    
                    # æœºå™¨äººç§»åŠ¨è®¡æ—¶
                    robot_movement_start = time.time()
                    
                    # æ‰§è¡Œç›¸å¯¹ç§»åŠ¨
                    #import pdb; pdb.set_trace()
                    self.robot.set_digital_output(0, 0, 1)

                    ret = self.robot.linear_move(relative_move, 1, True, 500)
                    # if ret != 0:
                    #     print(f"æœºå™¨äººç§»åŠ¨å¤±è´¥: {ret}")
                    #     self.robot.linear_move(original_tcp, 0 , True, 400)
                    #     self.robot.set_digital_output(0, 0, 0)
                    #     continue

                    #  robot move up of 20 cm relatively 
                    #  ret = self.robot.linear_move([current_tcp[0], current_tcp[1], current_tcp[2] -100, current_tcp[3], current_tcp[4], current_tcp[5]], 0, True, 400)
                    # if ret != 0:
                    #     print(f"æœºå™¨äººç§»åŠ¨å¤±è´¥: {ret}")
                    #     self.robot.linear_move(original_tcp, 0 , True, 400)
                    #     self.robot.set_digital_output(0, 0, 0)
                    #     continue
                    self.robot.linear_move(original_tcp, 0 , True, 500)

                    print(f"æ—‹è½¬åŸºåº§{angle_rad:.4f}å¼§åº¦")
                    ret = self.robot.joint_move([-np.pi  * 0.6, 0, 0, 0, 0, angle_rad -  np.pi * 0.6], 1, True, 1)
                    ret = self.robot.linear_move([0, 0, -350, 0, 0, 0], 1 , True, 500)

                    self.robot.set_digital_output(0, 0, 0)
                    time.sleep(0.4)
                    ret = self.robot.linear_move([0, 0, 350, 0, 0, 0], 1 , True, 500)
                    ret = self.robot.joint_move([np.pi  * 0.6, 0, 0, 0, 0, 0], 1, True, 2)
                    ret = self.robot.joint_move([0, 0, 0, 0, 0,  np.pi * 0.6 - angle_rad], 1, True, 2)
                

                    #time.sleep(0.01)
                    #robot move back to the original position
                    self.robot.linear_move(original_tcp, 0 , True, 500)
                    
                    # è®°å½•é±¼åˆ°è·Ÿè¸ªå™¨
                    if self.fish_tracker is not None and estimated_weight > 0:
                        # é¢„æµ‹æœ€ç»ˆæ”¾ç½®ä½ç½®
                        predicted_final_pose = None
                        if self.position_solver is not None:
                            # ä¼°ç®—é±¼å°ºå¯¸ï¼ˆåŸºäºç‚¹äº‘è¾¹ç•Œæ¡†ï¼‰
                            if points_gripper is not None and len(points_gripper) > 0:
                                min_coords = np.min(points_gripper, axis=0)
                                max_coords = np.max(points_gripper, axis=0)
                                fish_size_mm = (max_coords - min_coords) * 1000.0  # è½¬æ¢ä¸ºmm
                                
                                # è·å–ä¸‹ä¸€ä¸ªé±¼ID
                                next_fish_id = self.fish_tracker.current_fish_id + 1
                                
                                # é¢„æµ‹æ”¾ç½®ä½ç½®
                                placement = self.position_solver.find_optimal_position(
                                    fish_id=next_fish_id,
                                    fish_size_mm=fish_size_mm
                                )
                                
                                if placement:
                                    # å°†å®¹å™¨åæ ‡è½¬æ¢ä¸ºæœºå™¨äººåæ ‡ç³»
                                    # å‡è®¾å®¹å™¨åœ¨æœºå™¨äººå·¥ä½œç©ºé—´ä¸­çš„ä½ç½®
                                    container_offset = [500.0, 0.0, 100.0]  # å®¹å™¨åœ¨æœºå™¨äººåæ ‡ç³»ä¸­çš„åç§»
                                    predicted_final_pose = [
                                        container_offset[0] + placement.x_mm,
                                        container_offset[1] + placement.y_mm,
                                        container_offset[2] + placement.z_mm,
                                        0.0, 0.0, 0.0  # æœ«ç«¯å§¿æ€
                                    ]
                                    print(f"ğŸ“ é¢„æµ‹æ”¾ç½®ä½ç½®: ({placement.x_mm:.1f}, {placement.y_mm:.1f}, {placement.z_mm:.1f})mm")
                                else:
                                    print("âš ï¸  æ— æ³•æ‰¾åˆ°åˆé€‚çš„æ”¾ç½®ä½ç½®")
                        
                        # æ·»åŠ é±¼è®°å½•
                        fish_id = self.fish_tracker.add_fish(
                            weight_kg=estimated_weight,
                            initial_pose=current_tcp,
                            grasp_angle=angle_rad
                        )
                        
                        # æ›´æ–°é±¼çŠ¶æ€ä¸ºå·²æ”¾ç½®
                        processing_time = time.time() - robot_movement_start
                        self.fish_tracker.update_fish_status(
                            fish_id=fish_id,
                            status="placed",
                            final_pose=predicted_final_pose,
                            processing_time=processing_time
                        )
                        
                        # æ˜¾ç¤ºå®¹å™¨çŠ¶æ€
                        self.fish_tracker.print_status()
                        
                        # æ˜¾ç¤ºä½ç½®æ±‚è§£å™¨çŠ¶æ€
                        if self.position_solver is not None:
                            self.position_solver.print_placement_status()
                   
                    robot_movement_time = time.time() - robot_movement_start
                    self.timers['robot_movement'].append(robot_movement_time)
                    print(f"â±ï¸  robot_movement: {robot_movement_time:.3f}s")
            
                else:
                    print("ç‚¹äº‘ä¸ºç©ºï¼Œè·³è¿‡æœºå™¨äººæ§åˆ¶")

                # æ•´ä¸ªå¾ªç¯è®¡æ—¶ç»“æŸ
                cycle_time = time.time() - cycle_start
                self.timers['total_cycle'].append(cycle_time)
                print(f"â±ï¸  total_cycle: {cycle_time:.3f}s")
                print("-" * 50)
 
                # self.robot.logout()
                # exit()
                

        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­å¤„ç†")
            self.robot.set_digital_output(0, 0, 0)
        except Exception as e:
            print(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        finally:
            self.cleanup()

    def cleanup(self):
        """
        æ¸…ç†èµ„æº
        """
        cv2.destroyAllWindows()
        if self.pipeline:
            self.pipeline.stop()
        
        # æ‰“å°æ—¶é—´ç»Ÿè®¡æ‘˜è¦
        self.print_timing_summary()
        
        print(f"å¤„ç†å®Œæˆï¼æ€»å…±å¤„ç†äº† {self.frame_count} å¸§")
        print(f"ç»“æœä¿å­˜åœ¨: {self.output_dir}")

def main():
    parser = argparse.ArgumentParser(description='å®æ—¶äººä½“åˆ†å‰²å’Œ3Dç‚¹äº‘ç”Ÿæˆ')
    parser.add_argument('--output_dir', type=str, default='realtime_output',
                      help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: realtime_output)')
    parser.add_argument('--device', type=str, default='cuda',
                      choices=['cpu', 'cuda'],
                      help='è¿è¡Œè®¾å¤‡ (é»˜è®¤: cuda)')
    parser.add_argument('--save_pointcloud', action='store_true',
                      help='ä¿å­˜3Dç‚¹äº‘')
    parser.add_argument('--max_frames', type=int, default=None,
                      help='æœ€å¤§å¤„ç†å¸§æ•° (é»˜è®¤: æ— é™)')
    parser.add_argument('--no_preview', action='store_true',
                      help='ä¸æ˜¾ç¤ºé¢„è§ˆçª—å£')
    parser.add_argument('--intrinsics_file', type=str, default=None,
                      help='ç›¸æœºå†…å‚JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--hand_eye_file', type=str, default=None,
                      help='æ‰‹çœ¼æ ‡å®š4x4é½æ¬¡çŸ©é˜µçš„.npyæ–‡ä»¶è·¯å¾„ï¼ˆç›¸æœºâ†’å¤¹çˆªï¼‰')
    parser.add_argument('--bbox_selection', type=str, default='highest_confidence',
                      choices=['smallest', 'largest', 'highest_confidence'],
                      help='è¾¹ç•Œæ¡†é€‰æ‹©ç­–ç•¥: smallest(é€‰æ‹©é¢ç§¯æœ€å°çš„é±¼) æˆ– largest(é€‰æ‹©é¢ç§¯æœ€å¤§çš„é±¼) (é»˜è®¤: largest)')
    parser.add_argument('--debug', action='store_true',
                      help='å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜æ‰€æœ‰ä¸­é—´æ–‡ä»¶ï¼ˆRGBã€æ·±åº¦ã€æ£€æµ‹ã€åˆ†å‰²ã€ç‚¹äº‘ï¼‰')
    parser.add_argument('--use_yolo', action='store_true',
                      help='ä½¿ç”¨YOLOä½œä¸ºæ£€æµ‹å™¨ï¼ˆæ›¿ä»£Grounding DINOï¼‰')
    parser.add_argument('--yolo_weights', type=str, default=None,
                      help='YOLOæƒé‡æ–‡ä»¶(.pt)è·¯å¾„ï¼ˆä¸ --use_yolo æ­é…ä½¿ç”¨ï¼‰')
    parser.add_argument('--grasp_point_mode', type=str, default='centroid',
                      choices=['centroid', 'ai'],
                      help='æŠ“å–ç‚¹æ¨¡å¼: centroid(ç‚¹äº‘è´¨å¿ƒ) æˆ– ai(ä½¿ç”¨AIèº«ä½“ä¸­å¿ƒ)')
    parser.add_argument('--landmark_model_path', type=str, default=None,
                      help='AIèº«ä½“ä¸­å¿ƒæ¨¡å‹è·¯å¾„ (.pth)ï¼Œå½“ grasp_point_mode=ai æ—¶å¿…éœ€')
    parser.add_argument('--enable_weight_tracking', action='store_true',
                      help='å¯ç”¨é±¼é‡é‡è·Ÿè¸ªåŠŸèƒ½')
    parser.add_argument('--max_container_weight', type=float, default=12.5,
                      help='å®¹å™¨æœ€å¤§é‡é‡ï¼ˆkgï¼‰ï¼Œé»˜è®¤12.5kg')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºå¤„ç†å™¨
        processor = RealtimeSegmentation3D(
            output_dir=args.output_dir,
            device=args.device,
            save_pointcloud=args.save_pointcloud,
            intrinsics_file=args.intrinsics_file,
            hand_eye_file=args.hand_eye_file,
            bbox_selection=args.bbox_selection,
            debug=args.debug,
            use_yolo=args.use_yolo,
            yolo_weights=args.yolo_weights,
            grasp_point_mode=args.grasp_point_mode,
            landmark_model_path=args.landmark_model_path,
            enable_weight_tracking=args.enable_weight_tracking,
            max_container_weight=args.max_container_weight
        )
        
        # è¿è¡Œå®æ—¶å¤„ç†
        processor.run_realtime(
            max_frames=args.max_frames,
            show_preview=not args.no_preview
        )
        
    except Exception as e:
        print(f"ç¨‹åºå‡ºé”™: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
