#!/usr/bin/env python3
"""
å®æ—¶äººä½“åˆ†å‰²å’Œ3Dç‚¹äº‘ç”Ÿæˆè„šæœ¬

æ•´åˆç°æœ‰åŠŸèƒ½ï¼š
1. RealSenseç›¸æœºè¯»å–RGBå’Œæ·±åº¦æ•°æ®
2. SAM + Grounding DINOè¿›è¡Œäººä½“åˆ†å‰²
3. å°†æ©ç è½¬æ¢ä¸º3Dç‚¹äº‘

ä½¿ç”¨æ–¹æ³•:
    python3 realtime_segmentation_3d.py --output_dir output_data --save_pointcloud

ä¾èµ–:
    - ç°æœ‰çš„seg.py, mask_to_3d.py, realsense_capture.py
    - pyrealsense2, opencv-python, numpy, torch
    - segment_anything, transformers, open3d



    add a training mechasim that grasp only on the fish with masked out other region 
"""

import argparse
import os
import sys
import time
import math
import numpy as np
import cv2
import torch
from datetime import datetime
from tqdm import tqdm
from PIL import Image
import json

# å¯¼å…¥ç°æœ‰æ¨¡å—çš„åŠŸèƒ½
from seg import init_models# process_image_cv2
from util import (
    estimate_body_angle_alpha1,
    draw_principal_axis,
    angle_between_2d_from_origin,
    apply_hand_eye_transform as util_apply_hand_eye_transform,
    tool_offset_to_base as util_tool_offset_to_base,
)
from mask_to_3d import mask_to_3d_pointcloud, save_pointcloud, load_camera_intrinsics
from realsense_capture import (
    setup_realsense,
    depth_to_pointcloud,
    save_pointcloud_to_file,
    capture_frames as rs_capture_frames,
    capture_frames_with_retry as rs_capture_frames_with_retry,
    validate_camera_connection as rs_validate_camera_connection,
    check_camera_health as rs_check_camera_health,
)
# Landmark detector for AI-based grasp point estimation
try:
    # ä¼˜å…ˆä»¥åŒ…å½¢å¼å¯¼å…¥
    from landmarks.fish_landmark_detector import FishLandmarkDetector
except Exception:
    # å…¼å®¹ç›´æ¥åœ¨å·¥ä½œåŒºæ ¹ç›®å½•è¿è¡Œ
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'landmarks'))
    try:
        from fish_landmark_detector import FishLandmarkDetector
    except Exception:
        FishLandmarkDetector = None

# è¿½åŠ è‡ªå®šä¹‰æ¨¡å—æœç´¢è·¯å¾„ï¼ˆæ‰‹çœ¼æ ‡å®šç›®å½•ï¼‰
_extra_paths = [
    "/home/ai/AI_perception/hand_eye_calibrate",
]
for _p in _extra_paths:
    try:
        if os.path.isdir(_p) and _p not in sys.path:
            sys.path.insert(0, _p)
    except Exception:
        pass

# å¯¼å…¥é±¼å®¹å™¨è·Ÿè¸ªå™¨
try:
    from FishContainerTracker import FishContainerTracker
except ImportError:
    print("[è­¦å‘Š] æ— æ³•å¯¼å…¥ FishContainerTrackerï¼Œå°†è·³è¿‡é‡é‡è·Ÿè¸ªåŠŸèƒ½")
    FishContainerTracker = None

# å¯¼å…¥ä½ç½®æ±‚è§£å™¨
try:
    from PositionSolver import PositionSolver, ContainerConfig
except ImportError:
    print("[è­¦å‘Š] æ— æ³•å¯¼å…¥ PositionSolverï¼Œå°†è·³è¿‡ä½ç½®é¢„æµ‹åŠŸèƒ½")
    PositionSolver = None
    ContainerConfig = None

class RealtimeSegmentation3D:
    def __init__(self, output_dir, device="cpu", save_pointcloud=True, intrinsics_file=None, hand_eye_file=None, bbox_selection="highest_confidence", debug=False, use_yolo=False, yolo_weights=None,
                 grasp_point_mode: str = "centroid", landmark_model_path: str = None, enable_weight_tracking: bool = True, max_container_weight: float = 12.5, det_gray: bool = False,
                 fish_paths_file: str = "configs/fish_paths.json", camera_calib_json: str = None, robot_config: str = "config/robot.json"):
        """
        åˆå§‹åŒ–å®æ—¶åˆ†å‰²å’Œ3Dç‚¹äº‘ç”Ÿæˆå™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            device: è¿è¡Œè®¾å¤‡ (cpu/cuda)
            save_pointcloud: æ˜¯å¦ä¿å­˜3Dç‚¹äº‘
            intrinsics_file: ç›¸æœºå†…å‚JSONæ–‡ä»¶è·¯å¾„
            hand_eye_file: æ‰‹çœ¼æ ‡å®š4x4é½æ¬¡çŸ©é˜µçš„.npyæ–‡ä»¶è·¯å¾„ï¼ˆç›¸æœºâ†’å¤¹çˆªï¼‰
            bbox_selection: è¾¹ç•Œæ¡†é€‰æ‹©ç­–ç•¥ ("smallest" æˆ– "largest" æˆ– "highest_confidence")
            debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆä¿å­˜æ‰€æœ‰ä¸­é—´æ–‡ä»¶ï¼‰
            use_yolo: æ˜¯å¦ä½¿ç”¨YOLOä½œä¸ºæ£€æµ‹å™¨
            yolo_weights: YOLOæƒé‡è·¯å¾„ï¼ˆ.ptï¼‰
            grasp_point_mode: æŠ“å–ç‚¹æ¨¡å¼ ("centroid" æˆ– "ai")
            landmark_model_path: AIå…³é”®ç‚¹æ¨¡å‹è·¯å¾„
            enable_weight_tracking: æ˜¯å¦å¯ç”¨é‡é‡è·Ÿè¸ª
            max_container_weight: å®¹å™¨æœ€å¤§é‡é‡ï¼ˆkgï¼‰
        """
        self.output_dir = output_dir
        self.device = device
        self.save_pointcloud = save_pointcloud
        self.bbox_selection = bbox_selection
        self.debug = debug
        self.use_yolo = use_yolo
        self.yolo_weights = yolo_weights
        # detection-only grayscale support (optional)
        self.det_gray = det_gray
        # æŠ“å–ç‚¹æ¨¡å¼ï¼šcentroid æˆ– ai
        self.grasp_point_mode = grasp_point_mode
        self.landmark_model_path = landmark_model_path
        # é‡é‡è·Ÿè¸ªç›¸å…³
        self.enable_weight_tracking = enable_weight_tracking
        self.max_container_weight = max_container_weight
        # configs
        self.fish_paths_file = fish_paths_file
        self.camera_calib_json = camera_calib_json
        self.robot_config = robot_config
        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹åˆ›å»ºï¼‰
        if self.debug:
            self.rgb_dir = os.path.join(output_dir, "rgb")
            self.depth_dir = os.path.join(output_dir, "depth")
            self.mask_dir = os.path.join(output_dir, "masks")
            self.pointcloud_dir = os.path.join(output_dir, "pointclouds")
            self.segmentation_dir = os.path.join(output_dir, "segmentation")
            self.detection_dir = os.path.join(output_dir, "detection")
            
            os.makedirs(self.rgb_dir, exist_ok=True)
            os.makedirs(self.depth_dir, exist_ok=True)
            os.makedirs(self.mask_dir, exist_ok=True)
            if save_pointcloud:
                os.makedirs(self.pointcloud_dir, exist_ok=True)
            os.makedirs(self.segmentation_dir, exist_ok=True)
            os.makedirs(self.detection_dir, exist_ok=True)
            print("è°ƒè¯•æ¨¡å¼å·²å¯ç”¨ï¼Œå°†ä¿å­˜æ‰€æœ‰ä¸­é—´æ–‡ä»¶")
        else:
            print("æ­£å¸¸æ¨¡å¼ï¼Œä¸ä¿å­˜ä¸­é—´æ–‡ä»¶")
        
        # åˆå§‹åŒ–æ¨¡å‹
        print("æ­£åœ¨åˆå§‹åŒ–AIæ¨¡å‹...")
        #self.sam_predictor, self.grounding_dino_model, self.processor = init_models(device)
        self.sam_predictor = init_models(device)

        if self.use_yolo:
            if not self.yolo_weights or not os.path.exists(self.yolo_weights):
                print(f"[è­¦å‘Š] å·²å¯ç”¨YOLOæ£€æµ‹ï¼Œä½†æœªæ‰¾åˆ°æƒé‡: {self.yolo_weights}ï¼Œå°†å›é€€Grounding DINO")
                self.use_yolo = False
        
            try:
                from ultralytics import YOLO
            except Exception as e:
                print("[é”™è¯¯] æœªæ‰¾åˆ° ultralyticsï¼Œè¯·å…ˆ: pip install ultralytics")
                print(e)
                return []

            # åŠ è½½æ¨¡å‹ï¼ˆæ¯æ¬¡è°ƒç”¨åŠ è½½é¿å…ä¸å…¶ä»–ä¾èµ–å†²çªï¼›è‹¥é¢‘ç¹è°ƒç”¨å¯å¤–éƒ¨ç¼“å­˜æ¨¡å‹å®ä¾‹ï¼‰
            self.yolo_model = YOLO(self.yolo_weights)
            print(f"å·²åŠ è½½YOLOæƒé‡: {self.yolo_weights}")
                
        # åˆå§‹åŒ–RealSenseç›¸æœº
        print("æ­£åœ¨åˆå§‹åŒ–RealSenseç›¸æœº...")
        self.pipeline, self.config = setup_realsense()
        if self.pipeline is None:
            raise RuntimeError("æ— æ³•å¯åŠ¨RealSenseç›¸æœº")
        
        # è·å–ç›¸æœºå†…å‚å’Œç•¸å˜ç³»æ•°
        self.fx, self.fy, self.cx, self.cy, self.dist, self.mtx = load_camera_intrinsics(intrinsics_file)
        print(f"ä½¿ç”¨ç›¸æœºå†…å‚: fx={self.fx}, fy={self.fy}, cx={self.cx}, cy={self.cy}")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç•¸å˜æ ¡æ­£
        if np.any(self.dist != 0):
            print("æ£€æµ‹åˆ°ç•¸å˜ç³»æ•°ï¼Œå°†è¿›è¡Œå®æ—¶å›¾åƒç•¸å˜æ ¡æ­£")
            print(f"ç•¸å˜ç³»æ•°: k1={self.dist[0]:.6f}, k2={self.dist[1]:.6f}, k3={self.dist[4]:.6f}")
        else:
            print("æœªæ£€æµ‹åˆ°ç•¸å˜ç³»æ•°ï¼Œè·³è¿‡ç•¸å˜æ ¡æ­£")
            self.mtx = None
            self.dist = None
        
        # åˆ›å»ºå¯¹é½å¯¹è±¡
        import pyrealsense2 as rs
        self.align = rs.align(rs.stream.color)
        
        # å¸§è®¡æ•°å™¨
        self.frame_count = 0
        self.start_time = time.time()
        
        # è®¡æ—¶å™¨
        self.timers = {
            'detection': [],
            'segmentation': [],
            'pointcloud_generation': [],
            'grasp_calculation': [],
            'robot_movement': [],
            'total_cycle': []
        }
        
        # åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µï¼ˆå¯é€‰ï¼‰
        self.hand_eye_transform = None  # 4x4 é½æ¬¡çŸ©é˜µï¼Œç›¸æœºåæ ‡â†’å¤¹çˆªåæ ‡
        if hand_eye_file is not None and os.path.exists(hand_eye_file):
            try:
                mat = np.load(hand_eye_file)
                if mat.shape == (4, 4):
                    self.hand_eye_transform = mat.astype(np.float32)
                    print("å·²åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µ (ç›¸æœºâ†’å¤¹çˆª):")
                    print(self.hand_eye_transform)
                else:
                    print(f"hand_eye_file æ ¼å¼ä¸æ­£ç¡®ï¼ŒæœŸæœ›(4,4)ï¼Œå®é™…{mat.shape}ï¼Œå¿½ç•¥ã€‚")
            except Exception as e:
                print(f"åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µå¤±è´¥: {e}")
        # è‹¥æœªåŠ è½½åˆ°ï¼Œå°è¯•ä» camera_calib_json åŠ è½½ï¼Œå¦åˆ™ä½¿ç”¨ç¡¬ç¼–ç çš„Rã€tï¼ˆç›¸æœºâ†’å¤¹çˆªï¼‰
        if self.hand_eye_transform is None:
            loaded_json = False
            try:
                if self.camera_calib_json and os.path.exists(self.camera_calib_json):
                    with open(self.camera_calib_json, 'r', encoding='utf-8') as f:
                        calib = json.load(f)
                    he = calib.get('hand_eye', {})
                    R = np.array(he.get('R', []), dtype=np.float32)
                    t = np.array(he.get('t', []), dtype=np.float32).reshape(3, 1) if he.get('t', None) is not None else None
                    if R.shape == (3, 3) and t is not None and t.shape == (3, 1):
                        self.hand_eye_transform = np.eye(4, dtype=np.float32)
                        self.hand_eye_transform[:3, :3] = R
                        self.hand_eye_transform[:3, 3:4] = t
                        loaded_json = True
                        print(f"å·²ä» JSON åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µ: {self.camera_calib_json}")
            except Exception as e:
                print(f"è¯»å– camera_calib_json å¤±è´¥: {e}")
            if not loaded_json:
                R_default = np.array([
                    [-0.99791369, -0.06094636, -0.02130291],
                    [ 0.06027516, -0.99770511,  0.03084494],
                    [-0.02313391,  0.02949655,  0.99929714]
                ], dtype=np.float32)
                t_default = np.array([[0.04], [0.113], [-0.22081495]], dtype=np.float32)
                self.hand_eye_transform = np.eye(4, dtype=np.float32)
                self.hand_eye_transform[:3, :3] = R_default
                self.hand_eye_transform[:3, 3:4] = t_default
                print("ä½¿ç”¨ç¡¬ç¼–ç æ‰‹çœ¼æ ‡å®šçŸ©é˜µ (ç›¸æœºâ†’å¤¹çˆª):")
                print(self.hand_eye_transform)
        
        print("åˆå§‹åŒ–å®Œæˆï¼")

        # åˆå§‹åŒ–AIå…³é”®ç‚¹æ£€æµ‹å™¨ï¼ˆå¯é€‰ï¼‰
        self.landmark_detector = None
        if self.grasp_point_mode == "ai":
            if FishLandmarkDetector is None:
                print("[è­¦å‘Š] æ— æ³•å¯¼å…¥ FishLandmarkDetectorï¼Œå°†å›é€€ä¸ºè´¨å¿ƒæ¨¡å¼")
                self.grasp_point_mode = "centroid"
            else:
                try:
                    if landmark_model_path is None:
                        print("[è­¦å‘Š] æœªæä¾› landmark_model_pathï¼Œå°†å›é€€ä¸ºè´¨å¿ƒæ¨¡å¼")
                        self.grasp_point_mode = "centroid"
                    else:
                        self.landmark_detector = FishLandmarkDetector(model_path=landmark_model_path, device=('cuda' if self.device=='cuda' and torch.cuda.is_available() else 'cpu'))
                        print(f"å·²åŠ è½½AIå…³é”®ç‚¹æ¨¡å‹: {landmark_model_path}")
                except Exception as e:
                    print(f"[è­¦å‘Š] å…³é”®ç‚¹æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†å›é€€ä¸ºè´¨å¿ƒæ¨¡å¼")
                    self.grasp_point_mode = "centroid"

        # åˆå§‹åŒ–é±¼å®¹å™¨è·Ÿè¸ªå™¨ï¼ˆå¯é€‰ï¼‰
        self.fish_tracker = None
        if self.enable_weight_tracking and FishContainerTracker is not None:
            try:
                self.fish_tracker = FishContainerTracker(
                    max_weight_kg=self.max_container_weight,
                    data_file=os.path.join(self.output_dir, "fish_tracking_data.json")
                )
                print(f"å·²å¯ç”¨é±¼å®¹å™¨è·Ÿè¸ªå™¨ï¼Œæœ€å¤§å®¹é‡: {self.max_container_weight}kg")
            except Exception as e:
                print(f"[è­¦å‘Š] é±¼å®¹å™¨è·Ÿè¸ªå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.fish_tracker = None
        else:
            print("é±¼å®¹å™¨è·Ÿè¸ªå™¨æœªå¯ç”¨")

        # åˆå§‹åŒ–ä½ç½®æ±‚è§£å™¨ï¼ˆå¯é€‰ï¼‰
        self.position_solver = None
        if self.enable_weight_tracking and PositionSolver is not None:
            try:
                # é…ç½®å®¹å™¨å‚æ•°ï¼ˆæ ¹æ®å®é™…å®¹å™¨å°ºå¯¸è°ƒæ•´ï¼‰
                container_config = ContainerConfig(
                    width_mm=300.0,      # å®¹å™¨å®½åº¦
                    height_mm=200.0,     # å®¹å™¨é«˜åº¦
                    depth_mm=150.0,      # å®¹å™¨æ·±åº¦
                    grid_spacing_mm=30.0, # ç½‘æ ¼é—´è·
                    margin_mm=20.0,      # è¾¹è·
                    base_height_mm=0.0   # åŸºç¡€é«˜åº¦
                )
                self.position_solver = PositionSolver(container_config)
                print("å·²å¯ç”¨ä½ç½®æ±‚è§£å™¨")
            except Exception as e:
                print(f"[è­¦å‘Š] ä½ç½®æ±‚è§£å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.position_solver = None
        else:
            print("ä½ç½®æ±‚è§£å™¨æœªå¯ç”¨")


        import jkrc 
        self.robot = jkrc.RC("192.168.80.116")
        self.robot.login()   
        self.robot.set_digital_output(0, 0, 0)

    
    def time_step(self, step_name):
        """è®¡æ—¶å™¨è£…é¥°å™¨ï¼Œç”¨äºæµ‹é‡å„ä¸ªæ­¥éª¤çš„æ—¶é—´"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                start_time = time.time()
                result = func(*args, **kwargs)
                end_time = time.time()
                elapsed = end_time - start_time
                self.timers[step_name].append(elapsed)
                print(f"â±ï¸  {step_name}: {elapsed:.3f}s")
                return result
            return wrapper
        return decorator
    
    def print_timing_summary(self):
        """æ‰“å°æ—¶é—´ç»Ÿè®¡æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æ—¶é—´ç»Ÿè®¡æ‘˜è¦")
        print("="*60)
        
        for step_name, times in self.timers.items():
            if times:
                avg_time = np.mean(times)
                min_time = np.min(times)
                max_time = np.max(times)
                total_time = np.sum(times)
                print(f"{step_name:20s}: å¹³å‡={avg_time:.3f}s, æœ€å°={min_time:.3f}s, æœ€å¤§={max_time:.3f}s, æ€»è®¡={total_time:.3f}s")
            else:
                print(f"{step_name:20s}: æ— æ•°æ®")
        
        print("="*60)
    
    def capture_frames(self, timeout_ms=10000):
        """å§”æ‰˜åˆ° realsense_capture.capture_frames"""
        return rs_capture_frames(self.pipeline, self.align, timeout_ms)
    
    def capture_frames_with_retry(self, max_retries=3, timeout_ms=10000):
        return rs_capture_frames_with_retry(self.pipeline, self.align, max_retries, timeout_ms)
    
    def validate_camera_connection(self, timeout_ms=5000):
        return rs_validate_camera_connection(self.pipeline, self.align, timeout_ms)
    
    def check_camera_health(self):
        return rs_check_camera_health(self.pipeline)
    
    # write seperate function to do detection and segmentation togther but segmentation is done for all the objects, 
    # and then we can select the  grasp object based on the distance of camera,.
    def detect_and_segment_and_dump_all(self, color_image, depth_image):
        """
        æ£€æµ‹æ‰€æœ‰å€™é€‰é±¼ï¼Œåˆ†åˆ«åˆ†å‰²å¹¶è®¡ç®—ç‚¹äº‘è´¨å¿ƒæ·±åº¦ï¼Œé€‰å–ç¦»ç›¸æœºæœ€è¿‘çš„ä¸€ä¸ªã€‚

        Args:
            color_image: BGR å›¾åƒ (H,W,3)
            depth_image: æ·±åº¦å›¾ (æ¯«ç±³, uint16)

        Returns:
            mask_np: é€‰ä¸­é±¼çš„å•é€šé“uint8æ©ç ï¼ˆ0/255ï¼‰ï¼Œè‹¥å¤±è´¥è¿”å›None
            base_name: æ–‡ä»¶åŸºåå­—ç¬¦ä¸²ï¼Œè‹¥å¤±è´¥è¿”å›None
        """
        # 1) æ£€æµ‹æ‰€æœ‰å€™é€‰æ¡†
        detection_start = time.time()
        #if getattr(self, 'use_yolo', False):
        # YOLO è·¯å¾„ï¼šdetect_yolo å·²è¿”å›æ‰€æœ‰æ»¡è¶³æ¡ä»¶çš„æ¡† (x1,y1,x2,y2,conf)
        boxes = self.detect_yolo(color_image, self.yolo_weights, conf=0.25, iou=0.45, imgsz=640, min_area=2500)
       
        detection_time = time.time() - detection_start
        self.timers['detection'].append(detection_time)
        print(f"â±ï¸  detection(all): {detection_time:.3f}s  å€™é€‰æ•°: {len(boxes) if boxes else 0}")

        if not boxes:
            print("æœªæ£€æµ‹åˆ°ç›®æ ‡ï¼Œè·³è¿‡åˆ†å‰²ã€‚")
            return None, None

        # 2) é€ä¸ªå€™é€‰æ¡†è¿›è¡Œåˆ†å‰²ï¼Œå¹¶è®¡ç®—ç‚¹äº‘è´¨å¿ƒæ·±åº¦
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
        base_name = f"frame_{self.frame_count:06d}_{timestamp}"

        image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)
        self.sam_predictor.set_image(image_rgb)

        best_idx = -1
        best_depth_m = float('inf')
        best_mask = None

        segmentation_start = time.time()
        for i, b in enumerate(boxes):
            x1, y1, x2, y2 = b[:4]
            boxes_tensor = torch.tensor([[x1, y1, x2, y2]], device=self.device)
            transformed_boxes = self.sam_predictor.transform.apply_boxes_torch(boxes_tensor, image_rgb.shape[:2])

            try:
                masks, scores, logits = self.sam_predictor.predict_torch(
                    point_coords=None,
                    point_labels=None,
                    boxes=transformed_boxes,
                    multimask_output=False
                )
            except Exception as e:
                print(f"[åˆ†å‰²] å€™é€‰æ¡† {i} é¢„æµ‹å¤±è´¥: {e}")
                continue

            if masks.shape[0] == 0 or masks.shape[1] == 0:
                print(f"[åˆ†å‰²] å€™é€‰æ¡† {i} æœªç”Ÿæˆæ©ç ")
                continue

            m_bool = masks[0][0].detach().cpu().numpy().astype(np.uint8)
            mask_np = m_bool * 255
            # é™åˆ¶åœ¨ bbox å†…
            restricted_mask = np.zeros_like(mask_np, dtype=np.uint8)
            restricted_mask[y1:y2, x1:x2] = mask_np[y1:y2, x1:x2]
            mask_np = restricted_mask

            # è®¡ç®—ç‚¹äº‘å¹¶æ±‚è´¨å¿ƒæ·±åº¦ï¼ˆç›¸æœºåæ ‡ç³»ï¼Œå•ä½ç±³ï¼‰
            mask_bool = (mask_np > 0)
            if not np.any(mask_bool):
                print(f"[åˆ†å‰²] å€™é€‰æ¡† {i} æ©ç ä¸ºç©ºï¼Œè·³è¿‡")
                continue

            points, colors = self.generate_pointcloud(color_image, depth_image, mask_bool)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          
            if points is None or len(points) == 0:
                print(f"[ç‚¹äº‘] å€™é€‰æ¡† {i} ç‚¹äº‘ä¸ºç©ºï¼Œè·³è¿‡")
                continue

            centroid = np.mean(points, axis=0)  # (x,y,z) in meters (cam frame)
            depth_m = float(centroid[2])
            print(f"å€™é€‰æ¡† {i} è´¨å¿ƒæ·±åº¦: {depth_m:.4f} m  bbox=({x1},{y1},{x2},{y2})")

            # è®°å½•è°ƒè¯•è¾“å‡º
            if self.debug:
                cv2.imwrite(os.path.join(self.segmentation_dir, f"{base_name}_cand{i}_mask.png"), mask_np)
                det_vis = color_image.copy()
                cv2.rectangle(det_vis, (x1, y1), (x2, y2), (0, 255, 255), 2)
                cv2.putText(det_vis, f"cand {i}", (x1, max(0, y1-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 2)
                cv2.imwrite(os.path.join(self.detection_dir, f"{base_name}_cand{i}_box.png"), det_vis)

            if depth_m < best_depth_m:
                best_depth_m = depth_m
                best_mask = mask_np
                best_idx = i

        segmentation_time = time.time() - segmentation_start
        self.timers['segmentation'].append(segmentation_time)
        print(f"â±ï¸  segmentation(all): {segmentation_time:.3f}s")

        if best_idx == -1 or best_mask is None:
            print("åˆ†å‰²/ç‚¹äº‘å‡å¤±è´¥ï¼Œæœªé€‰å‡ºå€™é€‰")
            return None, None

        print(f"é€‰æ‹©æœ€è¿‘å€™é€‰: idx={best_idx}, æ·±åº¦={best_depth_m:.4f} m")

        if best_depth_m > 0.8:
            print(f"æ·±åº¦è¶…è¿‡0.8mï¼Œè·³è¿‡")
            return None, None

        # å¯è§†åŒ–æœ€ç»ˆé€‰æ‹©
        if self.debug:
            colored = np.zeros_like(color_image)
            colored[best_mask > 0] = [0, 255, 0]
            vis = cv2.addWeighted(color_image, 1.0, colored, 0.4, 0)
            vis_path = os.path.join(self.segmentation_dir, f"{base_name}_closest_overlay.png")
            cv2.imwrite(vis_path, vis)

        return best_mask, base_name


    def detect_yolo(self, color_image, yolo_weights_path, conf=0.25, iou=0.45, imgsz=640, min_area=1000):
        """
        ä½¿ç”¨Ultralytics YOLOè¿›è¡Œé±¼çš„æ£€æµ‹ï¼Œè¿”å›æ‰€æœ‰æ£€æµ‹åˆ°çš„bboxã€‚
        
        Args:
            color_image: OpenCV BGRå›¾åƒ (H,W,3)
            yolo_weights_path: è®­ç»ƒå¥½çš„YOLOæƒé‡ .pt è·¯å¾„
            conf: ç½®ä¿¡åº¦é˜ˆå€¼
            iou: NMS IOU é˜ˆå€¼
            imgsz: æ¨ç†è¾“å…¥å°ºå¯¸
            min_area: è¿‡æ»¤æœ€å°é¢ç§¯ï¼ˆåƒç´ ï¼‰
        
        Returns:
            boxes: List[Tuple[x1, y1, x2, y2, confidence]] æ‰€æœ‰æ»¡è¶³æ¡ä»¶çš„bboxï¼›è‹¥æ— åˆ™è¿”å›ç©ºåˆ—è¡¨
        """
     

        # YOLOæ”¯æŒç›´æ¥ä¼ å…¥numpyå›¾åƒï¼›ç¡®ä¿ä¸ºRGB
        #image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)
        #import pdb; pdb.set_trace()
        try:
            # grayscale only for detection if enabled
            det_input = color_image
            if getattr(self, 'det_gray', False):
                gray = cv2.cvtColor(color_image, cv2.COLOR_BGR2GRAY)
                det_input = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
            results = self.yolo_model.predict(
                source=[det_input],
                imgsz=imgsz,
                conf=conf,
                iou=iou,
                verbose=False,
                save=False
            )
        except Exception as e:
            print(f"[é”™è¯¯] YOLO æ¨ç†å¤±è´¥: {e}")
            return []

        if not results:
            return []

        res = results[0]
        boxes_np = None
        confidences = None
        try:
            # xyxy (N,4) å’Œ conf (N,)
            boxes_np = res.boxes.xyxy.cpu().numpy() if hasattr(res, 'boxes') and res.boxes is not None else None
            confidences = res.boxes.conf.cpu().numpy() if hasattr(res, 'boxes') and res.boxes is not None else None
        except Exception:
            boxes_np = None
            confidences = None

        if boxes_np is None or len(boxes_np) == 0:
            return []

        # è¿‡æ»¤é¢ç§¯ã€è£å‰ªåˆ°å›¾åƒèŒƒå›´ï¼ŒåŒæ—¶ä¿å­˜ç½®ä¿¡åº¦ä¿¡æ¯
        H, W = color_image.shape[0], color_image.shape[1]
        valid_boxes = []
        for i, xyxy in enumerate(boxes_np):
            x1, y1, x2, y2 = [int(round(v)) for v in xyxy[:4].tolist()]
            x1 = max(0, min(x1, W - 1))
            y1 = max(0, min(y1, H - 1))
            x2 = max(0, min(x2, W - 1))
            y2 = max(0, min(y2, H - 1))
            area = max(0, x2 - x1) * max(0, y2 - y1)
            if area > min_area:
                confidence = confidences[i] if confidences is not None else 0.0
                valid_boxes.append(((x1, y1, x2, y2), area, confidence))

        boxes = []
        if valid_boxes:
            # è¿”å›æ‰€æœ‰æ£€æµ‹åˆ°çš„bboxï¼ŒåŒ…å«ç½®ä¿¡åº¦ä¿¡æ¯
            for bbox_info in valid_boxes:
                bbox_coords, area, confidence = bbox_info
                # è¿”å›æ ¼å¼: (x1, y1, x2, y2, confidence)
                boxes.append((*bbox_coords, confidence))
            
            print(f"[YOLO] æ£€æµ‹åˆ° {len(valid_boxes)} ä¸ªå€™é€‰æ¡†ï¼Œå…¨éƒ¨è¿”å›ç”¨äºå¤„ç†")
            for i, (bbox_coords, area, confidence) in enumerate(valid_boxes):
                print(f"[YOLO] æ¡† {i+1}: {bbox_coords}, ç½®ä¿¡åº¦: {confidence:.3f}, é¢ç§¯: {area} åƒç´ ")
        else:
            print("[YOLO] æ²¡æœ‰æ»¡è¶³é¢ç§¯è¦æ±‚çš„è¾¹ç•Œæ¡†")

        return boxes
    

    def generate_pointcloud(self, color_image, depth_image, mask):
        """
        ä»æ©ç ç”Ÿæˆ3Dç‚¹äº‘
        
        Args:
            color_image: RGBå›¾åƒ
            depth_image: æ·±åº¦å›¾åƒ (æ¯«ç±³)
            mask: åˆ†å‰²æ©ç 
            
        Returns:
            points: 3Dç‚¹åæ ‡
            colors: RGBé¢œè‰²
        """
        try:
            # è½¬æ¢æ·±åº¦å›¾åƒå•ä½ä¸ºç±³
            depth_image_meters = depth_image.astype(np.float32) / 1000.0
            
            # è½¬æ¢ä¸ºRGBæ ¼å¼
            color_image_rgb = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)
            
            # ä½¿ç”¨mask_to_3d_pointcloudå‡½æ•°ï¼ˆæ”¯æŒç•¸å˜æ ¡æ­£ï¼‰
            points, colors = mask_to_3d_pointcloud(
                color_image_rgb, 
                depth_image_meters, 
                mask, 
                self.fx, self.fy, self.cx, self.cy,
                self.mtx, self.dist
            )
            
            return points, colors
            
        except Exception as e:
            print(f"ç”Ÿæˆç‚¹äº‘æ—¶å‡ºé”™: {e}")
            return np.array([]), np.array([])

    def apply_hand_eye_transform(self, points):
        """ä½¿ç”¨ util.apply_hand_eye_transform åº”ç”¨æ‰‹çœ¼æ ‡å®šå˜æ¢"""
        return util_apply_hand_eye_transform(points, self.hand_eye_transform)

    def _rpy_to_rotation_matrix(self, rx, ry, rz):
        # ä¿ç•™å…¼å®¹æ–¹æ³•ä½†å§”æ‰˜åˆ° utilï¼ˆå¦‚åç»­ç›´æ¥è°ƒç”¨ utilï¼Œå¯åˆ é™¤æ­¤æ–¹æ³•ï¼‰
        from util import rpy_to_rotation_matrix
        return rpy_to_rotation_matrix(rx, ry, rz)

    def _tool_offset_to_base(self, delta_tool_xyz_mm, tcp_rpy):
        # ä¿ç•™å…¼å®¹æ–¹æ³•ä½†å§”æ‰˜åˆ° util
        dx, dy, dz = util_tool_offset_to_base(delta_tool_xyz_mm, tcp_rpy)
        return [dx, dy, dz]

    def calculate_pointcloud_bbox(self, points):
        from point_cloud_utils import calculate_pointcloud_bbox
        return calculate_pointcloud_bbox(points)

    def calculate_surface_normal(self, points, method='pca'):
        from point_cloud_utils import calculate_surface_normal
        return calculate_surface_normal(points, method)

    def _simple_plane_fitting(self, points, centroid):
        from point_cloud_utils import _simple_plane_fitting
        return _simple_plane_fitting(points, centroid)

    def _nearest_neighbors_normal(self, points, centroid, k=20):
        from point_cloud_utils import _nearest_neighbors_normal
        return _nearest_neighbors_normal(points, centroid, k)

    def normal_to_rpy(self, normal_vector, current_rpy=None):
        from pose import normal_to_rpy as pose_normal_to_rpy
        return pose_normal_to_rpy(normal_vector, current_rpy)

    def _smooth_rpy_transition(self, current_rpy, target_rpy, max_change=0.1):
        from pose import smooth_rpy_transition
        return smooth_rpy_transition(current_rpy, target_rpy, max_change)

    def estimate_fish_weight(self, points_gripper, volume_factor: float = 1.0) -> float:
        from util import estimate_fish_weight
        return estimate_fish_weight(points_gripper, volume_factor)

    def calculate_grasp_pose_with_normal(self, points_gripper, current_tcp):
        from pose import calculate_grasp_pose_with_normal as pose_calculate_grasp_pose_with_normal
        return pose_calculate_grasp_pose_with_normal(points_gripper, current_tcp)
    
    
    def show_preview(self, color_image, depth_image, mask, detection_vis=None, landmark_vis=None):
        """
        åœ¨ä¸€ä¸ªçª—å£ä¸­æ˜¾ç¤º2x2ç½‘æ ¼ï¼šRGBã€æ£€æµ‹ã€åˆ†å‰²å’Œå…³é”®ç‚¹é¢„æµ‹ç»“æœ
        """
 
        # è°ƒæ•´å›¾åƒå¤§å°
        display_size = (640, 480)
        
        # 1. RGBå›¾åƒ
        rgb_display = cv2.resize(color_image, display_size)
        
        # 2. æ£€æµ‹å¯è§†åŒ–ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼Œä½¿ç”¨RGBå›¾åƒï¼‰
        if detection_vis is not None:
            detection_display = cv2.resize(detection_vis, display_size)
        else:
            detection_display = rgb_display.copy()
        
        # 3. åˆ†å‰²å¯è§†åŒ–
        if mask is not None:
            # å°†æ©ç è½¬æ¢ä¸ºå½©è‰²å›¾åƒ
            mask_colored = np.zeros_like(color_image)
            mask_colored[mask > 0] = [0, 255, 0]  # ç»¿è‰²æ©ç 
            # å åŠ åˆ°åŸå›¾ä¸Š
            segmentation_vis = cv2.addWeighted(color_image, 0.7, mask_colored, 0.3, 0)
        else:
            segmentation_vis = color_image.copy()
        seg_display = cv2.resize(segmentation_vis, display_size)
        
        # 4. å…³é”®ç‚¹é¢„æµ‹å¯è§†åŒ–ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼Œä½¿ç”¨RGBå›¾åƒï¼‰
        if landmark_vis is not None:
            landmark_display = cv2.resize(landmark_vis, display_size)
        else:
            landmark_display = rgb_display.copy()
        
        # åˆ›å»º2x2ç½‘æ ¼
        # ç¬¬ä¸€è¡Œï¼šRGB | æ£€æµ‹
        top_row = np.hstack((rgb_display, detection_display))
        # ç¬¬äºŒè¡Œï¼šåˆ†å‰² | å…³é”®ç‚¹
        bottom_row = np.hstack((seg_display, landmark_display))
        # å‚ç›´æ‹¼æ¥
        combined = np.vstack((top_row, bottom_row))
        
        # æ·»åŠ æ ‡ç­¾
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.8
        font_thickness = 2
        text_color = (0, 255, 0)
        
        # ç¬¬ä¸€è¡Œæ ‡ç­¾
        cv2.putText(combined, "RGB", (15, 45), font, font_scale, text_color, font_thickness)
        cv2.putText(combined, "Detection", (615, 45), font, font_scale, text_color, font_thickness)
        
        # ç¬¬äºŒè¡Œæ ‡ç­¾
        cv2.putText(combined, "Segmentation", (15, 495), font, font_scale, text_color, font_thickness)
        cv2.putText(combined, "Landmarks", (615, 495), font, font_scale, text_color, font_thickness)
        
        # æ·»åŠ å¸§è®¡æ•°
        cv2.putText(combined, f"Frame: {self.frame_count}", (10, combined.shape[0] - 10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        cv2.imshow('Real-time Processing (2x2 Grid)', combined)

    
    def run_realtime(self, max_frames=None, show_preview=True):
        """
        è¿è¡Œå®æ—¶å¤„ç†
        
        Args:
            max_frames: æœ€å¤§å¸§æ•° (Noneè¡¨ç¤ºæ— é™)
            show_preview: æ˜¯å¦æ˜¾ç¤ºé¢„è§ˆçª—å£
        """
        print("å¼€å§‹å®æ—¶å¤„ç†...")
        print("æŒ‰ 'q' é”®åœæ­¢")
        if self.fish_tracker is not None:
            print("æŒ‰ 'r' é”®é‡ç½®å®¹å™¨")
            print("æŒ‰ 's' é”®æ˜¾ç¤ºçŠ¶æ€")
            print("æŒ‰ 'e' é”®å¯¼å‡ºæ•°æ®")
        if self.position_solver is not None:
            print("æŒ‰ 'p' é”®æ˜¾ç¤ºæ”¾ç½®çŠ¶æ€")
            print("æŒ‰ 'v' é”®æ˜¾ç¤ºæ”¾ç½®å¯è§†åŒ–")
        
        # éªŒè¯ç›¸æœºè¿æ¥
        if not self.validate_camera_connection():
            print("âŒ ç›¸æœºè¿æ¥éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸æœºè¿æ¥åé‡è¯•")
            return


        # Move to initial pose from robot config if provided
        try:
            if self.robot_config and os.path.exists(self.robot_config):
                with open(self.robot_config, 'r', encoding='utf-8') as f:
                    robot_conf = json.load(f)
                init = robot_conf.get('initial_pose', {})
                pose_type = init.get('type', 'joint')
                mode = int(init.get('mode', 0))
                blocking = bool(init.get('blocking', True))
                speed = float(init.get('speed', 1))
                if pose_type == 'joint':
                    vals_deg = init.get('values_deg')
                    vals_rad = init.get('values_rad')
                    if vals_rad is not None:
                        joints = [float(x) for x in vals_rad]
                    elif vals_deg is not None:
                        joints = [float(x) * np.pi / 180.0 for x in vals_deg]
                    else:
                        joints = None
                    if joints is not None and len(joints) == 6:
                        ret = self.robot.joint_move(joints, mode, blocking, speed)
                        time.sleep(0.2)
                elif pose_type == 'tcp':
                    tcp = init.get('values_mmrad')  # [x,y,z,rx,ry,rz]
                    if tcp is not None and len(tcp) == 6:
                        ret = self.robot.linear_move(tcp, mode, blocking, speed)
                        time.sleep(0.2)
        except Exception as e:
            print(f"åŠ è½½/ç§»åŠ¨åˆå§‹ä½å§¿å¤±è´¥: {e}")
            
        tcp_result = self.robot.get_tcp_position()
        if isinstance(tcp_result, tuple) and len(tcp_result) == 2:
            tcp_ok, original_tcp = tcp_result
        else:
            # å¦‚æœåªè¿”å›ä¸€ä¸ªå€¼ï¼Œå‡è®¾å®ƒæ˜¯ä½ç½®ä¿¡æ¯
            original_tcp = tcp_result
            tcp_ok = True

        fish_count = 0 # count the number of fish in the container
        rows = -1
        cols = -1
        # Load fish paths path from fish_grid_params.json
        fish_paths_path = getattr(self, 'fish_paths_file', None)
        if fish_paths_path is None:
            # Try to load from fish_grid_params.json
            try:
                grid_params_path = "configs/fish_grid_params.json"
                with open(grid_params_path, 'r', encoding='utf-8') as f:
                    grid_params = json.load(f)
                fish_paths_path = grid_params.get('output', {}).get('waypoints_json_path', 'configs/fish_paths.json')
                grid = grid_params.get('grid', {})
                rows = int(grid.get('rows', 0))
                cols = int(grid.get('cols', 0))
                print(f"ä» fish_grid_params.json åŠ è½½è·¯å¾„: {fish_paths_path}, rows:{rows}, cols:{cols}")
            except Exception as e:
                print(f"æ— æ³•ä» fish_grid_params.json åŠ è½½è·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
                fish_paths_path = 'configs/fish_paths.json'
        
        try:
            with open(fish_paths_path, 'r', encoding='utf-8') as f:
                fish_path_json = json.load(f)
        except Exception as e:
            print(f"åŠ è½½é±¼è·¯å¾„é…ç½®å¤±è´¥ {fish_paths_path}: {e}")
            fish_path_json = {}
            
        try:
            while True:
                # æ•´ä¸ªå¾ªç¯è®¡æ—¶å¼€å§‹
                cycle_start = time.time()
                
                # æ•è·å¸§ï¼ˆä½¿ç”¨é‡è¯•æœºåˆ¶ï¼‰
                color_image, depth_image, success = self.capture_frames() #self.capture_frames_with_retry(max_retries=3, timeout_ms=10000)
                if not success:
                    print("âš ï¸  è·³è¿‡æ­¤å¸§ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€å¸§...")
                    continue
                
                self.frame_count = self.frame_count + 1
                        # è¿™é‡Œå¯ä»¥æ·»åŠ é‡æ–°è¿æ¥é€»è¾‘
                if self.frame_count < 10 :
                    print(f"è·³è¿‡å‰10å¸§ï¼Œç­‰å¾…ç›¸æœºç¨³å®š...")
                    if self.frame_count == 9 and show_preview:
                        self.show_preview(color_image, depth_image, None, None, None)
                        cv2.waitKey(1)
                    continue

                # æ£€æµ‹ + åˆ†å‰² + è½ç›˜ï¼ˆæœ€è¿‘ç›®æ ‡é€‰æ‹©ï¼‰
                mask_vis, base_name = self.detect_and_segment_and_dump_all(color_image, depth_image)
                
                # ä¿å­˜RGBå’Œæ·±åº¦å›¾åƒï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹ï¼‰
                if self.debug and base_name is not None:
                    # ä¿å­˜RGBå›¾åƒ
                    rgb_path = os.path.join(self.rgb_dir, f"{base_name}.png")
                    cv2.imwrite(rgb_path, color_image)
                    
                    # ä¿å­˜æ·±åº¦å›¾åƒï¼ˆåŸå§‹16ä½ï¼‰
                    depth_path = os.path.join(self.depth_dir, f"{base_name}.png")
                    cv2.imwrite(depth_path, depth_image.astype(np.uint16))
                    
                    # ä¿å­˜å¯è§†åŒ–æ·±åº¦å›¾åƒï¼ˆ8ä½å½©è‰²ï¼‰
                    valid_depth = depth_image > 0
                    if valid_depth.any():
                        depth_min = depth_image[valid_depth].min()
                        depth_max = depth_image[valid_depth].max()
                        depth_normalized = np.zeros_like(depth_image, dtype=np.uint8)
                        if depth_max > depth_min:
                            depth_normalized[valid_depth] = ((depth_image[valid_depth] - depth_min) / (depth_max - depth_min) * 255).astype(np.uint8)
                        depth_colormap = cv2.applyColorMap(depth_normalized, cv2.COLORMAP_JET)
                        depth_vis_path = os.path.join(self.depth_dir, f"{base_name}_visualization.png")
                        cv2.imwrite(depth_vis_path, depth_colormap)

                # ç”Ÿæˆæ£€æµ‹å¯è§†åŒ–
                detection_vis = None
                if mask_vis is not None:
                    # é‡æ–°è¿è¡Œæ£€æµ‹ä»¥è·å–è¾¹ç•Œæ¡†å¯è§†åŒ–
                    #if getattr(self, 'use_yolo', False):
                    boxes = self.detect_yolo(color_image, self.yolo_weights, conf=0.15, iou=0.45, imgsz=640)
                  
                    if boxes:
                        detection_vis = color_image.copy()
                        # ç»˜åˆ¶æ‰€æœ‰æ£€æµ‹æ¡†
                        for i, box in enumerate(boxes):
                            if len(box) >= 4:
                                x1, y1, x2, y2 = box[:4]
                                color = (0, 255, 0) if i == 0 else (0, 255, 255)  # ç¬¬ä¸€ä¸ªæ¡†ç”¨ç»¿è‰²ï¼Œå…¶ä»–ç”¨é»„è‰²
                                thickness = 3 if i == 0 else 2
                                cv2.rectangle(detection_vis, (x1, y1), (x2, y2), color, thickness)
                                if len(box) >= 5:
                                    confidence = box[4]
                                    cv2.putText(detection_vis, f"{confidence:.2f}", (x1, y1-10), 
                                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
                
                # ç”Ÿæˆå…³é”®ç‚¹/æ–¹å‘å¯è§†åŒ–
                landmark_vis = None
                if (self.grasp_point_mode == "ai" and self.landmark_detector is not None and 
                    mask_vis is not None):
                    try:
                        # æ ¹æ®æ©ç è®¡ç®—å¤–æ¥çŸ©å½¢ï¼Œå¾—åˆ°é±¼çš„è£å‰ªåŒºåŸŸ
                        ys, xs = np.where(mask_vis > 0)
                        if ys.size > 0 and xs.size > 0:
                            x1, y1 = int(xs.min()), int(ys.min())
                            x2, y2 = int(xs.max())+1, int(ys.max())+1
                            crop_bgr = color_image[y1:y2, x1:x2]
                            crop_rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)
                            
                            # é¢„æµ‹å…³é”®ç‚¹
                            pred_landmarks, pred_visibility = self.landmark_detector.predict(crop_rgb)
                            
                            # å¯è§†åŒ–å…³é”®ç‚¹
                            landmark_vis_crop = self.landmark_detector.visualize_landmarks(crop_rgb, pred_landmarks, pred_visibility)
                            
                            # å°†è£å‰ªåŒºåŸŸçš„å¯è§†åŒ–ç»“æœæ”¾å›åŸå›¾
                            landmark_vis = color_image.copy()
                            landmark_vis_bgr = cv2.cvtColor(landmark_vis_crop, cv2.COLOR_RGB2BGR)
                            landmark_vis[y1:y2, x1:x2] = landmark_vis_bgr
                            
                            # ç»˜åˆ¶è¾¹ç•Œæ¡†
                            cv2.rectangle(landmark_vis, (x1, y1), (x2, y2), (255, 0, 0), 2)
                    except Exception as e:
                        print(f"[å¯è§†åŒ–] å…³é”®ç‚¹é¢„æµ‹å¯è§†åŒ–å¤±è´¥: {e}")

                # è‹¥æœªç”Ÿæˆå…³é”®ç‚¹å¯è§†åŒ–ï¼Œåˆ™å¯è§†åŒ–ä¸»æ–¹å‘ï¼ˆPCAï¼‰
                if landmark_vis is None and mask_vis is not None:
                    try:
                        landmark_vis = draw_principal_axis(color_image, mask_vis > 0)
                    except Exception:
                        landmark_vis = None
                
                # æ˜¾ç¤ºé¢„è§ˆçª—å£
                self.show_preview(color_image, depth_image, mask_vis, detection_vis, landmark_vis)
                
                # ç¡®ä¿çª—å£æ˜¾ç¤ºå¹¶å¤„ç†æŒ‰é”®
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("ç”¨æˆ·æŒ‰ 'q' é”®åœæ­¢")
                    break
                elif key == ord('r') and self.fish_tracker is not None:
                    print("ç”¨æˆ·æŒ‰ 'r' é”®é‡ç½®å®¹å™¨")
                    self.fish_tracker.reset_container(confirm=True)
                elif key == ord('s') and self.fish_tracker is not None:
                    print("ç”¨æˆ·æŒ‰ 's' é”®æ˜¾ç¤ºçŠ¶æ€")
                    self.fish_tracker.print_status()
                elif key == ord('e') and self.fish_tracker is not None:
                    print("ç”¨æˆ·æŒ‰ 'e' é”®å¯¼å‡ºæ•°æ®")
                    self.fish_tracker.export_data()
                elif key == ord('p') and self.position_solver is not None:
                    print("ç”¨æˆ·æŒ‰ 'p' é”®æ˜¾ç¤ºæ”¾ç½®çŠ¶æ€")
                    self.position_solver.print_placement_status()
                elif key == ord('v') and self.position_solver is not None:
                    print("ç”¨æˆ·æŒ‰ 'v' é”®æ˜¾ç¤ºæ”¾ç½®å¯è§†åŒ–")
                    print(self.position_solver.visualize_placements())

                # æ ¹æ®æ©ç ç”Ÿæˆ3Dç‚¹äº‘å¹¶ä¿å­˜ï¼ˆå¯é€‰åº”ç”¨æ‰‹çœ¼æ ‡å®šï¼‰
                points_gripper = None  # åˆå§‹åŒ–å˜é‡

                #import pdb; pdb.set_trace()
                if mask_vis is not None and base_name is not None:
                    # ç‚¹äº‘ç”Ÿæˆè®¡æ—¶
                    pointcloud_start = time.time()
                    mask_bool = (mask_vis > 0)
                    points, colors = self.generate_pointcloud(color_image, depth_image, mask_bool)
                    pointcloud_time = time.time() - pointcloud_start
                    self.timers['pointcloud_generation'].append(pointcloud_time)
                    print(f"â±ï¸  pointcloud_generation: {pointcloud_time:.3f}s")

                    #import pdb; pdb.set_trace()
                    if len(points) > 0:
                        # åº”ç”¨æ‰‹çœ¼å˜æ¢ï¼šç›¸æœºâ†’å¤¹çˆª
                        points_gripper = self.apply_hand_eye_transform(points)
                        
                        # ä¿å­˜ç‚¹äº‘ï¼ˆä»…åœ¨debugæ¨¡å¼ä¸‹ï¼‰
                        if self.debug:
                            # ä¿å­˜ç›¸æœºåæ ‡ç³»ç‚¹äº‘
                            cam_ply = os.path.join(self.pointcloud_dir, f"{base_name}_cam_pointcloud.ply")
                            save_pointcloud_to_file(points, colors, cam_ply)
                            # ä¿å­˜å¤¹çˆªåæ ‡ç³»ç‚¹äº‘
                            grip_ply = os.path.join(self.pointcloud_dir, f"{base_name}_gripper_pointcloud.ply")
                            save_pointcloud_to_file(points_gripper, colors, grip_ply)
                
                # don't forget to transform the units, the point cloud is in meter, but robot
                # control would like to be in mm. 

                # è®¡ç®—ç‚¹äº‘è´¨å¿ƒå’Œæ³•å‘é‡ï¼ˆåœ¨å¤¹çˆªåæ ‡ç³»ä¸­ï¼‰
                if points_gripper is not None and len(points_gripper) > 0:
                    # æŠ“å–ç‚¹è®¡ç®—è®¡æ—¶
                    grasp_calc_start = time.time()
                    
                    # è·å–å½“å‰æœºå™¨äººTCPä½ç½®
                    tcp_result = self.robot.get_tcp_position()
                    if isinstance(tcp_result, tuple) and len(tcp_result) == 2:
                        tcp_ok, current_tcp = tcp_result
                    else:
                        # å¦‚æœåªè¿”å›ä¸€ä¸ªå€¼ï¼Œå‡è®¾å®ƒæ˜¯ä½ç½®ä¿¡æ¯
                        current_tcp = tcp_result
                        tcp_ok = True
                    print(f"å½“å‰TCPä½ç½®: {current_tcp}")
                    
                    # æ£€æŸ¥å®¹å™¨æ˜¯å¦å·²æ»¡
                    if self.fish_tracker is not None and self.fish_tracker.is_container_full():
                        print("ğŸ“¦ å®¹å™¨å·²æ»¡ï¼åœæ­¢æŠ“å–æ–°é±¼ã€‚")
                        print("æŒ‰ 'r' é”®é‡ç½®å®¹å™¨ï¼ŒæŒ‰ 'q' é”®é€€å‡º")
                        # æ˜¾ç¤ºçŠ¶æ€
                        self.fish_tracker.print_status()
                        continue
                    
                    # è®¡ç®—æŠ“å–ç‚¹ï¼ˆä¼˜å…ˆAIï¼‰
                    relative_move = None
                    angle_rad = 0
                    alpha_1 = None  # angle between headâ†’body (image) and vertical (image y-axis)
                    # å…ˆç”¨æ©ç åŸºäºPCAä¼°è®¡ä¸€ä¸ª alpha_1 ä½œä¸ºé»˜è®¤å€¼
                    try:
                        if mask_vis is not None:
                            alpha_1 = estimate_body_angle_alpha1(mask_vis > 0)
                            print(f"[PCA] ä¼°è®¡ alpha_1(rad) = {alpha_1:.4f}, deg = {np.degrees(alpha_1):.2f}")
                    except Exception as e:
                        print(f"[PCA] ä¼°è®¡ alpha_1 å¤±è´¥: {e}")
                    if self.grasp_point_mode == "ai" and self.landmark_detector is not None and mask_vis is not None:
                        try:
                            # æ ¹æ®æ©ç è®¡ç®—å¤–æ¥çŸ©å½¢ï¼Œå¾—åˆ°é±¼çš„è£å‰ªåŒºåŸŸ
                            ys, xs = np.where(mask_vis > 0)
                            if ys.size > 0 and xs.size > 0:
                                x1, y1 = int(xs.min()), int(ys.min())
                                x2, y2 = int(xs.max())+1, int(ys.max())+1
                                crop_bgr = color_image[y1:y2, x1:x2]
                                crop_rgb = cv2.cvtColor(crop_bgr, cv2.COLOR_BGR2RGB)
                                # é¢„æµ‹å±€éƒ¨åæ ‡ç³»ä¸‹çš„ä¸¤ä¸ªå…³é”®ç‚¹ï¼ˆ0=body_center, 1=head_centerï¼‰
                                pred_landmarks, pred_visibility = self.landmark_detector.predict(crop_rgb)
                                if pred_landmarks.shape[0] >= 2:
                                    body_xy_local = pred_landmarks[0]
                                    head_xy_local = pred_landmarks[1]
                                else:
                                    # å…¼å®¹åªæœ‰ä¸€ä¸ªç‚¹çš„æƒ…å†µ
                                    body_xy_local = pred_landmarks[0]
                                    head_xy_local = pred_landmarks[0]

                                # æ˜ å°„å›å…¨å›¾åæ ‡ï¼ˆåƒç´ ï¼‰
                                u_body = float(x1 + body_xy_local[0])
                                v_body = float(y1 + body_xy_local[1])
                                u_head = float(x1 + head_xy_local[0])
                                v_head = float(y1 + head_xy_local[1])

                                # æ·±åº¦ï¼ˆç±³ï¼‰
                                z_body_m = float(depth_image[int(round(v_body)), int(round(u_body))]) / 1000.0 if 0 <= int(round(v_body)) < depth_image.shape[0] and 0 <= int(round(u_body)) < depth_image.shape[1] else 0.0
                                z_head_m = float(depth_image[int(round(v_head)), int(round(u_head))]) / 1000.0 if 0 <= int(round(v_head)) < depth_image.shape[0] and 0 <= int(round(u_head)) < depth_image.shape[1] else 0.0
                                if z_body_m <= 0:
                                    print(f"æ— æ•ˆèº«ä½“ä¸­å¿ƒæ·±åº¦: {z_body_m}")
                                    raise ValueError("æ— æ•ˆæ·±åº¦")

                                # åæŠ•å½±åˆ°ç›¸æœºåæ ‡ç³»ï¼ˆèº«ä½“ä¸­å¿ƒï¼‰
                                Xb = (u_body - self.cx) / self.fx * z_body_m
                                Yb = (v_body - self.cy) / self.fy * z_body_m
                                point_cam_body = np.array([[Xb, Yb, z_body_m]], dtype=np.float32)

                                # åæŠ•å½±åˆ°ç›¸æœºåæ ‡ç³»ï¼ˆå¤´éƒ¨ä¸­å¿ƒï¼Œå¦‚æ— æ•ˆæ·±åº¦åˆ™æ²¿ç”¨èº«ä½“æ·±åº¦ï¼‰
                                if z_head_m <= 0:
                                    z_head_m = z_body_m
                                Xh = (u_head - self.cx) / self.fx * z_head_m
                                Yh = (v_head - self.cy) / self.fy * z_head_m
                                point_cam_head = np.array([[Xh, Yh, z_head_m]], dtype=np.float32)

                                # ç›¸æœºâ†’å¤¹çˆª
                                point_grip_body = self.apply_hand_eye_transform(point_cam_body)[0]
                                point_grip_head = self.apply_hand_eye_transform(point_cam_head)[0]
                                body_grip_mm = point_grip_body * 1000.0
                                head_grip_mm = point_grip_head * 1000.0

                                # æ–¹å‘å‘é‡ï¼ˆå›¾åƒåæ ‡ç³»ï¼Œå•ä½å‘é‡ï¼‰ headâ†’body
                                dir_img = np.array([u_body - u_head, v_body - v_head], dtype=np.float32)
                                norm_img = np.linalg.norm(dir_img) + 1e-6
                                dir_img_unit = (dir_img / norm_img).tolist()

                                # ä¸å›¾åƒç«–ç›´è½´(å³yè½´)çš„å¤¹è§’ï¼šä½¿ç”¨ atan2(vx, vy)
                                # è¿”å›[-pi, pi] çš„æœ‰ç¬¦å·è§’åº¦
                                alpha_1 = float(np.arctan2(dir_img[0], dir_img[1]))

                                # æ–¹å‘å‘é‡ï¼ˆå¤¹çˆªåæ ‡ç³»XYï¼Œå•ä½å‘é‡ï¼Œmmï¼‰
                                dir_grip_xy = np.array([head_grip_mm[0] - body_grip_mm[0], head_grip_mm[1] - body_grip_mm[1]], dtype=np.float32)
                                norm_grip = np.linalg.norm(dir_grip_xy) + 1e-6
                                dir_grip_xy_unit = (dir_grip_xy / norm_grip).tolist()

                                # å½“å‰æŠ“å–æŒ‰èº«ä½“ä¸­å¿ƒ
                                delta_tool_mm = [body_grip_mm[0], body_grip_mm[1], body_grip_mm[2]]
                                delta_base_xyz = self._tool_offset_to_base(delta_tool_mm, current_tcp[3:6])
                                z_offset = -delta_tool_mm[2] - 25

                                print(f"ğŸ¯ ä½¿ç”¨AIèº«ä½“ä¸­å¿ƒ: uv=({u_body:.1f},{v_body:.1f}) -> grip(mm)={body_grip_mm}")
                                print(f"ğŸ“ å¤´éƒ¨ä¸­å¿ƒ: uv=({u_head:.1f},{v_head:.1f}) -> grip(mm)={head_grip_mm}")
                                print(f"ğŸ§­ æ–¹å‘(åƒç´ xy,å•ä½å‘é‡) headâ†’body = {dir_img_unit}")
                                print(f"ğŸ“ ä¸å›¾åƒç«–ç›´è½´çš„å¤¹è§’ alpha_1(rad) = {alpha_1:.4f}, deg = {np.degrees(alpha_1):.2f}")
                                print(f"ğŸ§­ æ–¹å‘(å¤¹çˆªXY,å•ä½å‘é‡) bodyâ†’head = {dir_grip_xy_unit}")
                                # ä¸Xè½´(1,0,0)çš„å¤¹è§’ï¼ˆå¼§åº¦ï¼‰ï¼Œå¹¶è§„èŒƒåŒ–åˆ° [-pi/2, pi/2]
                                # è¿™æ ·æ— è®ºé±¼ä½“åŸå§‹æœå‘å¦‚ä½•ï¼Œéƒ½ä¼šè¢«æ˜ å°„åˆ°â€œæœå‘+XåŠå¹³é¢â€çš„ç­‰æ•ˆå§¿æ€ï¼Œä¾¿äºç»Ÿä¸€æ”¾ç½®æ–¹å‘
                                angle_rad = float(np.arctan2(dir_grip_xy_unit[1], dir_grip_xy_unit[0]))
                                if angle_rad > np.pi/2:
                                    angle_rad -= np.pi
                                elif angle_rad < -np.pi/2:
                                    angle_rad += np.pi
                                
                                relative_move = [delta_base_xyz[0], delta_base_xyz[1], z_offset, 0, 0, 0]

                                print(f"ğŸ§® æ–¹å‘ä¸Xè½´çš„å¤¹è§’(rad): {angle_rad:.4f}")
                            else:
                                print("[AI] æ©ç ä¸ºç©ºï¼Œå›é€€è´¨å¿ƒ")
                        except Exception as e:
                            print(f"[AI] é¢„æµ‹èº«ä½“ä¸­å¿ƒå¤±è´¥ï¼Œå›é€€è´¨å¿ƒ: {e}")

                    # è‹¥AIæœªç”Ÿæˆç§»åŠ¨ï¼Œä½¿ç”¨è´¨å¿ƒç‚¹äº‘æ–¹æ¡ˆ
                    if relative_move is None:
                        # è´¨å¿ƒç‚¹ï¼ˆå¤¹çˆªç³»ï¼‰
                        centroid = np.mean(points_gripper, axis=0)
                        print(f"å¤¹çˆªåæ ‡ç³»ç‚¹äº‘è´¨å¿ƒ: {centroid}")
                        center_gripper_mm = centroid * 1000
                        delta_tool_mm = [center_gripper_mm[0], center_gripper_mm[1], center_gripper_mm[2]]
                        delta_base_xyz = self._tool_offset_to_base(delta_tool_mm, current_tcp[3:6])
                        z_offset = -delta_tool_mm[2] -25
                        relative_move = [delta_base_xyz[0], delta_base_xyz[1], z_offset, 0, 0, 0]
                    
                    grasp_calc_time = time.time() - grasp_calc_start
                    self.timers['grasp_calculation'].append(grasp_calc_time)
                    print(f"â±ï¸  grasp_calculation: {grasp_calc_time:.3f}s")
                    
                    print("Step1 : å‡†å¤‡æŠ“å–")
                    print("ç›¸å¯¹ç§»åŠ¨é‡:", relative_move)
                    
                    # ä¼°ç®—é±¼é‡é‡ï¼ˆåœ¨æŠ“å–å‰ï¼‰
                    estimated_weight = 0.0
                    if self.fish_tracker is not None:
                        estimated_weight = self.estimate_fish_weight(points_gripper)
                        print(f"ğŸŸ ä¼°ç®—é±¼é‡é‡: {estimated_weight:.3f}kg")
                    
                    # æœºå™¨äººç§»åŠ¨è®¡æ—¶
                    robot_movement_start = time.time()
                    
                    # æ‰§è¡Œç›¸å¯¹ç§»åŠ¨
                    #import pdb; pdb.set_trace()    
                    fish_count += 1
                    counter = rows*cols
                    fish_count %= counter
                    
                    self.robot.set_digital_output(0, 0, 1)

                    # catch fish
                    ret = self.robot.linear_move(relative_move, 1, True, 500)
                  
                    # go back to original point
                    self.robot.linear_move(original_tcp, 0 , True, 40)
                    
                    # get target point1
                    xy_path = fish_path_json[str(fish_count)]
                    joint_pos1 = [0, 0, 0, 0, 0, 0]
                    joint_pos1[0] = xy_path[0][0]
                    joint_pos1[1] = xy_path[0][1]
                    joint_pos1[2] = 0
                    joint_pos1[3] = 0
                    joint_pos1[4] = 0
                    joint_pos1[5] = 0

                    ret = self.robot.linear_move(joint_pos1, 1, True, 400)
                    joint_pos2 = [0, 0, 0, 0, 0, 0]
                    joint_pos2[0] = xy_path[1][0]
                    joint_pos2[1] = xy_path[1][1]
                    joint_pos2[2] = -200
                    joint_pos2[3] = 0
                    joint_pos2[4] = 0
                    joint_pos2[5] = 0

                    target_xy = [xy_path[0][0] + xy_path[1][0], xy_path[0][1] + xy_path[1][1]]
                    start_xy = [original_tcp[0], original_tcp[1]]

                    start_vec = np.asarray(start_xy, dtype=np.float64)
                    target_vec = np.asarray(target_xy, dtype=np.float64)
                    distance_s_t = float(np.linalg.norm(target_vec - start_vec))

                    # è®¡ç®—ä»åŸç‚¹åˆ° start/target çš„å¤¹è§’å·®
                    alpha_2 = angle_between_2d_from_origin(start_vec, target_vec)
                    # è‹¥ä»æœªå¾—åˆ° alpha_1ï¼Œåˆ™ç½®ä¸º 0
                    if alpha_1 is None:
                        alpha_1 = 0.0
                    ret = self.robot.linear_move([start_xy[0], start_xy[1], 0, 0, 0, 0], 1 , True, 400)
                    print("fish : {}".format(fish_count))
                    print(joint_pos1)
                    print(joint_pos2)
                    # move to target point2
                    ret = self.robot.linear_move(joint_pos2, 1, True, 400)

                    offset_angle = np.pi / 2 - alpha_1 - alpha_2
                    print(f"offset_angle: {offset_angle:.4f}")

                    # rotate joint6 make sure the fish is vertical
                    ret = self.robot.joint_move([0, 0, 0, 0, 0, offset_angle], 1, True, 2)
                    self.robot.set_digital_output(0, 1, 1)
                    time.sleep(0.1)
                    ret = self.robot.linear_move([-joint_pos2[0], -joint_pos2[1], 200, 0, 0, 0], 1 , True, 400)
                    self.robot.linear_move(original_tcp, 0 , True, 200)
                    self.robot.set_digital_output(0,0,0)
                    self.robot.set_digital_output(0,1,0)

                    time.sleep(0.3)
                    
                    robot_movement_time = time.time() - robot_movement_start
                    self.timers['robot_movement'].append(robot_movement_time)
                    print(f"â±ï¸  robot_movement: {robot_movement_time:.3f}s")
            
                else:
                    print("ç‚¹äº‘ä¸ºç©ºï¼Œè·³è¿‡æœºå™¨äººæ§åˆ¶")

                # æ•´ä¸ªå¾ªç¯è®¡æ—¶ç»“æŸ
                cycle_time = time.time() - cycle_start
                self.timers['total_cycle'].append(cycle_time)
                print(f"â±ï¸  total_cycle: {cycle_time:.3f}s")
                print("-" * 50)
 
                # self.robot.logout()
                # exit()
                

        except KeyboardInterrupt:
            print("\nç”¨æˆ·ä¸­æ–­å¤„ç†")
            self.robot.set_digital_output(0, 0, 0)
        except Exception as e:
            print(f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        finally:
            self.cleanup()

    def cleanup(self):
        """
        æ¸…ç†èµ„æº
        """
        cv2.destroyAllWindows()
        if self.pipeline:
            self.pipeline.stop()
        
        # æ‰“å°æ—¶é—´ç»Ÿè®¡æ‘˜è¦
        self.print_timing_summary()
        
        print(f"å¤„ç†å®Œæˆï¼æ€»å…±å¤„ç†äº† {self.frame_count} å¸§")
        print(f"ç»“æœä¿å­˜åœ¨: {self.output_dir}")

def main():
    parser = argparse.ArgumentParser(description='å®æ—¶äººä½“åˆ†å‰²å’Œ3Dç‚¹äº‘ç”Ÿæˆ')
    parser.add_argument('--output_dir', type=str, default='realtime_output',
                      help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: realtime_output)')
    parser.add_argument('--device', type=str, default='cuda',
                      choices=['cpu', 'cuda'],
                      help='è¿è¡Œè®¾å¤‡ (é»˜è®¤: cuda)')
    parser.add_argument('--save_pointcloud', action='store_true',
                      help='ä¿å­˜3Dç‚¹äº‘')
    parser.add_argument('--max_frames', type=int, default=None,
                      help='æœ€å¤§å¤„ç†å¸§æ•° (é»˜è®¤: æ— é™)')
    parser.add_argument('--no_preview', action='store_true',
                      help='ä¸æ˜¾ç¤ºé¢„è§ˆçª—å£')
    parser.add_argument('--intrinsics_file', type=str, default=None,
                      help='ç›¸æœºå†…å‚JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--hand_eye_file', type=str, default=None,
                      help='æ‰‹çœ¼æ ‡å®š4x4é½æ¬¡çŸ©é˜µçš„.npyæ–‡ä»¶è·¯å¾„ï¼ˆç›¸æœºâ†’å¤¹çˆªï¼‰')
    parser.add_argument('--bbox_selection', type=str, default='highest_confidence',
                      choices=['smallest', 'largest', 'highest_confidence'],
                      help='è¾¹ç•Œæ¡†é€‰æ‹©ç­–ç•¥: smallest(é€‰æ‹©é¢ç§¯æœ€å°çš„é±¼) æˆ– largest(é€‰æ‹©é¢ç§¯æœ€å¤§çš„é±¼) (é»˜è®¤: largest)')
    parser.add_argument('--debug', action='store_true',
                      help='å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œä¿å­˜æ‰€æœ‰ä¸­é—´æ–‡ä»¶ï¼ˆRGBã€æ·±åº¦ã€æ£€æµ‹ã€åˆ†å‰²ã€ç‚¹äº‘ï¼‰')
    parser.add_argument('--use_yolo', action='store_true',
                      help='ä½¿ç”¨YOLOä½œä¸ºæ£€æµ‹å™¨ï¼ˆæ›¿ä»£Grounding DINOï¼‰')
    parser.add_argument('--yolo_weights', type=str, default=None,
                      help='YOLOæƒé‡æ–‡ä»¶(.pt)è·¯å¾„ï¼ˆä¸ --use_yolo æ­é…ä½¿ç”¨ï¼‰')
    parser.add_argument('--det_gray', action='store_true',
                      help='ä»…åœ¨æ£€æµ‹é˜¶æ®µä½¿ç”¨ç°åº¦å›¾åƒï¼ˆSAMä¸æŠ“å–ä¿æŒRGBï¼‰')
    parser.add_argument('--grasp_point_mode', type=str, default='centroid',
                      choices=['centroid', 'ai'],
                      help='æŠ“å–ç‚¹æ¨¡å¼: centroid(ç‚¹äº‘è´¨å¿ƒ) æˆ– ai(ä½¿ç”¨AIèº«ä½“ä¸­å¿ƒ)')
    parser.add_argument('--landmark_model_path', type=str, default=None,
                      help='AIèº«ä½“ä¸­å¿ƒæ¨¡å‹è·¯å¾„ (.pth)ï¼Œå½“ grasp_point_mode=ai æ—¶å¿…éœ€')
    parser.add_argument('--enable_weight_tracking', action='store_true',
                      help='å¯ç”¨é±¼é‡é‡è·Ÿè¸ªåŠŸèƒ½')
    parser.add_argument('--max_container_weight', type=float, default=12.5,
                      help='å®¹å™¨æœ€å¤§é‡é‡ï¼ˆkgï¼‰ï¼Œé»˜è®¤12.5kg')
    parser.add_argument('--fish_paths', type=str, default='configs/fish_paths.json',
                      help='é±¼è·¯å¾„é…ç½®JSONæ–‡ä»¶è·¯å¾„ (é»˜è®¤: configs/fish_paths.json)')
    parser.add_argument('--camera_calib_json', type=str, default=None,
                      help='æ‰‹çœ¼æ ‡å®šJSONæ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å« hand_eye.R å’Œ hand_eye.t')
    parser.add_argument('--robot_config', type=str, default='configs/robot.json',
                      help='æœºå™¨äººé…ç½®æ–‡ä»¶ï¼ŒåŒ…å«åˆå§‹ä½å§¿ (é»˜è®¤: configs/robot.json)')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºå¤„ç†å™¨
        processor = RealtimeSegmentation3D(
            output_dir=args.output_dir,
            device=args.device,
            save_pointcloud=args.save_pointcloud,
            intrinsics_file=args.intrinsics_file,
            hand_eye_file=args.hand_eye_file,
            bbox_selection=args.bbox_selection,
            debug=args.debug,
            use_yolo=args.use_yolo,
            yolo_weights=args.yolo_weights,
            grasp_point_mode=args.grasp_point_mode,
            landmark_model_path=args.landmark_model_path,
            enable_weight_tracking=args.enable_weight_tracking,
            max_container_weight=args.max_container_weight,
            det_gray=args.det_gray,
            fish_paths_file=args.fish_paths,
            camera_calib_json=args.camera_calib_json,
            robot_config=args.robot_config
        )
        # è¿è¡Œå®æ—¶å¤„ç†
        processor.run_realtime(
            max_frames=args.max_frames,
            show_preview=not args.no_preview
        )
        
    except Exception as e:
        print(f"ç¨‹åºå‡ºé”™: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
