#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜å·¥å…·

è¿™ä¸ªæ¨¡å—æä¾›äº†ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œå¯ä»¥é›†æˆåˆ°ç°æœ‰çš„YOLOè®­ç»ƒæµç¨‹ä¸­ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. è¯†åˆ«è®­ç»ƒè¿‡ç¨‹ä¸­çš„å›°éš¾æ ·æœ¬
2. åŠ¨æ€è°ƒæ•´æ ·æœ¬æƒé‡
3. ç”Ÿæˆç¡¬è´Ÿæ ·æœ¬æŠ¥å‘Š
4. å¯è§†åŒ–å›°éš¾æ ·æœ¬åˆ†å¸ƒ

ä½¿ç”¨æ–¹æ³•ï¼š
    from detection.hard_negative_mining import HardNegativeMiner
    
    miner = HardNegativeMiner()
    hard_negatives = miner.find_hard_negatives(predictions, ground_truth)
"""

import numpy as np
import cv2
import json
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from datetime import datetime
import matplotlib.pyplot as plt


class HardNegativeMiner:
    """ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜å™¨"""
    
    def __init__(self, 
                 confidence_threshold: float = 0.5,
                 iou_threshold: float = 0.5,
                 hard_negative_ratio: float = 0.3,
                 save_samples: bool = False,
                 output_dir: str = "hard_negatives"):
        """
        åˆå§‹åŒ–ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜å™¨
        
        Args:
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            iou_threshold: IoUé˜ˆå€¼
            hard_negative_ratio: ç¡¬è´Ÿæ ·æœ¬æ¯”ä¾‹
            save_samples: æ˜¯å¦ä¿å­˜å›°éš¾æ ·æœ¬
            output_dir: è¾“å‡ºç›®å½•
        """
        self.confidence_threshold = confidence_threshold
        self.iou_threshold = iou_threshold
        self.hard_negative_ratio = hard_negative_ratio
        self.save_samples = save_samples
        self.output_dir = Path(output_dir)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if self.save_samples:
            self.output_dir.mkdir(parents=True, exist_ok=True)
            (self.output_dir / "images").mkdir(exist_ok=True)
            (self.output_dir / "reports").mkdir(exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_predictions': 0,
            'hard_negatives': 0,
            'false_positives': 0,
            'false_negatives': 0,
            'true_positives': 0,
            'confidence_distribution': [],
            'iou_distribution': []
        }
        
        print(f"âœ… ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ç½®ä¿¡åº¦é˜ˆå€¼: {confidence_threshold}")
        print(f"   IoUé˜ˆå€¼: {iou_threshold}")
        print(f"   ç¡¬è´Ÿæ ·æœ¬æ¯”ä¾‹: {hard_negative_ratio}")
    
    def calculate_iou(self, box1: List[float], box2: List[float]) -> float:
        """è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„IoU"""
        x1_1, y1_1, x2_1, y2_1 = box1
        x1_2, y1_2, x2_2, y2_2 = box2
        
        # è®¡ç®—äº¤é›†
        x1_i = max(x1_1, x1_2)
        y1_i = max(y1_1, y1_2)
        x2_i = min(x2_1, x2_2)
        y2_i = min(y2_1, y2_2)
        
        if x2_i <= x1_i or y2_i <= y1_i:
            return 0.0
        
        intersection = (x2_i - x1_i) * (y2_i - y1_i)
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0
    
    def find_hard_negatives(self, 
                          predictions: List[Dict], 
                          ground_truth: List[Dict],
                          image_path: Optional[str] = None) -> List[Dict]:
        """
        æ‰¾åˆ°ç¡¬è´Ÿæ ·æœ¬
        
        Args:
            predictions: æ¨¡å‹é¢„æµ‹ç»“æœ [{'bbox': [x1,y1,x2,y2], 'confidence': float, 'class': int}]
            ground_truth: çœŸå®æ ‡æ³¨ [{'bbox': [x1,y1,x2,y2], 'class': int}]
            image_path: å›¾åƒè·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ç¡¬è´Ÿæ ·æœ¬åˆ—è¡¨
        """
        hard_negatives = []
        
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.stats['total_predictions'] += len(predictions)
        
        # ä¸ºæ¯ä¸ªé¢„æµ‹æ‰¾åˆ°æœ€ä½³åŒ¹é…çš„çœŸå®æ ‡æ³¨
        matched_gt = set()
        
        for pred in predictions:
            best_iou = 0.0
            best_gt_idx = -1
            
            # æ‰¾åˆ°IoUæœ€é«˜çš„çœŸå®æ ‡æ³¨
            for i, gt in enumerate(ground_truth):
                if i in matched_gt:
                    continue
                    
                iou = self.calculate_iou(pred['bbox'], gt['bbox'])
                if iou > best_iou:
                    best_iou = iou
                    best_gt_idx = i
            
            # è®°å½•IoUåˆ†å¸ƒ
            self.stats['iou_distribution'].append(best_iou)
            self.stats['confidence_distribution'].append(pred['confidence'])
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºç¡¬è´Ÿæ ·æœ¬
            if pred['confidence'] > self.confidence_threshold:
                if best_iou < self.iou_threshold:
                    # é«˜ç½®ä¿¡åº¦ä½†ä½IoU -> å‡é˜³æ€§ï¼ˆç¡¬è´Ÿæ ·æœ¬ï¼‰
                    hard_negatives.append({
                        'type': 'false_positive',
                        'prediction': pred,
                        'ground_truth': ground_truth[best_gt_idx] if best_gt_idx >= 0 else None,
                        'iou': best_iou,
                        'confidence': pred['confidence'],
                        'image_path': image_path
                    })
                    self.stats['false_positives'] += 1
                    self.stats['hard_negatives'] += 1
                else:
                    # é«˜ç½®ä¿¡åº¦é«˜IoU -> çœŸé˜³æ€§
                    self.stats['true_positives'] += 1
                    if best_gt_idx >= 0:
                        matched_gt.add(best_gt_idx)
            else:
                if best_iou >= self.iou_threshold:
                    # ä½ç½®ä¿¡åº¦é«˜IoU -> å‡é˜´æ€§
                    hard_negatives.append({
                        'type': 'false_negative',
                        'prediction': pred,
                        'ground_truth': ground_truth[best_gt_idx] if best_gt_idx >= 0 else None,
                        'iou': best_iou,
                        'confidence': pred['confidence'],
                        'image_path': image_path
                    })
                    self.stats['false_negatives'] += 1
                    self.stats['hard_negatives'] += 1
        
        return hard_negatives
    
    def analyze_difficulty_distribution(self, predictions: List[Dict], ground_truth: List[Dict]) -> Dict:
        """
        åˆ†æå›°éš¾æ ·æœ¬çš„åˆ†å¸ƒ
        
        Args:
            predictions: é¢„æµ‹ç»“æœ
            ground_truth: çœŸå®æ ‡æ³¨
            
        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        analysis = {
            'confidence_ranges': {
                'high_conf_low_iou': 0,    # é«˜ç½®ä¿¡åº¦ä½IoU
                'high_conf_high_iou': 0,   # é«˜ç½®ä¿¡åº¦é«˜IoU
                'low_conf_low_iou': 0,     # ä½ç½®ä¿¡åº¦ä½IoU
                'low_conf_high_iou': 0     # ä½ç½®ä¿¡åº¦é«˜IoU
            },
            'iou_ranges': {
                'very_low': 0,    # IoU < 0.3
                'low': 0,         # 0.3 <= IoU < 0.5
                'medium': 0,      # 0.5 <= IoU < 0.7
                'high': 0         # IoU >= 0.7
            },
            'confidence_stats': {},
            'iou_stats': {}
        }
        
        ious = []
        confidences = []
        
        for pred in predictions:
            best_iou = 0.0
            for gt in ground_truth:
                iou = self.calculate_iou(pred['bbox'], gt['bbox'])
                best_iou = max(best_iou, iou)
            
            ious.append(best_iou)
            confidences.append(pred['confidence'])
            
            # åˆ†ç±»ç½®ä¿¡åº¦å’ŒIoUç»„åˆ
            if pred['confidence'] >= self.confidence_threshold:
                if best_iou >= self.iou_threshold:
                    analysis['confidence_ranges']['high_conf_high_iou'] += 1
                else:
                    analysis['confidence_ranges']['high_conf_low_iou'] += 1
            else:
                if best_iou >= self.iou_threshold:
                    analysis['confidence_ranges']['low_conf_high_iou'] += 1
                else:
                    analysis['confidence_ranges']['low_conf_low_iou'] += 1
            
            # åˆ†ç±»IoUèŒƒå›´
            if best_iou < 0.3:
                analysis['iou_ranges']['very_low'] += 1
            elif best_iou < 0.5:
                analysis['iou_ranges']['low'] += 1
            elif best_iou < 0.7:
                analysis['iou_ranges']['medium'] += 1
            else:
                analysis['iou_ranges']['high'] += 1
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if ious:
            analysis['iou_stats'] = {
                'mean': np.mean(ious),
                'std': np.std(ious),
                'min': np.min(ious),
                'max': np.max(ious)
            }
        
        if confidences:
            analysis['confidence_stats'] = {
                'mean': np.mean(confidences),
                'std': np.std(confidences),
                'min': np.min(confidences),
                'max': np.max(confidences)
            }
        
        return analysis
    
    def visualize_difficulty_distribution(self, 
                                        predictions: List[Dict], 
                                        ground_truth: List[Dict],
                                        save_path: Optional[str] = None):
        """
        å¯è§†åŒ–å›°éš¾æ ·æœ¬åˆ†å¸ƒ
        
        Args:
            predictions: é¢„æµ‹ç»“æœ
            ground_truth: çœŸå®æ ‡æ³¨
            save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        # æ”¶é›†æ•°æ®
        ious = []
        confidences = []
        colors = []
        
        for pred in predictions:
            best_iou = 0.0
            for gt in ground_truth:
                iou = self.calculate_iou(pred['bbox'], gt['bbox'])
                best_iou = max(best_iou, iou)
            
            ious.append(best_iou)
            confidences.append(pred['confidence'])
            
            # æ ¹æ®ç±»å‹è®¾ç½®é¢œè‰²
            if pred['confidence'] >= self.confidence_threshold and best_iou >= self.iou_threshold:
                colors.append('green')  # çœŸé˜³æ€§
            elif pred['confidence'] >= self.confidence_threshold and best_iou < self.iou_threshold:
                colors.append('red')    # å‡é˜³æ€§
            elif pred['confidence'] < self.confidence_threshold and best_iou >= self.iou_threshold:
                colors.append('orange') # å‡é˜´æ€§
            else:
                colors.append('blue')   # çœŸé˜´æ€§
        
        # åˆ›å»ºå›¾å½¢
        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))
        
        # æ•£ç‚¹å›¾ï¼šç½®ä¿¡åº¦ vs IoU
        scatter = ax1.scatter(confidences, ious, c=colors, alpha=0.6)
        ax1.axhline(y=self.iou_threshold, color='black', linestyle='--', alpha=0.5)
        ax1.axvline(x=self.confidence_threshold, color='black', linestyle='--', alpha=0.5)
        ax1.set_xlabel('Confidence')
        ax1.set_ylabel('IoU')
        ax1.set_title('Confidence vs IoU Distribution')
        ax1.grid(True, alpha=0.3)
        
        # ç½®ä¿¡åº¦åˆ†å¸ƒç›´æ–¹å›¾
        ax2.hist(confidences, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        ax2.axvline(x=self.confidence_threshold, color='red', linestyle='--', label='Threshold')
        ax2.set_xlabel('Confidence')
        ax2.set_ylabel('Count')
        ax2.set_title('Confidence Distribution')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # IoUåˆ†å¸ƒç›´æ–¹å›¾
        ax3.hist(ious, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')
        ax3.axvline(x=self.iou_threshold, color='red', linestyle='--', label='Threshold')
        ax3.set_xlabel('IoU')
        ax3.set_ylabel('Count')
        ax3.set_title('IoU Distribution')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š å›°éš¾æ ·æœ¬åˆ†å¸ƒå›¾å·²ä¿å­˜: {save_path}")
        
        plt.show()
    
    def generate_report(self, output_path: Optional[str] = None) -> Dict:
        """
        ç”Ÿæˆç¡¬è´Ÿæ ·æœ¬æŒ–æ˜æŠ¥å‘Š
        
        Args:
            output_path: æŠ¥å‘Šä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æŠ¥å‘Šå­—å…¸
        """
        # è®¡ç®—å‡†ç¡®ç‡æŒ‡æ ‡
        total = self.stats['total_predictions']
        if total > 0:
            precision = self.stats['true_positives'] / (self.stats['true_positives'] + self.stats['false_positives']) if (self.stats['true_positives'] + self.stats['false_positives']) > 0 else 0
            recall = self.stats['true_positives'] / (self.stats['true_positives'] + self.stats['false_negatives']) if (self.stats['true_positives'] + self.stats['false_negatives']) > 0 else 0
            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        else:
            precision = recall = f1_score = 0
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        confidence_stats = {}
        iou_stats = {}
        
        if self.stats['confidence_distribution']:
            confidences = np.array(self.stats['confidence_distribution'])
            confidence_stats = {
                'mean': float(np.mean(confidences)),
                'std': float(np.std(confidences)),
                'min': float(np.min(confidences)),
                'max': float(np.max(confidences)),
                'median': float(np.median(confidences))
            }
        
        if self.stats['iou_distribution']:
            ious = np.array(self.stats['iou_distribution'])
            iou_stats = {
                'mean': float(np.mean(ious)),
                'std': float(np.std(ious)),
                'min': float(np.min(ious)),
                'max': float(np.max(ious)),
                'median': float(np.median(ious))
            }
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'timestamp': datetime.now().isoformat(),
            'parameters': {
                'confidence_threshold': self.confidence_threshold,
                'iou_threshold': self.iou_threshold,
                'hard_negative_ratio': self.hard_negative_ratio
            },
            'statistics': {
                'total_predictions': self.stats['total_predictions'],
                'hard_negatives': self.stats['hard_negatives'],
                'false_positives': self.stats['false_positives'],
                'false_negatives': self.stats['false_negatives'],
                'true_positives': self.stats['true_positives']
            },
            'metrics': {
                'precision': precision,
                'recall': recall,
                'f1_score': f1_score,
                'hard_negative_rate': self.stats['hard_negatives'] / total if total > 0 else 0
            },
            'distributions': {
                'confidence': confidence_stats,
                'iou': iou_stats
            }
        }
        
        # ä¿å­˜æŠ¥å‘Š
        if output_path:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“Š ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
        
        return report
    
    def print_summary(self):
        """æ‰“å°æŒ–æ˜æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜æ‘˜è¦")
        print("="*60)
        print(f"æ€»é¢„æµ‹æ•°: {self.stats['total_predictions']}")
        print(f"ç¡¬è´Ÿæ ·æœ¬æ•°: {self.stats['hard_negatives']}")
        print(f"å‡é˜³æ€§æ•°: {self.stats['false_positives']}")
        print(f"å‡é˜´æ€§æ•°: {self.stats['false_negatives']}")
        print(f"çœŸé˜³æ€§æ•°: {self.stats['true_positives']}")
        
        if self.stats['total_predictions'] > 0:
            hard_negative_rate = self.stats['hard_negatives'] / self.stats['total_predictions']
            print(f"ç¡¬è´Ÿæ ·æœ¬ç‡: {hard_negative_rate:.2%}")
        
        if self.stats['confidence_distribution']:
            confidences = np.array(self.stats['confidence_distribution'])
            print(f"å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidences):.3f}")
        
        if self.stats['iou_distribution']:
            ious = np.array(self.stats['iou_distribution'])
            print(f"å¹³å‡IoU: {np.mean(ious):.3f}")
        
        print("="*60)


def integrate_with_yolo_training():
    """
    å±•ç¤ºå¦‚ä½•å°†ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜é›†æˆåˆ°YOLOè®­ç»ƒä¸­
    
    è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹å‡½æ•°ï¼Œå±•ç¤ºå¦‚ä½•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜
    """
    print("ğŸ”§ ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜é›†æˆç¤ºä¾‹")
    print("="*50)
    
    # åˆ›å»ºæŒ–æ˜å™¨
    miner = HardNegativeMiner(
        confidence_threshold=0.5,
        iou_threshold=0.5,
        hard_negative_ratio=0.3,
        save_samples=True
    )
    
    # æ¨¡æ‹Ÿé¢„æµ‹ç»“æœ
    predictions = [
        {'bbox': [100, 100, 200, 200], 'confidence': 0.8, 'class': 0},
        {'bbox': [300, 300, 400, 400], 'confidence': 0.3, 'class': 0},
        {'bbox': [500, 500, 600, 600], 'confidence': 0.9, 'class': 0}
    ]
    
    # æ¨¡æ‹ŸçœŸå®æ ‡æ³¨
    ground_truth = [
        {'bbox': [110, 110, 210, 210], 'class': 0},
        {'bbox': [520, 520, 620, 620], 'class': 0}
    ]
    
    # è¿›è¡Œç¡¬è´Ÿæ ·æœ¬æŒ–æ˜
    hard_negatives = miner.find_hard_negatives(predictions, ground_truth)
    
    print(f"æ‰¾åˆ° {len(hard_negatives)} ä¸ªç¡¬è´Ÿæ ·æœ¬")
    for i, hn in enumerate(hard_negatives):
        print(f"  {i+1}. ç±»å‹: {hn['type']}, ç½®ä¿¡åº¦: {hn['confidence']:.3f}, IoU: {hn['iou']:.3f}")
    
    # åˆ†æå›°éš¾æ ·æœ¬åˆ†å¸ƒ
    analysis = miner.analyze_difficulty_distribution(predictions, ground_truth)
    print(f"\nå›°éš¾æ ·æœ¬åˆ†æ:")
    print(f"  é«˜ç½®ä¿¡åº¦ä½IoU: {analysis['confidence_ranges']['high_conf_low_iou']}")
    print(f"  é«˜ç½®ä¿¡åº¦é«˜IoU: {analysis['confidence_ranges']['high_conf_high_iou']}")
    print(f"  ä½ç½®ä¿¡åº¦ä½IoU: {analysis['confidence_ranges']['low_conf_low_iou']}")
    print(f"  ä½ç½®ä¿¡åº¦é«˜IoU: {analysis['confidence_ranges']['low_conf_high_iou']}")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = miner.generate_report()
    print(f"\næ¨¡å‹æ€§èƒ½æŒ‡æ ‡:")
    print(f"  ç²¾ç¡®ç‡: {report['metrics']['precision']:.3f}")
    print(f"  å¬å›ç‡: {report['metrics']['recall']:.3f}")
    print(f"  F1åˆ†æ•°: {report['metrics']['f1_score']:.3f}")
    
    # æ‰“å°æ‘˜è¦
    miner.print_summary()


if __name__ == "__main__":
    # è¿è¡Œé›†æˆç¤ºä¾‹
    integrate_with_yolo_training()
