# å¤šç±»åˆ«æ£€æµ‹é…ç½®æŒ‡å—

## ğŸ¯ ç±»åˆ«é…ç½®

### å½“å‰ç±»åˆ«è®¾ç½®
```yaml
# datasets/l0_9.12/dataset.yaml
names: ['èƒŒæ™¯', 'é±¿é±¼']
```

### ç±»åˆ«ç´¢å¼•æ˜ å°„
- **ç±»åˆ« 0**: èƒŒæ™¯ (background)
- **ç±»åˆ« 1**: é±¿é±¼ (squid)

## ğŸ“Š è®­ç»ƒè¾“å‡ºè§£è¯»

### å¤šç±»åˆ«æ£€æµ‹çš„è¾“å‡ºæ ¼å¼
```
Class     Images  Instances      Box(P          R      mAP50  mAP50-95)
all        241       1160      0.944      0.972      0.977      0.722
èƒŒæ™¯         241        800      0.950      0.980      0.985      0.750
é±¿é±¼         241        360      0.938      0.964      0.969      0.694
```

### æŒ‡æ ‡è¯´æ˜
- **all**: æ‰€æœ‰ç±»åˆ«çš„å¹³å‡æ€§èƒ½
- **èƒŒæ™¯**: èƒŒæ™¯ç±»åˆ«çš„æ£€æµ‹æ€§èƒ½
- **é±¿é±¼**: é±¿é±¼ç±»åˆ«çš„æ£€æµ‹æ€§èƒ½

## ğŸ”§ è®­ç»ƒå‘½ä»¤

### åŸºæœ¬è®­ç»ƒ
```bash
python3 detection/train_yolo.py \
    --data ./datasets/l0_9.12/dataset.yaml \
    --model yolov8s.pt \
    --epochs 100 \
    --project runs/train \
    --name multi_class_squid_background_$(date +%Y%m%d_%H%M%S)
```

### ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜è®­ç»ƒ
```bash
python3 detection/train_yolo_with_hard_negative.py \
    --data ./datasets/l0_9.12/dataset.yaml \
    --model yolov8s.pt \
    --epochs 100 \
    --project runs/train \
    --name multi_class_hard_negative_$(date +%Y%m%d_%H%M%S) \
    --mining_strategy confidence_based \
    --hard_negative_ratio 0.3
```

## ğŸ“ˆ æ€§èƒ½åˆ†æ

### ç±»åˆ«å¹³è¡¡æ€§æ£€æŸ¥
```python
# æ£€æŸ¥æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡
def analyze_class_distribution(dataset_path):
    train_labels = Path(dataset_path) / "labels" / "train"
    val_labels = Path(dataset_path) / "labels" / "val"
    
    class_counts = {0: 0, 1: 0}  # èƒŒæ™¯, é±¿é±¼
    
    for split in [train_labels, val_labels]:
        for label_file in split.glob("*.txt"):
            with open(label_file, 'r') as f:
                for line in f:
                    if line.strip():
                        class_id = int(line.split()[0])
                        class_counts[class_id] += 1
    
    print("ç±»åˆ«åˆ†å¸ƒ:")
    print(f"èƒŒæ™¯ (ç±»åˆ«0): {class_counts[0]} ä¸ªå®ä¾‹")
    print(f"é±¿é±¼ (ç±»åˆ«1): {class_counts[1]} ä¸ªå®ä¾‹")
    
    return class_counts
```

### ç±»åˆ«ä¸å¹³è¡¡å¤„ç†
å¦‚æœå‘ç°ç±»åˆ«ä¸å¹³è¡¡ï¼Œå¯ä»¥ï¼š

1. **è°ƒæ•´æŸå¤±æƒé‡**
```python
# åœ¨è®­ç»ƒæ—¶ä½¿ç”¨ç±»åˆ«æƒé‡
class_weights = [1.0, 2.0]  # ç»™é±¿é±¼æ›´é«˜æƒé‡
```

2. **æ•°æ®å¢å¼º**
```python
# å¯¹å°‘æ•°ç±»åˆ«è¿›è¡Œæ›´å¤šå¢å¼º
--aug strong
--mixup 0.3
--copy_paste 0.4
```

3. **ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜**
```python
# é‡ç‚¹å…³æ³¨å›°éš¾æ ·æœ¬
--mining_strategy confidence_based
--hard_negative_ratio 0.4
```

## ğŸ¯ å®é™…åº”ç”¨åœºæ™¯

### åœºæ™¯1: é±¿é±¼æ£€æµ‹
- **ç›®æ ‡**: å‡†ç¡®æ£€æµ‹é±¿é±¼
- **ç­–ç•¥**: é‡ç‚¹å…³æ³¨é±¿é±¼ç±»åˆ«çš„æ€§èƒ½
- **æŒ‡æ ‡**: é±¿é±¼çš„mAP50å’ŒRecall

### åœºæ™¯2: èƒŒæ™¯è¿‡æ»¤
- **ç›®æ ‡**: å‡å°‘èƒŒæ™¯è¯¯æ£€
- **ç­–ç•¥**: æé«˜èƒŒæ™¯ç±»åˆ«çš„ç²¾ç¡®ç‡
- **æŒ‡æ ‡**: èƒŒæ™¯çš„Precision

### åœºæ™¯3: å¹³è¡¡æ£€æµ‹
- **ç›®æ ‡**: ä¸¤ä¸ªç±»åˆ«éƒ½è¡¨ç°è‰¯å¥½
- **ç­–ç•¥**: å…³æ³¨æ•´ä½“mAP50
- **æŒ‡æ ‡**: allç±»åˆ«çš„ç»¼åˆæ€§èƒ½

## ğŸ“Š æ€§èƒ½ç›‘æ§

### è®­ç»ƒè¿‡ç¨‹ç›‘æ§
```python
# ç›‘æ§æ¯ä¸ªç±»åˆ«çš„æ€§èƒ½å˜åŒ–
def monitor_class_performance(results):
    for epoch, result in enumerate(results):
        print(f"Epoch {epoch}:")
        print(f"  èƒŒæ™¯ mAP50: {result['background_map50']:.3f}")
        print(f"  é±¿é±¼ mAP50: {result['squid_map50']:.3f}")
        print(f"  æ•´ä½“ mAP50: {result['overall_map50']:.3f}")
```

### ç±»åˆ«ç‰¹å®šåˆ†æ
```python
# åˆ†ææ¯ä¸ªç±»åˆ«çš„å›°éš¾æ ·æœ¬
def analyze_class_specific_hard_negatives(hard_negatives):
    background_hard = [hn for hn in hard_negatives if hn['prediction']['class'] == 0]
    squid_hard = [hn for hn in hard_negatives if hn['prediction']['class'] == 1]
    
    print(f"èƒŒæ™¯å›°éš¾æ ·æœ¬: {len(background_hard)}")
    print(f"é±¿é±¼å›°éš¾æ ·æœ¬: {len(squid_hard)}")
```

## ğŸ” è°ƒè¯•æŠ€å·§

### 1. ç±»åˆ«æ··æ·†åˆ†æ
```python
# åˆ†æç±»åˆ«é—´çš„æ··æ·†æƒ…å†µ
def analyze_class_confusion(predictions, ground_truth):
    confusion_matrix = np.zeros((2, 2))  # 2x2çŸ©é˜µ
    
    for pred, gt in zip(predictions, ground_truth):
        pred_class = pred['class']
        gt_class = gt['class']
        confusion_matrix[gt_class][pred_class] += 1
    
    print("æ··æ·†çŸ©é˜µ:")
    print("        é¢„æµ‹")
    print("å®é™…    èƒŒæ™¯  é±¿é±¼")
    print(f"èƒŒæ™¯   {confusion_matrix[0][0]:.0f}   {confusion_matrix[0][1]:.0f}")
    print(f"é±¿é±¼   {confusion_matrix[1][0]:.0f}   {confusion_matrix[1][1]:.0f}")
```

### 2. è¾¹ç•Œæ¡†è´¨é‡åˆ†æ
```python
# åˆ†ææ¯ä¸ªç±»åˆ«çš„è¾¹ç•Œæ¡†è´¨é‡
def analyze_bbox_quality(predictions, ground_truth):
    for class_id, class_name in enumerate(['èƒŒæ™¯', 'é±¿é±¼']):
        class_predictions = [p for p in predictions if p['class'] == class_id]
        class_ground_truth = [g for g in ground_truth if g['class'] == class_id]
        
        ious = []
        for pred in class_predictions:
            max_iou = 0
            for gt in class_ground_truth:
                iou = calculate_iou(pred['bbox'], gt['bbox'])
                max_iou = max(max_iou, iou)
            ious.append(max_iou)
        
        avg_iou = np.mean(ious) if ious else 0
        print(f"{class_name} å¹³å‡IoU: {avg_iou:.3f}")
```

## ğŸ“ æœ€ä½³å®è·µ

### 1. æ•°æ®å‡†å¤‡
- ç¡®ä¿ä¸¤ä¸ªç±»åˆ«éƒ½æœ‰è¶³å¤Ÿçš„æ ·æœ¬
- æ£€æŸ¥æ ‡æ³¨è´¨é‡
- å¹³è¡¡è®­ç»ƒé›†å’ŒéªŒè¯é›†

### 2. è®­ç»ƒç­–ç•¥
- ä½¿ç”¨é€‚å½“çš„æ•°æ®å¢å¼º
- ç›‘æ§ç±»åˆ«å¹³è¡¡æ€§
- è°ƒæ•´å­¦ä¹ ç‡å’Œè®­ç»ƒè½®æ•°

### 3. è¯„ä¼°æ–¹æ³•
- å…³æ³¨æ¯ä¸ªç±»åˆ«çš„æ€§èƒ½
- åˆ†ææ··æ·†çŸ©é˜µ
- æ£€æŸ¥å›°éš¾æ ·æœ¬

### 4. ä¼˜åŒ–æ–¹å‘
- æ ¹æ®æ€§èƒ½åˆ†æè°ƒæ•´ç­–ç•¥
- ä½¿ç”¨ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜
- è€ƒè™‘ç±»åˆ«æƒé‡è°ƒæ•´

## ğŸ¯ æ€»ç»“

å¤šç±»åˆ«æ£€æµ‹çš„å…³é”®ç‚¹ï¼š

1. **ç±»åˆ«é…ç½®**: æ­£ç¡®è®¾ç½®ç±»åˆ«åç§°å’Œç´¢å¼•
2. **æ€§èƒ½ç›‘æ§**: åˆ†åˆ«ç›‘æ§æ¯ä¸ªç±»åˆ«çš„æ€§èƒ½
3. **å¹³è¡¡æ€§**: ç¡®ä¿ç±»åˆ«é—´çš„å¹³è¡¡
4. **å›°éš¾æ ·æœ¬**: ä½¿ç”¨ç¡¬è´Ÿæ ·æœ¬æŒ–æ˜æé«˜æ€§èƒ½
5. **è°ƒè¯•åˆ†æ**: æ·±å…¥åˆ†æç±»åˆ«é—´çš„æ··æ·†æƒ…å†µ

é€šè¿‡åˆç†çš„é…ç½®å’Œç›‘æ§ï¼Œå¯ä»¥å®ç°é«˜è´¨é‡çš„å¤šç±»åˆ«æ£€æµ‹æ¨¡å‹ã€‚











