#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®æ—¶é±¼ä½“å…³é”®ç‚¹æ£€æµ‹

é›†æˆåˆ°ä¸»ç³»ç»Ÿä¸­ï¼Œç”¨äºå®æ—¶æ£€æµ‹é±¼çš„å…³é”®ç‚¹å¹¶è®¡ç®—ç²¾ç¡®çš„æŠ“å–ä½ç½®
"""

import cv2
import numpy as np
import torch
import time
from typing import Tuple, Optional, Dict, Any
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from fish_landmark_detector import FishLandmarkDetector


class RealtimeFishLandmarkDetector:
    """å®æ—¶é±¼ä½“å…³é”®ç‚¹æ£€æµ‹å™¨"""
    
    def __init__(self, model_path: str, device: str = 'cuda', confidence_threshold: float = 0.5):
        """
        åˆå§‹åŒ–å®æ—¶æ£€æµ‹å™¨
        
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡ ('cuda' æˆ– 'cpu')
            confidence_threshold: å…³é”®ç‚¹å¯è§æ€§é˜ˆå€¼
        """
        self.detector = FishLandmarkDetector(model_path=model_path, device=device)
        self.confidence_threshold = confidence_threshold
        self.landmark_names = self.detector.landmark_names
        
        # æ€§èƒ½ç»Ÿè®¡
        self.inference_times = []
        self.last_detection_time = 0
        
        print(f"âœ… é±¼ä½“å…³é”®ç‚¹æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ¨¡å‹: {model_path}")
        print(f"   è®¾å¤‡: {device}")
        print(f"   å…³é”®ç‚¹: {self.landmark_names}")
    
    def detect_landmarks(self, image: np.ndarray) -> Tuple[Optional[np.ndarray], Optional[np.ndarray], Dict[str, Any]]:
        """
        æ£€æµ‹å›¾åƒä¸­çš„é±¼ä½“å…³é”®ç‚¹
        
        Args:
            image: è¾“å…¥å›¾åƒ (BGRæ ¼å¼)
            
        Returns:
            landmarks: å…³é”®ç‚¹åæ ‡ (N, 2) æˆ– None
            visibility: å…³é”®ç‚¹å¯è§æ€§ (N,) æˆ– None
            info: æ£€æµ‹ä¿¡æ¯å­—å…¸
        """
        start_time = time.time()
        
        # è½¬æ¢é¢œè‰²æ ¼å¼
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        try:
            # é¢„æµ‹å…³é”®ç‚¹
            landmarks, visibility = self.detector.predict(image_rgb)
            
            # è¿‡æ»¤ä½ç½®ä¿¡åº¦çš„å…³é”®ç‚¹
            valid_mask = visibility > self.confidence_threshold
            valid_landmarks = landmarks[valid_mask]
            valid_visibility = visibility[valid_mask]
            
            # è®¡ç®—æ¨ç†æ—¶é—´
            inference_time = time.time() - start_time
            self.inference_times.append(inference_time)
            self.last_detection_time = inference_time
            
            # å‡†å¤‡è¿”å›ä¿¡æ¯
            info = {
                'inference_time': inference_time,
                'num_valid_landmarks': np.sum(valid_mask),
                'total_landmarks': len(landmarks),
                'confidence_scores': visibility.tolist(),
                'valid_landmarks': valid_landmarks.tolist() if len(valid_landmarks) > 0 else [],
                'detection_success': len(valid_landmarks) > 0
            }
            
            if len(valid_landmarks) > 0:
                return valid_landmarks, valid_visibility, info
            else:
                return None, None, info
                
        except Exception as e:
            inference_time = time.time() - start_time
            self.inference_times.append(inference_time)
            
            info = {
                'inference_time': inference_time,
                'error': str(e),
                'detection_success': False
            }
            
            return None, None, info
    
    def calculate_grasp_point(self, landmarks: np.ndarray, visibility: np.ndarray) -> Tuple[np.ndarray, Dict[str, Any]]:
        """
        åŸºäºå…³é”®ç‚¹è®¡ç®—ç²¾ç¡®çš„æŠ“å–ç‚¹
        
        Args:
            landmarks: å…³é”®ç‚¹åæ ‡ (N, 2)
            visibility: å…³é”®ç‚¹å¯è§æ€§ (N,)
            
        Returns:
            grasp_point: æŠ“å–ç‚¹åæ ‡ (2,)
            info: è®¡ç®—ä¿¡æ¯
        """
        if landmarks is None or len(landmarks) == 0:
            return np.array([0, 0]), {'method': 'default', 'confidence': 0.0}
        
        # æ–¹æ³•1: ä½¿ç”¨å¤´éƒ¨å’Œèº«ä½“ä¸­å¿ƒçš„ä¸­ç‚¹
        if len(landmarks) >= 2:
            # å‡è®¾å‰ä¸¤ä¸ªç‚¹æ˜¯å¤´éƒ¨å’Œèº«ä½“ä¸­å¿ƒ
            head_center = landmarks[0]
            body_center = landmarks[1]
            grasp_point = (head_center + body_center) / 2
            method = 'head_body_midpoint'
            confidence = np.mean(visibility[:2])
        
        # æ–¹æ³•2: ä½¿ç”¨æ‰€æœ‰å¯è§ç‚¹çš„ä¸­å¿ƒ
        elif len(landmarks) >= 1:
            grasp_point = np.mean(landmarks, axis=0)
            method = 'all_points_center'
            confidence = np.mean(visibility)
        
        # æ–¹æ³•3: é»˜è®¤ä½ç½®
        else:
            grasp_point = np.array([0, 0])
            method = 'default'
            confidence = 0.0
        
        info = {
            'method': method,
            'confidence': float(confidence),
            'num_landmarks_used': len(landmarks),
            'landmarks': landmarks.tolist(),
            'visibility': visibility.tolist()
        }
        
        return grasp_point, info
    
    def visualize_detection(self, image: np.ndarray, landmarks: Optional[np.ndarray], 
                           visibility: Optional[np.ndarray], grasp_point: Optional[np.ndarray] = None,
                           info: Optional[Dict] = None) -> np.ndarray:
        """
        å¯è§†åŒ–æ£€æµ‹ç»“æœ
        
        Args:
            image: åŸå§‹å›¾åƒ
            landmarks: æ£€æµ‹åˆ°çš„å…³é”®ç‚¹
            visibility: å…³é”®ç‚¹å¯è§æ€§
            grasp_point: è®¡ç®—çš„æŠ“å–ç‚¹
            info: æ£€æµ‹ä¿¡æ¯
            
        Returns:
            vis_image: å¯è§†åŒ–å›¾åƒ
        """
        vis_image = image.copy()
        
        # ç»˜åˆ¶å…³é”®ç‚¹
        if landmarks is not None and len(landmarks) > 0:
            colors = [(0, 0, 255), (0, 255, 0)]  # çº¢è‰²(å¤´éƒ¨), ç»¿è‰²(èº«ä½“)
            
            for i, (landmark, vis) in enumerate(zip(landmarks, visibility)):
                if vis > self.confidence_threshold:
                    x, y = int(landmark[0]), int(landmark[1])
                    color = colors[i % len(colors)]
                    
                    # ç»˜åˆ¶å…³é”®ç‚¹
                    cv2.circle(vis_image, (x, y), 8, color, -1)
                    cv2.circle(vis_image, (x, y), 12, (255, 255, 255), 2)
                    
                    # æ·»åŠ æ ‡ç­¾
                    label = self.landmark_names[i] if i < len(self.landmark_names) else f'Point_{i}'
                    cv2.putText(vis_image, label, (x + 15, y - 10), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        
        # ç»˜åˆ¶æŠ“å–ç‚¹
        if grasp_point is not None:
            x, y = int(grasp_point[0]), int(grasp_point[1])
            cv2.circle(vis_image, (x, y), 15, (255, 0, 255), -1)  # ç´«è‰²å¤§åœ†
            cv2.circle(vis_image, (x, y), 20, (255, 255, 255), 3)  # ç™½è‰²è¾¹æ¡†
            cv2.putText(vis_image, 'GRASP', (x + 25, y + 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)
        
        # æ·»åŠ ä¿¡æ¯æ–‡æœ¬
        if info:
            y_offset = 30
            cv2.putText(vis_image, f"FPS: {1.0/info.get('inference_time', 0.001):.1f}", 
                       (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            y_offset += 25
            
            if 'detection_success' in info:
                status = "SUCCESS" if info['detection_success'] else "FAILED"
                color = (0, 255, 0) if info['detection_success'] else (0, 0, 255)
                cv2.putText(vis_image, f"Detection: {status}", 
                           (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
                y_offset += 25
            
            if 'confidence' in info:
                cv2.putText(vis_image, f"Confidence: {info['confidence']:.2f}", 
                           (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return vis_image
    
    def get_performance_stats(self) -> Dict[str, float]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        if not self.inference_times:
            return {}
        
        times = np.array(self.inference_times)
        return {
            'avg_inference_time': float(np.mean(times)),
            'min_inference_time': float(np.min(times)),
            'max_inference_time': float(np.max(times)),
            'std_inference_time': float(np.std(times)),
            'avg_fps': float(1.0 / np.mean(times)),
            'total_inferences': len(times)
        }
    
    def reset_stats(self):
        """é‡ç½®æ€§èƒ½ç»Ÿè®¡"""
        self.inference_times = []


def test_realtime_detection(model_path: str, camera_index: int = 0):
    """æµ‹è¯•å®æ—¶æ£€æµ‹åŠŸèƒ½"""
    
    print("="*60)
    print("ğŸ¥ å®æ—¶é±¼ä½“å…³é”®ç‚¹æ£€æµ‹æµ‹è¯•")
    print("="*60)
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = RealtimeFishLandmarkDetector(model_path=model_path)
    
    # åˆå§‹åŒ–æ‘„åƒå¤´
    cap = cv2.VideoCapture(camera_index)
    if not cap.isOpened():
        print(f"âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´ {camera_index}")
        return
    
    print("âœ… æ‘„åƒå¤´åˆå§‹åŒ–æˆåŠŸ")
    print("ğŸ“‹ æ“ä½œè¯´æ˜:")
    print("   - æŒ‰ 'q' é€€å‡º")
    print("   - æŒ‰ 'r' é‡ç½®ç»Ÿè®¡")
    print("   - æŒ‰ 's' ä¿å­˜å½“å‰å¸§")
    
    frame_count = 0
    save_count = 0
    
    try:
        while True:
            ret, frame = cap.read()
            if not ret:
                print("âŒ æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
                break
            
            frame_count += 1
            
            # æ£€æµ‹å…³é”®ç‚¹
            landmarks, visibility, info = detector.detect_landmarks(frame)
            
            # è®¡ç®—æŠ“å–ç‚¹
            if landmarks is not None:
                grasp_point, grasp_info = detector.calculate_grasp_point(landmarks, visibility)
                info.update(grasp_info)
            else:
                grasp_point = None
            
            # å¯è§†åŒ–ç»“æœ
            vis_frame = detector.visualize_detection(frame, landmarks, visibility, grasp_point, info)
            
            # æ˜¾ç¤ºç»“æœ
            # æ£€æŸ¥æ˜¯å¦æ”¯æŒGUIæ˜¾ç¤º
            try:
                cv2.imshow('Fish Landmark Detection', vis_frame)
                
                # å¤„ç†æŒ‰é”®
                key = cv2.waitKey(1) & 0xFF
            except cv2.error as e:
                if "not implemented" in str(e).lower():
                    print("âš ï¸  OpenCV GUIä¸æ”¯æŒï¼Œè·³è¿‡å›¾åƒæ˜¾ç¤º")
                    key = 0  # è®¾ç½®é»˜è®¤å€¼
                else:
                    raise e
            if key == ord('q'):
                break
            elif key == ord('r'):
                detector.reset_stats()
                print("ğŸ“Š ç»Ÿè®¡å·²é‡ç½®")
            elif key == ord('s'):
                save_path = f"landmark_detection_{save_count:03d}.jpg"
                cv2.imwrite(save_path, vis_frame)
                save_count += 1
                print(f"ğŸ’¾ å›¾åƒå·²ä¿å­˜: {save_path}")
            
            # æ¯100å¸§æ˜¾ç¤ºä¸€æ¬¡ç»Ÿè®¡
            if frame_count % 100 == 0:
                stats = detector.get_performance_stats()
                if stats:
                    print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡ (å¸§ {frame_count}): "
                          f"FPS={stats['avg_fps']:.1f}, "
                          f"æ¨ç†æ—¶é—´={stats['avg_inference_time']*1000:.1f}ms")
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­")
    
    finally:
        # æ¸…ç†èµ„æº
        cap.release()
        # å®‰å…¨åœ°å…³é—­æ‰€æœ‰OpenCVçª—å£
        try:
            cv2.destroyAllWindows()
        except cv2.error as e:
            if "not implemented" in str(e).lower():
                print("âš ï¸  OpenCV GUIä¸æ”¯æŒï¼Œè·³è¿‡çª—å£æ¸…ç†")
            else:
                raise e
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        stats = detector.get_performance_stats()
        if stats:
            print("\nğŸ“ˆ æœ€ç»ˆæ€§èƒ½ç»Ÿè®¡:")
            for key, value in stats.items():
                print(f"   {key}: {value}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='å®æ—¶é±¼ä½“å…³é”®ç‚¹æ£€æµ‹')
    parser.add_argument('--model_path', type=str, required=True, help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--camera', type=int, default=0, help='æ‘„åƒå¤´ç´¢å¼•')
    parser.add_argument('--device', type=str, default='cuda', choices=['cuda', 'cpu'], help='è®¡ç®—è®¾å¤‡')
    parser.add_argument('--confidence', type=float, default=0.5, help='ç½®ä¿¡åº¦é˜ˆå€¼')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.model_path):
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model_path}")
        return
    
    test_realtime_detection(args.model_path, args.camera)


if __name__ == "__main__":
    main()



