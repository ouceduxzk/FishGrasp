#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é±¼ä½“å…³é”®ç‚¹æ£€æµ‹å™¨

ä½¿ç”¨MediaPipeæˆ–OpenPoseé£æ ¼çš„æ¨¡å‹æ¥æ£€æµ‹é±¼çš„å…³é”®ç‚¹ï¼š
- å¤´éƒ¨ä¸­å¿ƒç‚¹
- èº«ä½“ä¸­å¿ƒç‚¹  
- å°¾éƒ¨ä¸­å¿ƒç‚¹
- èƒŒé³ç‚¹
- è…¹é³ç‚¹

è¿™äº›å…³é”®ç‚¹å¯ä»¥ç”¨äºï¼š
1. è®¡ç®—é±¼çš„ç²¾ç¡®ä¸­å¿ƒä½ç½®
2. ä¼°è®¡é±¼çš„å§¿æ€å’Œæ–¹å‘
3. ç¡®å®šæœ€ä½³æŠ“å–ç‚¹
"""

import cv2
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import json
import os
from PIL import Image
import matplotlib
matplotlib.use('Agg')  # Use non-GUI backend to avoid Qt issues
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import albumentations as A
from albumentations.pytorch import ToTensorV2
from typing import List, Tuple, Dict, Optional
import argparse
from datetime import datetime
import torch.nn.functional as F


class FishLandmarkDataset(Dataset):
    """é±¼ä½“å…³é”®ç‚¹æ•°æ®é›†"""
    
    def __init__(self, image_paths: List[str], landmarks: List[np.ndarray], 
                 transform=None, image_size: Tuple[int, int] = (256, 256)):
        """
        Args:
            image_paths: å›¾åƒæ–‡ä»¶è·¯å¾„åˆ—è¡¨
            landmarks: å…³é”®ç‚¹åæ ‡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯(N, 2)çš„numpyæ•°ç»„
            transform: æ•°æ®å¢å¼ºå˜æ¢
            image_size: ç›®æ ‡å›¾åƒå°ºå¯¸
        """
        self.image_paths = image_paths
        self.landmarks = landmarks
        self.transform = transform
        self.image_size = image_size
        
        # å®šä¹‰å…³é”®ç‚¹ç±»å‹ï¼ˆåªæœ‰èº«ä½“ä¸­å¿ƒï¼‰
        self.landmark_names = [
            'body_center'       # èº«ä½“ä¸­å¿ƒ
        ]
        self.num_landmarks = len(self.landmark_names)
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # åŠ è½½å›¾åƒ
        image_path = self.image_paths[idx]
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"æ— æ³•åŠ è½½å›¾åƒ: {image_path}")
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # è·å–å…³é”®ç‚¹
        landmarks = self.landmarks[idx].copy()
        
        # è®°å½•åŸå§‹å›¾åƒå°ºå¯¸
        original_h, original_w = image.shape[:2]
        
        # æ€»æ˜¯å…ˆè¿›è¡Œå¡«å……å¤„ç†
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼ˆä¿æŒå®½é«˜æ¯”ï¼‰
        scale = min(self.image_size[0] / original_w, self.image_size[1] / original_h)
        new_w = int(original_w * scale)
        new_h = int(original_h * scale)
        
        # ç¼©æ”¾å›¾åƒ
        image = cv2.resize(image, (new_w, new_h))
        
        # ç¼©æ”¾å…³é”®ç‚¹åæ ‡
        landmarks[:, 0] *= scale
        landmarks[:, 1] *= scale
        
        # åˆ›å»ºæ­£æ–¹å½¢ç”»å¸ƒå¹¶æ”¾ç½®å›¾åƒåˆ°å·¦ä¸Šè§’
        canvas = np.zeros((self.image_size[1], self.image_size[0], 3), dtype=np.uint8)
        x_offset = 0  # æ€»æ˜¯ä»å·¦è¾¹å¼€å§‹
        y_offset = 0  # æ€»æ˜¯ä»é¡¶éƒ¨å¼€å§‹
        canvas[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = image
        image = canvas
        
        # å…³é”®ç‚¹åæ ‡ä¸éœ€è¦è°ƒæ•´åç§»é‡ï¼ˆå› ä¸ºx_offset=0, y_offset=0ï¼‰
        
        # å½’ä¸€åŒ–å…³é”®ç‚¹åæ ‡åˆ°[0, 1]
        h, w = image.shape[:2]
        landmarks_normalized = landmarks / np.array([w, h])
        
        
        # æ‰‹åŠ¨å½’ä¸€åŒ–å›¾åƒï¼ˆä¸ä½¿ç”¨Albumentationsï¼‰
        image_normalized = image.astype(np.float32) / 255.0
        mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)
        std = np.array([0.229, 0.224, 0.225], dtype=np.float32)
        image_normalized = (image_normalized - mean) / std
        
        # è½¬æ¢ä¸ºtensorï¼Œç¡®ä¿æ˜¯float32
        image_tensor = torch.from_numpy(image_normalized).permute(2, 0, 1).float()
        
        landmarks_tensor = torch.from_numpy(landmarks_normalized.astype(np.float32)).float()
        
        return {
            'image': image_tensor,
            'landmarks': landmarks_tensor,
            'image_path': image_path,
            'original_size': (original_h, original_w)
        }


class FishLandmarkModel(nn.Module):
    """é±¼ä½“å…³é”®ç‚¹æ£€æµ‹æ¨¡å‹"""
    
    def __init__(self, num_landmarks: int = 1, backbone: str = 'resnet18'):
        super(FishLandmarkModel, self).__init__()
        
        self.num_landmarks = num_landmarks
        
        # é€‰æ‹©backbone
        if backbone == 'resnet18':
            import torchvision.models as models
            self.backbone = models.resnet18(pretrained=True)
            self.backbone.fc = nn.Identity()  # ç§»é™¤æœ€åçš„åˆ†ç±»å±‚
            feature_dim = 512
        elif backbone == 'efficientnet':
            import torchvision.models as models
            self.backbone = models.efficientnet_b0(pretrained=True)
            self.backbone.classifier = nn.Identity()
            feature_dim = 1280
        else:
            raise ValueError(f"Unsupported backbone: {backbone}")
        
        # å…³é”®ç‚¹å›å½’å¤´
        self.landmark_head = nn.Sequential(
            nn.Linear(feature_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, num_landmarks * 2)  # x, yåæ ‡
        )
        
        # å¯è§æ€§é¢„æµ‹å¤´ï¼ˆå…³é”®ç‚¹æ˜¯å¦å¯è§ï¼‰
        self.visibility_head = nn.Sequential(
            nn.Linear(feature_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, num_landmarks),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        features = self.backbone(x)
        
        landmarks = self.landmark_head(features)
        landmarks = landmarks.view(-1, self.num_landmarks, 2)
        
        visibility = self.visibility_head(features)
        
        return landmarks, visibility


class GaussianKernelLoss(nn.Module):
    """é«˜æ–¯æ ¸æŸå¤±å‡½æ•° - ä¸ºå…³é”®ç‚¹å‘¨å›´çš„åŒºåŸŸæä¾›å¹³æ»‘çš„æŸå¤±"""
    
    def __init__(self, sigma: float = 0.1, radius: float = 0.2, image_size: Tuple[int, int] = (256, 256)):
        """
        Args:
            sigma: é«˜æ–¯æ ¸çš„æ ‡å‡†å·®
            radius: è€ƒè™‘æŸå¤±çš„æœ€å¤§åŠå¾„ï¼ˆå½’ä¸€åŒ–åæ ‡ï¼‰
            image_size: å›¾åƒå°ºå¯¸ (width, height)
        """
        super(GaussianKernelLoss, self).__init__()
        self.sigma = sigma
        self.radius = radius
        self.image_size = image_size
        
        # åˆ›å»ºåæ ‡ç½‘æ ¼
        self.register_buffer('coord_grid', self._create_coord_grid())
    
    def _create_coord_grid(self):
        """åˆ›å»ºåæ ‡ç½‘æ ¼"""
        h, w = self.image_size[1], self.image_size[0]
        y_coords = torch.linspace(0, 1, h).view(-1, 1).repeat(1, w)
        x_coords = torch.linspace(0, 1, w).view(1, -1).repeat(h, 1)
        coords = torch.stack([x_coords, y_coords], dim=0)  # [2, H, W]
        return coords
    
    def forward(self, pred_landmarks: torch.Tensor, target_landmarks: torch.Tensor, 
                visibility: torch.Tensor) -> torch.Tensor:
        """
        Args:
            pred_landmarks: é¢„æµ‹çš„å…³é”®ç‚¹ [B, N, 2] (å½’ä¸€åŒ–åæ ‡)
            target_landmarks: ç›®æ ‡å…³é”®ç‚¹ [B, N, 2] (å½’ä¸€åŒ–åæ ‡)
            visibility: å¯è§æ€§ [B, N]
        Returns:
            loss: é«˜æ–¯æ ¸æŸå¤±
        """
        batch_size, num_landmarks, _ = pred_landmarks.shape
        total_loss = 0.0
        
        for b in range(batch_size):
            for n in range(num_landmarks):
                if visibility[b, n] < 0.5:  # è·³è¿‡ä¸å¯è§çš„å…³é”®ç‚¹
                    continue
                
                target_point = target_landmarks[b, n]  # [2]
                pred_point = pred_landmarks[b, n]  # [2]
                
                # è®¡ç®—åˆ°ç›®æ ‡ç‚¹çš„è·ç¦»
                distances = torch.norm(self.coord_grid - target_point.view(2, 1, 1), dim=0)
                
                # åˆ›å»ºé«˜æ–¯æƒé‡å›¾
                gaussian_weights = torch.exp(-(distances ** 2) / (2 * self.sigma ** 2))
                
                # åªåœ¨åŠå¾„å†…è®¡ç®—æŸå¤±
                mask = distances <= self.radius
                gaussian_weights = gaussian_weights * mask.float()
                
                # è®¡ç®—é¢„æµ‹ç‚¹åˆ°æ‰€æœ‰ç½‘æ ¼ç‚¹çš„è·ç¦»
                pred_distances = torch.norm(self.coord_grid - pred_point.view(2, 1, 1), dim=0)
                
                # é«˜æ–¯æ ¸æŸå¤±ï¼šé¢„æµ‹è·ç¦»ä¸é«˜æ–¯æƒé‡çš„åŠ æƒå’Œ
                loss = torch.sum(gaussian_weights * pred_distances) / (torch.sum(gaussian_weights) + 1e-8)
                total_loss += loss
        
        return total_loss / (batch_size * num_landmarks + 1e-8)


class CombinedLoss(nn.Module):
    """ç»„åˆæŸå¤±å‡½æ•°ï¼šé«˜æ–¯æ ¸æŸå¤± + ä¼ ç»Ÿå›å½’æŸå¤±"""
    
    def __init__(self, gaussian_weight: float = 0.7, regression_weight: float = 0.3, 
                 visibility_weight: float = 0.01, sigma: float = 0.1, radius: float = 0.2):
        super(CombinedLoss, self).__init__()
        self.gaussian_weight = gaussian_weight
        self.regression_weight = regression_weight
        self.visibility_weight = visibility_weight
        
        self.gaussian_loss = GaussianKernelLoss(sigma=sigma, radius=radius)
        self.regression_loss = nn.SmoothL1Loss()
        self.visibility_loss = nn.BCELoss()
    
    def forward(self, pred_landmarks: torch.Tensor, target_landmarks: torch.Tensor,
                pred_visibility: torch.Tensor, target_visibility: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Args:
            pred_landmarks: é¢„æµ‹å…³é”®ç‚¹ [B, N, 2]
            target_landmarks: ç›®æ ‡å…³é”®ç‚¹ [B, N, 2]
            pred_visibility: é¢„æµ‹å¯è§æ€§ [B, N]
            target_visibility: ç›®æ ‡å¯è§æ€§ [B, N]
        Returns:
            Dict containing individual losses and total loss
        """
        # é«˜æ–¯æ ¸æŸå¤±
        gaussian_loss = self.gaussian_loss(pred_landmarks, target_landmarks, target_visibility)
        
        # ä¼ ç»Ÿå›å½’æŸå¤±ï¼ˆåªå¯¹å¯è§ç‚¹ï¼‰
        visible_mask = target_visibility > 0.5
        if visible_mask.sum() > 0:
            visible_pred = pred_landmarks[visible_mask]
            visible_target = target_landmarks[visible_mask]
            regression_loss = self.regression_loss(visible_pred, visible_target)
        else:
            regression_loss = torch.tensor(0.0, device=pred_landmarks.device)
        
        # å¯è§æ€§æŸå¤±
        visibility_loss = self.visibility_loss(pred_visibility, target_visibility)
        
        # ç»„åˆæŸå¤±
        total_loss = (self.gaussian_weight * gaussian_loss + 
                     self.regression_weight * regression_loss + 
                     self.visibility_weight * visibility_loss)
        
        return {
            'total': total_loss,
            'gaussian': gaussian_loss,
            'regression': regression_loss,
            'visibility': visibility_loss
        }


class FishLandmarkDetector:
    """é±¼ä½“å…³é”®ç‚¹æ£€æµ‹å™¨ä¸»ç±»"""
    
    def __init__(self, model_path: Optional[str] = None, device: str = 'cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.model = None
        self.landmark_names = [
            'body_center'
        ]
        
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
    
    def calculate_pixel_error(self, landmark_loss, image_size=(640, 480)):
        """
        å°†å½’ä¸€åŒ–åæ ‡çš„æŸå¤±è½¬æ¢ä¸ºåƒç´ è¯¯å·®
        
        Args:
            landmark_loss: å½’ä¸€åŒ–åæ ‡çš„æŸå¤±å€¼
            image_size: å›¾åƒå°ºå¯¸ (width, height)
            
        Returns:
            pixel_error: å¹³å‡åƒç´ è¯¯å·®
        """
        # è®¡ç®—å½’ä¸€åŒ–åæ ‡çš„RMSE
        if isinstance(landmark_loss, torch.Tensor):
            rmse_normalized = torch.sqrt(landmark_loss).item()
        else:
            rmse_normalized = np.sqrt(landmark_loss)
        
        # è½¬æ¢ä¸ºåƒç´ è¯¯å·® (å–å®½é«˜çš„å¹³å‡å€¼)
        avg_image_size = (image_size[0] + image_size[1]) / 2
        pixel_error = rmse_normalized * avg_image_size
        
        return pixel_error
    
    def visualize_padding(self, image_path: str, landmarks: np.ndarray, target_size: Tuple[int, int] = (256, 256)):
        """
        å¯è§†åŒ–å¡«å……æ•ˆæœï¼Œç”¨äºè°ƒè¯•
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            landmarks: å…³é”®ç‚¹åæ ‡
            target_size: ç›®æ ‡å°ºå¯¸
        """
        import matplotlib.pyplot as plt
        
        # åŠ è½½åŸå§‹å›¾åƒ
        image = cv2.imread(image_path)
        if image is None:
            print(f"æ— æ³•åŠ è½½å›¾åƒ: {image_path}")
            return
        
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        original_h, original_w = image.shape[:2]
        
        # è®¡ç®—å¡«å……åçš„å›¾åƒ
        scale = min(target_size[0] / original_w, target_size[1] / original_h)
        new_w = int(original_w * scale)
        new_h = int(original_h * scale)
        
        # ç¼©æ”¾å›¾åƒå’Œå…³é”®ç‚¹
        resized_image = cv2.resize(image, (new_w, new_h))
        scaled_landmarks = landmarks.copy().astype(np.float64)
        scaled_landmarks[:, 0] *= scale
        scaled_landmarks[:, 1] *= scale
        
        # åˆ›å»ºå¡«å……ç”»å¸ƒï¼ˆå›¾åƒæ”¾åœ¨å·¦ä¸Šè§’ï¼‰
        canvas = np.zeros((target_size[1], target_size[0], 3), dtype=np.uint8)
        x_offset = 0  # æ€»æ˜¯ä»å·¦è¾¹å¼€å§‹
        y_offset = 0  # æ€»æ˜¯ä»é¡¶éƒ¨å¼€å§‹
        canvas[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized_image
        
        # å…³é”®ç‚¹åæ ‡ä¸éœ€è¦è°ƒæ•´ï¼ˆå› ä¸ºx_offset=0, y_offset=0ï¼‰
        final_landmarks = scaled_landmarks.copy()
        
        # å¯è§†åŒ–
        fig, axes = plt.subplots(1, 2, figsize=(12, 6))
        
        # åŸå§‹å›¾åƒ
        axes[0].imshow(image)
        axes[0].scatter(landmarks[:, 0], landmarks[:, 1], c='red', s=50)
        axes[0].set_title(f'åŸå§‹å›¾åƒ {original_w}x{original_h}')
        axes[0].axis('off')
        
        # å¡«å……åå›¾åƒ
        axes[1].imshow(canvas)
        axes[1].scatter(final_landmarks[:, 0], final_landmarks[:, 1], c='red', s=50)
        axes[1].set_title(f'å¡«å……å {target_size[0]}x{target_size[1]} (scale={scale:.3f})')
        axes[1].axis('off')
        
        plt.tight_layout()
        plt.show()
        
        print(f"åŸå§‹å°ºå¯¸: {original_w}x{original_h}")
        print(f"ç¼©æ”¾æ¯”ä¾‹: {scale:.3f}")
        print(f"å¡«å……åç§»: x={x_offset}, y={y_offset}")
        print(f"åŸå§‹å…³é”®ç‚¹: {landmarks}")
        print(f"æœ€ç»ˆå…³é”®ç‚¹: {final_landmarks}")
    
    def create_model(self, backbone: str = 'resnet18'):
        """åˆ›å»ºæ¨¡å‹"""
        self.model = FishLandmarkModel(
            num_landmarks=len(self.landmark_names),
            backbone=backbone
        ).to(self.device)
        return self.model
    
    def train_with_gaussian_loss(self, train_loader: DataLoader, val_loader: DataLoader, 
                                epochs: int = 100, lr: float = 0.001, save_dir: str = 'models'):
        """ä½¿ç”¨é«˜æ–¯æ ¸æŸå¤±çš„è®­ç»ƒæ–¹æ³•"""
        if self.model is None:
            self.create_model()
        
        # æŸå¤±å‡½æ•° - ä½¿ç”¨ç»„åˆæŸå¤±ï¼ˆé«˜æ–¯æ ¸ + å›å½’ + å¯è§æ€§ï¼‰
        criterion = CombinedLoss(
            gaussian_weight=0.7,    # é«˜æ–¯æ ¸æŸå¤±æƒé‡
            regression_weight=0.3,  # å›å½’æŸå¤±æƒé‡
            visibility_weight=0.01, # å¯è§æ€§æŸå¤±æƒé‡
            sigma=0.1,              # é«˜æ–¯æ ¸æ ‡å‡†å·®
            radius=0.2              # é«˜æ–¯æ ¸åŠå¾„
        ).to(self.device)
        
        # ä¼˜åŒ–å™¨
        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-4)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)
        
        # è®­ç»ƒå†å²
        train_losses = []
        val_losses = []
        
        os.makedirs(save_dir, exist_ok=True)
        
        best_val_loss = float('inf')
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0.0
            train_gaussian_loss = 0.0
            train_regression_loss = 0.0
            train_visibility_loss = 0.0
            
            for batch_idx, batch in enumerate(train_loader):
                images = batch['image'].to(self.device)
                landmarks = batch['landmarks'].to(self.device)
                
                optimizer.zero_grad()
                
                pred_landmarks, pred_visibility = self.model(images)
                
                # è®¡ç®—ç»„åˆæŸå¤±ï¼ˆé«˜æ–¯æ ¸ + å›å½’ + å¯è§æ€§ï¼‰
                visibility_target = torch.ones_like(pred_visibility)
                loss_dict = criterion(pred_landmarks, landmarks, pred_visibility, visibility_target)
                total_loss = loss_dict['total']
                
                total_loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                train_loss += total_loss.item()
                train_gaussian_loss += loss_dict['gaussian'].item()
                train_regression_loss += loss_dict['regression'].item()
                train_visibility_loss += loss_dict['visibility'].item()
                
                if batch_idx % 10 == 0:
                    pixel_error = self.calculate_pixel_error(loss_dict['regression'])
                    print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, '
                          f'Loss: {total_loss.item():.4f}, '
                          f'Gaussian: {loss_dict["gaussian"].item():.4f}, '
                          f'Regression: {loss_dict["regression"].item():.4f}, '
                          f'Pixel Error: ~{pixel_error:.1f}px, '
                          f'Visibility: {loss_dict["visibility"].item():.4f}')
            
            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_loss = 0.0
            val_gaussian_loss = 0.0
            val_regression_loss = 0.0
            val_visibility_loss = 0.0
            
            with torch.no_grad():
                for batch in val_loader:
                    images = batch['image'].to(self.device)
                    landmarks = batch['landmarks'].to(self.device)
                    
                    pred_landmarks, pred_visibility = self.model(images)
                    
                    visibility_target = torch.ones_like(pred_visibility)
                    loss_dict = criterion(pred_landmarks, landmarks, pred_visibility, visibility_target)
                    
                    val_loss += loss_dict['total'].item()
                    val_gaussian_loss += loss_dict['gaussian'].item()
                    val_regression_loss += loss_dict['regression'].item()
                    val_visibility_loss += loss_dict['visibility'].item()
            
            # è®¡ç®—å¹³å‡æŸå¤±
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            avg_train_gaussian = train_gaussian_loss / len(train_loader)
            avg_val_gaussian = val_gaussian_loss / len(val_loader)
            avg_train_regression = train_regression_loss / len(train_loader)
            avg_val_regression = val_regression_loss / len(val_loader)
            avg_train_visibility = train_visibility_loss / len(train_loader)
            avg_val_visibility = val_visibility_loss / len(val_loader)
            
            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)
            
            # è®¡ç®—åƒç´ è¯¯å·®
            train_pixel_error = self.calculate_pixel_error(avg_train_regression)
            val_pixel_error = self.calculate_pixel_error(avg_val_regression)
            
            print(f'Epoch {epoch+1}/{epochs}:')
            print(f'  Train - Total: {avg_train_loss:.4f}, '
                  f'Gaussian: {avg_train_gaussian:.4f}, '
                  f'Regression: {avg_train_regression:.4f}, '
                  f'Pixel Error: ~{train_pixel_error:.1f}px, '
                  f'Visibility: {avg_train_visibility:.4f}')
            print(f'  Val   - Total: {avg_val_loss:.4f}, '
                  f'Gaussian: {avg_val_gaussian:.4f}, '
                  f'Regression: {avg_val_regression:.4f}, '
                  f'Pixel Error: ~{val_pixel_error:.1f}px, '
                  f'Visibility: {avg_val_visibility:.4f}')
            print(f'  LR: {optimizer.param_groups[0]["lr"]:.6f}')
            print('-' * 60)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(avg_val_loss)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                model_path = os.path.join(save_dir, 'best_fish_landmark_model_gaussian.pth')
                torch.save({
                    'model_state_dict': self.model.state_dict(),
                    'epoch': epoch,
                    'val_loss': avg_val_loss,
                    'train_loss': avg_train_loss,
                    'gaussian_weight': 0.7,
                    'regression_weight': 0.3,
                    'visibility_weight': 0.01,
                    'sigma': 0.1,
                    'radius': 0.2
                }, model_path)
                print(f'âœ… ä¿å­˜æœ€ä½³æ¨¡å‹: {model_path}')
        
        # ä¿å­˜è®­ç»ƒæ›²çº¿
        self.plot_training_curves(train_losses, val_losses, save_dir)
        
        return train_losses, val_losses
    
    def train(self, train_loader: DataLoader, val_loader: DataLoader, 
              epochs: int = 100, lr: float = 0.001, save_dir: str = 'models'):
        """è®­ç»ƒæ¨¡å‹"""
        if self.model is None:
            self.create_model()
        
        # æŸå¤±å‡½æ•° - ä½¿ç”¨ç»„åˆæŸå¤±ï¼ˆé«˜æ–¯æ ¸ + å›å½’ + å¯è§æ€§ï¼‰
        criterion = CombinedLoss(
            gaussian_weight=0.7,    # é«˜æ–¯æ ¸æŸå¤±æƒé‡
            regression_weight=0.3,  # å›å½’æŸå¤±æƒé‡
            visibility_weight=0.01, # å¯è§æ€§æŸå¤±æƒé‡
            sigma=0.1,              # é«˜æ–¯æ ¸æ ‡å‡†å·®
            radius=0.2              # é«˜æ–¯æ ¸åŠå¾„
        ).to(self.device)
        
        # ä¼˜åŒ–å™¨
        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-4)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)
        
        # è®­ç»ƒå†å²
        train_losses = []
        val_losses = []
        
        os.makedirs(save_dir, exist_ok=True)
        
        best_val_loss = float('inf')
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0.0
            train_landmark_loss = 0.0
            train_visibility_loss = 0.0
            
            for batch_idx, batch in enumerate(train_loader):
                images = batch['image'].to(self.device)
                landmarks = batch['landmarks'].to(self.device)
                
                optimizer.zero_grad()
                
                pred_landmarks, pred_visibility = self.model(images)
                
                # è®¡ç®—å…³é”®ç‚¹æŸå¤±
                landmark_loss = landmark_criterion(pred_landmarks, landmarks)
                
                # è®¡ç®—å¯è§æ€§æŸå¤±ï¼ˆå‡è®¾æ‰€æœ‰å…³é”®ç‚¹éƒ½å¯è§ï¼‰
                visibility_target = torch.ones_like(pred_visibility)
                visibility_loss = visibility_criterion(pred_visibility, visibility_target)
                
                # æ€»æŸå¤± - é™ä½å¯è§æ€§æŸå¤±æƒé‡ï¼Œå› ä¸ºæ‰€æœ‰å…³é”®ç‚¹éƒ½å¯è§
                total_loss = landmark_loss + 0.01 * visibility_loss
                
                total_loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                train_loss += total_loss.item()
                train_landmark_loss += landmark_loss.item()
                train_visibility_loss += visibility_loss.item()
                
                # æ‰“å°æ‰¹æ¬¡ä¿¡æ¯
                if batch_idx % 10 == 0:
                    pixel_error = self.calculate_pixel_error(landmark_loss)
                    print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, '
                          f'Loss: {total_loss.item():.4f}, Landmark: {landmark_loss.item():.4f}, '
                          f'Pixel Error: ~{pixel_error:.1f}px, Visibility: {visibility_loss.item():.4f}')
            
            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_loss = 0.0
            val_landmark_loss = 0.0
            val_visibility_loss = 0.0
            
            with torch.no_grad():
                for batch in val_loader:
                    images = batch['image'].to(self.device)
                    landmarks = batch['landmarks'].to(self.device)
                    
                    pred_landmarks, pred_visibility = self.model(images)
                    
                    landmark_loss = landmark_criterion(pred_landmarks, landmarks)
                    visibility_target = torch.ones_like(pred_visibility)
                    visibility_loss = visibility_criterion(pred_visibility, visibility_target)
                    
                    total_loss = landmark_loss + 0.1 * visibility_loss
                    
                    val_loss += total_loss.item()
                    val_landmark_loss += landmark_loss.item()
                    val_visibility_loss += visibility_loss.item()
            
            # è®¡ç®—å¹³å‡æŸå¤±
            train_loss /= len(train_loader)
            train_landmark_loss /= len(train_loader)
            train_visibility_loss /= len(train_loader)
            val_loss /= len(val_loader)
            val_landmark_loss /= len(val_loader)
            val_visibility_loss /= len(val_loader)
            
            train_losses.append(train_loss)
            val_losses.append(val_loss)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(val_loss)
            current_lr = optimizer.param_groups[0]['lr']
            
            train_pixel_error = self.calculate_pixel_error(torch.tensor(train_landmark_loss))
            val_pixel_error = self.calculate_pixel_error(torch.tensor(val_landmark_loss))
            print(f'Epoch {epoch+1}/{epochs}:')
            print(f'  Train - Total: {train_loss:.4f}, Landmark: {train_landmark_loss:.4f}, Pixel Error: ~{train_pixel_error:.1f}px, Visibility: {train_visibility_loss:.4f}')
            print(f'  Val   - Total: {val_loss:.4f}, Landmark: {val_landmark_loss:.4f}, Pixel Error: ~{val_pixel_error:.1f}px, Visibility: {val_visibility_loss:.4f}')
            print(f'  LR: {current_lr:.6f}')
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                model_path = os.path.join(save_dir, 'best_fish_landmark_model.pth')
                torch.save({
                    'model_state_dict': self.model.state_dict(),
                    'landmark_names': self.landmark_names,
                    'epoch': epoch,
                    'val_loss': val_loss,
                    'train_loss': train_loss,
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict()
                }, model_path)
                print(f'  âœ… ä¿å­˜æœ€ä½³æ¨¡å‹: {model_path} (Val Loss: {val_loss:.4f})')
            
            # æ—©åœæ£€æŸ¥
            if epoch > 20 and val_loss > best_val_loss * 1.1:
                print(f'  âš ï¸  éªŒè¯æŸå¤±ä¸Šå‡ï¼Œè€ƒè™‘æ—©åœ')
            
            print('-' * 60)
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves(train_losses, val_losses, save_dir)
        
        return train_losses, val_losses
    
    def train_without_validation_gaussian(self, train_loader: DataLoader, epochs: int = 100, 
                                         lr: float = 0.001, save_dir: str = 'models'):
        """æ— éªŒè¯é›†çš„è®­ç»ƒæ–¹æ³•ï¼ˆä½¿ç”¨é«˜æ–¯æ ¸æŸå¤±ï¼‰"""
        if self.model is None:
            self.create_model()
        
        # æŸå¤±å‡½æ•° - ä½¿ç”¨ç»„åˆæŸå¤±ï¼ˆé«˜æ–¯æ ¸ + å›å½’ + å¯è§æ€§ï¼‰
        criterion = CombinedLoss(
            gaussian_weight=0.7,    # é«˜æ–¯æ ¸æŸå¤±æƒé‡
            regression_weight=0.3,  # å›å½’æŸå¤±æƒé‡
            visibility_weight=0.01, # å¯è§æ€§æŸå¤±æƒé‡
            sigma=0.1,              # é«˜æ–¯æ ¸æ ‡å‡†å·®
            radius=0.2              # é«˜æ–¯æ ¸åŠå¾„
        ).to(self.device)
        
        # ä¼˜åŒ–å™¨
        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-4)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10)
        
        # è®­ç»ƒå†å²
        train_losses = []
        
        os.makedirs(save_dir, exist_ok=True)
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0.0
            train_gaussian_loss = 0.0
            train_regression_loss = 0.0
            train_visibility_loss = 0.0
            
            for batch_idx, batch in enumerate(train_loader):
                images = batch['image'].to(self.device)
                landmarks = batch['landmarks'].to(self.device)
                
                optimizer.zero_grad()
                
                pred_landmarks, pred_visibility = self.model(images)
                
                # è®¡ç®—ç»„åˆæŸå¤±ï¼ˆé«˜æ–¯æ ¸ + å›å½’ + å¯è§æ€§ï¼‰
                visibility_target = torch.ones_like(pred_visibility)
                loss_dict = criterion(pred_landmarks, landmarks, pred_visibility, visibility_target)
                total_loss = loss_dict['total']
                
                total_loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                train_loss += total_loss.item()
                train_gaussian_loss += loss_dict['gaussian'].item()
                train_regression_loss += loss_dict['regression'].item()
                train_visibility_loss += loss_dict['visibility'].item()
                
                if batch_idx % 10 == 0:
                    pixel_error = self.calculate_pixel_error(loss_dict['regression'])
                    print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, '
                          f'Loss: {total_loss.item():.4f}, '
                          f'Gaussian: {loss_dict["gaussian"].item():.4f}, '
                          f'Regression: {loss_dict["regression"].item():.4f}, '
                          f'Pixel Error: ~{pixel_error:.1f}px, '
                          f'Visibility: {loss_dict["visibility"].item():.4f}')
            
            # è®¡ç®—å¹³å‡æŸå¤±
            avg_train_loss = train_loss / len(train_loader)
            avg_train_gaussian = train_gaussian_loss / len(train_loader)
            avg_train_regression = train_regression_loss / len(train_loader)
            avg_train_visibility = train_visibility_loss / len(train_loader)
            
            train_losses.append(avg_train_loss)
            
            # è®¡ç®—åƒç´ è¯¯å·®
            train_pixel_error = self.calculate_pixel_error(avg_train_regression)
            
            print(f'Epoch {epoch+1}/{epochs}:')
            print(f'  Train - Total: {avg_train_loss:.4f}, '
                  f'Gaussian: {avg_train_gaussian:.4f}, '
                  f'Regression: {avg_train_regression:.4f}, '
                  f'Pixel Error: ~{train_pixel_error:.1f}px, '
                  f'Visibility: {avg_train_visibility:.4f}')
            print(f'  LR: {optimizer.param_groups[0]["lr"]:.6f}')
            print('-' * 60)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(avg_train_loss)
            
            # ä¿å­˜æ¨¡å‹
            if (epoch + 1) % 10 == 0:
                model_path = os.path.join(save_dir, f'fish_landmark_model_gaussian_epoch_{epoch+1}.pth')
                torch.save({
                    'model_state_dict': self.model.state_dict(),
                    'epoch': epoch,
                    'train_loss': avg_train_loss,
                    'gaussian_weight': 0.7,
                    'regression_weight': 0.3,
                    'visibility_weight': 0.01,
                    'sigma': 0.1,
                    'radius': 0.2
                }, model_path)
                print(f'âœ… ä¿å­˜æ¨¡å‹: {model_path}')
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = os.path.join(save_dir, 'final_fish_landmark_model_gaussian.pth')
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'epoch': epochs,
            'train_loss': avg_train_loss,
            'gaussian_weight': 0.7,
            'regression_weight': 0.3,
            'visibility_weight': 0.01,
            'sigma': 0.1,
            'radius': 0.2
        }, final_model_path)
        print(f'âœ… ä¿å­˜æœ€ç»ˆæ¨¡å‹: {final_model_path}')
        
        return train_losses, []
    
    def train_without_validation(self, train_loader: DataLoader, epochs: int = 100, 
                                lr: float = 0.001, save_dir: str = 'models'):
        """æ— éªŒè¯é›†çš„è®­ç»ƒæ–¹æ³•"""
        if self.model is None:
            self.create_model()
        
        # æŸå¤±å‡½æ•° - ä½¿ç”¨Smooth L1 Loss (Huber Loss) å¯¹å…³é”®ç‚¹æ›´ç¨³å®š
        landmark_criterion = nn.SmoothL1Loss()
        visibility_criterion = nn.BCELoss()
        
        # ä¼˜åŒ–å™¨
        optimizer = optim.Adam(self.model.parameters(), lr=lr, weight_decay=1e-4)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
        
        # è®­ç»ƒå†å²
        train_losses = []
        
        os.makedirs(save_dir, exist_ok=True)
        
        print("å¼€å§‹è®­ç»ƒï¼ˆæ— éªŒè¯é›†ï¼‰...")
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0.0
            train_landmark_loss = 0.0
            train_visibility_loss = 0.0
            
            for batch_idx, batch in enumerate(train_loader):
                images = batch['image'].to(self.device)
                landmarks = batch['landmarks'].to(self.device)
                
                optimizer.zero_grad()
                
                pred_landmarks, pred_visibility = self.model(images)
                
                # è®¡ç®—å…³é”®ç‚¹æŸå¤±
                landmark_loss = landmark_criterion(pred_landmarks, landmarks)
                
                # è®¡ç®—å¯è§æ€§æŸå¤±ï¼ˆå‡è®¾æ‰€æœ‰å…³é”®ç‚¹éƒ½å¯è§ï¼‰
                visibility_target = torch.ones_like(pred_visibility)
                visibility_loss = visibility_criterion(pred_visibility, visibility_target)
                
                # æ€»æŸå¤± - é™ä½å¯è§æ€§æŸå¤±æƒé‡ï¼Œå› ä¸ºæ‰€æœ‰å…³é”®ç‚¹éƒ½å¯è§
                total_loss = landmark_loss + 0.01 * visibility_loss
                
                total_loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                train_loss += total_loss.item()
                train_landmark_loss += landmark_loss.item()
                train_visibility_loss += visibility_loss.item()
                
                # æ‰“å°æ‰¹æ¬¡ä¿¡æ¯
                if batch_idx % 10 == 0:
                    pixel_error = self.calculate_pixel_error(landmark_loss)
                    print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, '
                          f'Loss: {total_loss.item():.4f}, Landmark: {landmark_loss.item():.4f}, '
                          f'Pixel Error: ~{pixel_error:.1f}px, Visibility: {visibility_loss.item():.4f}')
            
            # è®¡ç®—å¹³å‡æŸå¤±
            train_loss /= len(train_loader)
            train_landmark_loss /= len(train_loader)
            train_visibility_loss /= len(train_loader)
            
            train_losses.append(train_loss)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']
            
            print(f'Epoch {epoch+1}/{epochs}:')
            print(f'  Train - Total: {train_loss:.4f}, Landmark: {train_landmark_loss:.4f}, Visibility: {train_visibility_loss:.4f}')
            print(f'  LR: {current_lr:.6f}')
            
            # æ¯10ä¸ªepochä¿å­˜ä¸€æ¬¡æ¨¡å‹
            if (epoch + 1) % 10 == 0:
                model_path = os.path.join(save_dir, f'fish_landmark_model_epoch_{epoch+1}.pth')
                torch.save({
                    'model_state_dict': self.model.state_dict(),
                    'landmark_names': self.landmark_names,
                    'epoch': epoch,
                    'train_loss': train_loss,
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict()
                }, model_path)
                print(f'  ğŸ’¾ ä¿å­˜æ¨¡å‹: {model_path}')
            
            print('-' * 60)
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = os.path.join(save_dir, 'final_fish_landmark_model.pth')
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'landmark_names': self.landmark_names,
            'epoch': epochs,
            'train_loss': train_loss,
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict()
        }, final_model_path)
        print(f'âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}')
        
        return train_losses, []
    
    def load_model(self, model_path: str):
        """åŠ è½½æ¨¡å‹"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        print(f"ğŸ“ åŠ è½½æ¨¡å‹: {model_path}")
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # æ£€æŸ¥checkpointæ ¼å¼
        if 'model_state_dict' not in checkpoint:
            raise ValueError(f"æ¨¡å‹æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œç¼ºå°‘ 'model_state_dict' é”®")
        
        # è·å–å…³é”®ç‚¹æ•°é‡
        if 'landmark_names' in checkpoint:
            num_landmarks = len(checkpoint['landmark_names'])
        else:
            # ä»state_dictæ¨æ–­å…³é”®ç‚¹æ•°é‡
            num_landmarks = 1  # é»˜è®¤å€¼
            print("âš ï¸  è­¦å‘Š: æ— æ³•ä»checkpointè·å–å…³é”®ç‚¹æ•°é‡ï¼Œä½¿ç”¨é»˜è®¤å€¼1")
        
        # æ£€æµ‹backboneç±»å‹
        backbone = 'resnet18'  # é»˜è®¤å€¼
        if 'backbone' in checkpoint:
            backbone = checkpoint['backbone']
            print(f"ğŸ“‹ ä»checkpointæ£€æµ‹åˆ°backbone: {backbone}")
        else:
            # ä»state_dictæ¨æ–­backboneç±»å‹
            state_dict = checkpoint['model_state_dict']
            if any(key.startswith('backbone.features.') for key in state_dict.keys()):
                backbone = 'efficientnet'
                print("ğŸ“‹ ä»state_dictæ£€æµ‹åˆ°backbone: efficientnet")
            elif any(key.startswith('backbone.conv1.') for key in state_dict.keys()):
                backbone = 'resnet18'
                print("ğŸ“‹ ä»state_dictæ£€æµ‹åˆ°backbone: resnet18")
            else:
                print("âš ï¸  è­¦å‘Š: æ— æ³•æ£€æµ‹backboneç±»å‹ï¼Œä½¿ç”¨é»˜è®¤å€¼resnet18")
        
        # åˆ›å»ºæ¨¡å‹ï¼ˆä½¿ç”¨æ£€æµ‹åˆ°çš„backboneå’Œå½“å‰é…ç½®çš„landmarkæ•°é‡ï¼‰
        current_num_landmarks = len(self.landmark_names)
        self.model = FishLandmarkModel(
            num_landmarks=current_num_landmarks,
            backbone=backbone
        ).to(self.device)
        
        # å¦‚æœä¿å­˜çš„æ¨¡å‹æœ‰ä¸åŒæ•°é‡çš„landmarksï¼Œéœ€è¦é€‚é…
        if num_landmarks != current_num_landmarks:
            print(f"âš ï¸  æ¨¡å‹é€‚é…: ä» {num_landmarks} ä¸ªå…³é”®ç‚¹é€‚é…åˆ° {current_num_landmarks} ä¸ªå…³é”®ç‚¹")
            # åŠ è½½å…¼å®¹çš„æƒé‡
            state_dict = checkpoint['model_state_dict']
            model_state_dict = self.model.state_dict()
            
            # åªåŠ è½½å…¼å®¹çš„æƒé‡
            compatible_state_dict = {}
            for key, value in state_dict.items():
                if key in model_state_dict and model_state_dict[key].shape == value.shape:
                    compatible_state_dict[key] = value
                else:
                    print(f"è·³è¿‡ä¸å…¼å®¹çš„æƒé‡: {key} (å½¢çŠ¶ä¸åŒ¹é…)")
            
            self.model.load_state_dict(compatible_state_dict, strict=False)
        else:
            self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        
        # è®¾ç½®landmark_names
        if 'landmark_names' in checkpoint:
            self.landmark_names = checkpoint['landmark_names']
        else:
            self.landmark_names = ['body_center']
            print("âš ï¸  è­¦å‘Š: ä½¿ç”¨é»˜è®¤å…³é”®ç‚¹åç§°")
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå…³é”®ç‚¹æ•°é‡: {num_landmarks}")
    
    def predict(self, image: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """é¢„æµ‹å…³é”®ç‚¹"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ¨¡å‹")
        
        original_h, original_w = image.shape[:2]
        target_size = (256, 256)
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹å’Œå¡«å……åç§»ï¼ˆä¸preprocess_imageä¿æŒä¸€è‡´ï¼‰
        scale = min(target_size[0] / original_w, target_size[1] / original_h)
        new_w = int(original_w * scale)
        new_h = int(original_h * scale)
        x_offset = 0  # æ€»æ˜¯ä»å·¦è¾¹å¼€å§‹
        y_offset = 0  # æ€»æ˜¯ä»é¡¶éƒ¨å¼€å§‹
        
        # é¢„å¤„ç†å›¾åƒ
        image_tensor = self.preprocess_image(image)
        
        with torch.no_grad():
            pred_landmarks, pred_visibility = self.model(image_tensor)
            
            # æ¨¡å‹è¾“å‡ºçš„æ˜¯å½’ä¸€åŒ–åæ ‡ [0,1]ï¼Œéœ€è¦è½¬æ¢å›åŸå§‹å›¾åƒåæ ‡
            landmarks_normalized = pred_landmarks[0].cpu().numpy()  # [0,1] èŒƒå›´
            
            # è½¬æ¢åˆ°å¡«å……åçš„256x256åæ ‡
            landmarks_padded = landmarks_normalized * np.array([target_size[0], target_size[1]])
            
            # ç”±äºå›¾åƒæ”¾åœ¨å·¦ä¸Šè§’ï¼Œä¸éœ€è¦å‡å»åç§»é‡
            # landmarks_unpadded = landmarks_padded - np.array([x_offset, y_offset])  # x_offset=0, y_offset=0
            
            # ç¼©æ”¾åˆ°åŸå§‹å›¾åƒå°ºå¯¸
            landmarks_pixel = landmarks_padded / scale
            
            visibility = pred_visibility[0].cpu().numpy()
        
        return landmarks_pixel, visibility
    
    def preprocess_image(self, image: np.ndarray) -> torch.Tensor:
        """é¢„å¤„ç†å›¾åƒ - ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼ˆä¿æŒå®½é«˜æ¯”å¹¶å¡«å……ï¼‰"""
        original_h, original_w = image.shape[:2]
        target_size = (256, 256)
        
        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼ˆä¿æŒå®½é«˜æ¯”ï¼‰
        scale = min(target_size[0] / original_w, target_size[1] / original_h)
        new_w = int(original_w * scale)
        new_h = int(original_h * scale)
        
        # ç¼©æ”¾å›¾åƒ
        image_resized = cv2.resize(image, (new_w, new_h))
        
        # åˆ›å»ºæ­£æ–¹å½¢ç”»å¸ƒå¹¶æ”¾ç½®å›¾åƒåˆ°å·¦ä¸Šè§’ï¼ˆä¸å±…ä¸­ï¼‰
        canvas = np.zeros((target_size[1], target_size[0], 3), dtype=np.uint8)
        x_offset = 0  # æ€»æ˜¯ä»å·¦è¾¹å¼€å§‹
        y_offset = 0  # æ€»æ˜¯ä»é¡¶éƒ¨å¼€å§‹
        canvas[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = image_resized
        
        # å½’ä¸€åŒ–ï¼ˆä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„ImageNetæ ‡å‡†åŒ–ï¼‰
        image_normalized = canvas.astype(np.float32) / 255.0
        mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)
        std = np.array([0.229, 0.224, 0.225], dtype=np.float32)
        image_normalized = (image_normalized - mean) / std
        
        # è½¬æ¢ä¸ºtensor
        image_tensor = torch.from_numpy(image_normalized).permute(2, 0, 1).unsqueeze(0)
        
        return image_tensor.to(self.device)
    
    def visualize_landmarks(self, image: np.ndarray, landmarks: np.ndarray, 
                           visibility: np.ndarray, save_path: Optional[str] = None):
        """å¯è§†åŒ–å…³é”®ç‚¹"""
        vis_image = image.copy()
        
        # å®šä¹‰é¢œè‰²ï¼ˆåªæœ‰èº«ä½“ä¸­å¿ƒï¼‰
        colors = [
            (0, 255, 0)     # èº«ä½“ä¸­å¿ƒ - ç»¿è‰²
        ]
        
        for i, (landmark, vis) in enumerate(zip(landmarks, visibility)):
            if vis > 0.5:  # åªæ˜¾ç¤ºå¯è§çš„å…³é”®ç‚¹
                x, y = int(landmark[0]), int(landmark[1])
                color = colors[i % len(colors)]
                
                # ç»˜åˆ¶å…³é”®ç‚¹
                cv2.circle(vis_image, (x, y), 5, color, -1)
                cv2.circle(vis_image, (x, y), 8, (255, 255, 255), 2)
                
                # æ·»åŠ æ ‡ç­¾
                cv2.putText(vis_image, self.landmark_names[i], 
                           (x + 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 
                           0.5, color, 1)
        
        if save_path:
            cv2.imwrite(save_path, cv2.cvtColor(vis_image, cv2.COLOR_RGB2BGR))
        
        return vis_image
    
    def calculate_fish_center(self, landmarks: np.ndarray, visibility: np.ndarray) -> np.ndarray:
        """è®¡ç®—é±¼çš„ç²¾ç¡®ä¸­å¿ƒä½ç½®"""
        # ç°åœ¨åªæœ‰èº«ä½“ä¸­å¿ƒï¼Œç›´æ¥ä½¿ç”¨å®ƒ
        if len(landmarks) > 0 and visibility[0] > 0.5:
            fish_center = landmarks[0]  # èº«ä½“ä¸­å¿ƒ
        else:
            # å¦‚æœèº«ä½“ä¸­å¿ƒä¸å¯è§ï¼Œä½¿ç”¨æ‰€æœ‰å¯è§ç‚¹
            valid_landmarks = [landmarks[i] for i in range(len(landmarks)) if visibility[i] > 0.5]
            if len(valid_landmarks) > 0:
                fish_center = np.mean(valid_landmarks, axis=0)
            else:
                fish_center = np.array([0, 0])  # é»˜è®¤å€¼
        
        return fish_center
    
    def plot_training_curves(self, train_losses: List[float], val_losses: List[float], save_dir: str):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        plt.figure(figsize=(10, 6))
        plt.plot(train_losses, label='Training Loss')
        plt.plot(val_losses, label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Fish Landmark Detection Training Curves')
        plt.legend()
        plt.grid(True)
        
        save_path = os.path.join(save_dir, 'training_curves.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        print(f'è®­ç»ƒæ›²çº¿å·²ä¿å­˜: {save_path}')


def create_data_transforms(image_size: Tuple[int, int] = (256, 256)):
    """åˆ›å»ºæ•°æ®å¢å¼ºå˜æ¢ - ç®€åŒ–ç‰ˆæœ¬ï¼Œåªåšå½’ä¸€åŒ–"""
    
    # ç®€åŒ–ç‰ˆæœ¬ï¼šä¸ä½¿ç”¨å¤æ‚çš„å˜æ¢ï¼Œè®©æ•°æ®é›†æ‰‹åŠ¨å¤„ç†å¡«å……
    train_transform = A.Compose([
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ], additional_targets={'image': 'image'})
    
    val_transform = A.Compose([
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ], additional_targets={'image': 'image'})
    
    return train_transform, val_transform


def main():
    parser = argparse.ArgumentParser(description='é±¼ä½“å…³é”®ç‚¹æ£€æµ‹è®­ç»ƒ')
    parser.add_argument('--data_dir', type=str, required=True, help='æ•°æ®ç›®å½•')
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=16, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=0.001, help='å­¦ä¹ ç‡')
    parser.add_argument('--backbone', type=str, default='resnet18', choices=['resnet18', 'efficientnet'])
    parser.add_argument('--save_dir', type=str, default='models', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    
    args = parser.parse_args()
    
    # è¿™é‡Œéœ€è¦å®ç°æ•°æ®åŠ è½½é€»è¾‘
    # å‡è®¾æ•°æ®æ ¼å¼ä¸ºï¼šimages/ ç›®å½•åŒ…å«å›¾åƒï¼Œannotations.json åŒ…å«å…³é”®ç‚¹æ ‡æ³¨
    print("è¯·å®ç°æ•°æ®åŠ è½½é€»è¾‘...")
    print("æ•°æ®æ ¼å¼ç¤ºä¾‹ï¼š")
    print("- images/ ç›®å½•ï¼šåŒ…å«æ‰€æœ‰é±¼ä½“å›¾åƒ")
    print("- annotations.jsonï¼šåŒ…å«å…³é”®ç‚¹æ ‡æ³¨")
    print("æ ‡æ³¨æ ¼å¼ï¼š")
    print('{"image_name.jpg": {"landmarks": [[x1,y1], [x2,y2], ...], "visibility": [1,1,0,1,...]}}')


if __name__ == "__main__":
    main()
