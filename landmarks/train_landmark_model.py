#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é±¼ä½“å…³é”®ç‚¹æ£€æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬

ä½¿ç”¨ç¤ºä¾‹:
    # è®­ç»ƒæ¨¡å¼
    python train_landmark_model.py --mode train --data_dir ./data --annotations train_annotations.json --epochs 100
    
    # æµ‹è¯•æ¨¡å¼
    python train_landmark_model.py --mode test --model_path ./models/best_model.pth --test_data_dir ./test_data --test_annotations test_annotations.json
    
    # æŸ¥çœ‹å¸®åŠ©
    python train_landmark_model.py --help

åŠŸèƒ½ç‰¹æ€§:
    - æ”¯æŒè®­ç»ƒå’Œæµ‹è¯•ä¸¤ç§æ¨¡å¼
    - æ”¯æŒå¤šç§æ¨¡å‹æ¶æ„ (ResNet18, EfficientNet)
    - è‡ªåŠ¨æ•°æ®åˆ†å‰² (è®­ç»ƒ/éªŒè¯/æµ‹è¯•)
    - æ”¯æŒJSONå’ŒTXTæ ¼å¼çš„æ ‡æ³¨æ–‡ä»¶
    - è‡ªåŠ¨ä¿å­˜è®­ç»ƒé…ç½®å’Œæ¨¡å‹æ£€æŸ¥ç‚¹
    - æä¾›è¯¦ç»†çš„è®­ç»ƒå’Œæµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
"""

import os
import sys
import argparse
import json
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, random_split
from sklearn.model_selection import train_test_split
import matplotlib
matplotlib.use('Agg')  # Use non-GUI backend to avoid Qt issues
import matplotlib.pyplot as plt
from datetime import datetime
import cv2
# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from fish_landmark_detector import FishLandmarkDetector, FishLandmarkDataset, create_data_transforms
from data_loader import FishLandmarkDataLoader


def train_model(data_dir: str, annotations_file: str, epochs: int = 100, 
                batch_size: int = 16, lr: float = 0.001, backbone: str = 'resnet18',
                save_dir: str = 'models', test_split: float = 0.2, val_split: float = 0.2,
                same_folder_mode: bool = False, sharpness: float = 1.0, 
                loss_type: str = 'ellipsoid'):
    """è®­ç»ƒé±¼ä½“å…³é”®ç‚¹æ£€æµ‹æ¨¡å‹"""
    
    print("="*60)
    print("ğŸŸ é±¼ä½“å…³é”®ç‚¹æ£€æµ‹æ¨¡å‹è®­ç»ƒ")
    print("="*60)
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    
    # åŠ è½½æ•°æ®
    print("ğŸ“ åŠ è½½æ•°æ®...")
    if same_folder_mode:
        print("ğŸ“‚ ä½¿ç”¨åŒæ–‡ä»¶å¤¹æ¨¡å¼ï¼šå›¾åƒå’ŒJSONæ–‡ä»¶åœ¨åŒä¸€ç›®å½•")
    loader = FishLandmarkDataLoader(data_dir, same_folder_mode=same_folder_mode)
    
    # æ ¹æ®æ–‡ä»¶æ‰©å±•åç¡®å®šæ ¼å¼
    if annotations_file.endswith('.json'):
        image_paths, landmarks_list = loader.load_from_json(annotations_file)
    elif annotations_file.endswith('.txt'):
        image_paths, landmarks_list = loader.load_from_txt(annotations_file)
    else:
        raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä½¿ç”¨.jsonæˆ–.txtæ–‡ä»¶")
    
    # éªŒè¯æ•°æ®
    image_paths, landmarks_list = loader.validate_data(image_paths, landmarks_list)
    
    if len(image_paths) == 0:
        raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®ï¼")
    
    # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡
    stats = loader.get_statistics(landmarks_list)
    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"  æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
    print(f"  å…³é”®ç‚¹èŒƒå›´: X[{stats['x_range'][0]:.1f}, {stats['x_range'][1]:.1f}], Y[{stats['y_range'][0]:.1f}, {stats['y_range'][1]:.1f}]")
    print(f"  å…³é”®ç‚¹å‡å€¼: X={stats['x_mean']:.1f}, Y={stats['y_mean']:.1f}")
    
    # æ•°æ®åˆ†å‰²
    print("ğŸ”„ åˆ†å‰²æ•°æ®é›†...")
    
    # é¦–å…ˆåˆ†å‰²å‡ºæµ‹è¯•é›†
    if test_split > 0:
        train_val_paths, test_paths, train_val_landmarks, test_landmarks = train_test_split(
            image_paths, landmarks_list, test_size=test_split, random_state=42, stratify=None
        )
    else:
        train_val_paths, train_val_landmarks = image_paths, landmarks_list
        test_paths, test_landmarks = [], []
    
    # ç„¶åä»è®­ç»ƒ+éªŒè¯é›†ä¸­åˆ†å‰²å‡ºéªŒè¯é›†
    if val_split > 0 and len(train_val_paths) > 0:
        train_paths, val_paths, train_landmarks, val_landmarks = train_test_split(
            train_val_paths, train_val_landmarks, test_size=val_split, random_state=42, stratify=None
        )
    else:
        train_paths, train_landmarks = train_val_paths, train_val_landmarks
        val_paths, val_landmarks = [], []
    
    print(f"  è®­ç»ƒé›†: {len(train_paths)} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {len(val_paths)} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {len(test_paths)} æ ·æœ¬")
    
    # åˆ›å»ºæ•°æ®å˜æ¢
    train_transform, val_transform = create_data_transforms()
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = FishLandmarkDataset(train_paths, train_landmarks, train_transform)  # ä½¿ç”¨æ‰€æœ‰å…³é”®ç‚¹
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
    
    # å¤„ç†éªŒè¯é›†
    if len(val_paths) > 0:
        val_dataset = FishLandmarkDataset(val_paths, val_landmarks, val_transform)  # ä½¿ç”¨æ‰€æœ‰å…³é”®ç‚¹
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)
    else:
        print("âš ï¸  éªŒè¯é›†ä¸ºç©ºï¼Œå°†ä½¿ç”¨è®­ç»ƒé›†çš„ä¸€éƒ¨åˆ†ä½œä¸ºéªŒè¯é›†")
        # ä»è®­ç»ƒé›†ä¸­å–ä¸€éƒ¨åˆ†ä½œä¸ºéªŒè¯é›†
        val_size = min(len(train_paths) // 5, 50)  # å–è®­ç»ƒé›†çš„1/5æˆ–æœ€å¤š50ä¸ªæ ·æœ¬
        if val_size > 0:
            val_paths = train_paths[:val_size]
            val_landmarks = train_landmarks[:val_size]
            train_paths = train_paths[val_size:]
            train_landmarks = train_landmarks[val_size:]
            
            # é‡æ–°åˆ›å»ºæ•°æ®é›†
            train_dataset = FishLandmarkDataset(train_paths, train_landmarks, train_transform)  # ä½¿ç”¨æ‰€æœ‰å…³é”®ç‚¹
            val_dataset = FishLandmarkDataset(val_paths, val_landmarks, val_transform)  # ä½¿ç”¨æ‰€æœ‰å…³é”®ç‚¹
            
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)
            
            print(f"  é‡æ–°åˆ†é… - è®­ç»ƒé›†: {len(train_paths)} æ ·æœ¬, éªŒè¯é›†: {len(val_paths)} æ ·æœ¬")
        else:
            val_loader = None
            print("âš ï¸  æ•°æ®é‡å¤ªå°‘ï¼Œæ— æ³•åˆ›å»ºéªŒè¯é›†")
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ—ï¸  åˆ›å»ºæ¨¡å‹...")
    detector = FishLandmarkDetector()
    model = detector.create_model(backbone=backbone)
    
    print(f"  æ¨¡å‹æ¶æ„: {backbone}")
    print(f"  å…³é”®ç‚¹æ•°é‡: {len(detector.landmark_names)}")
    print(f"  è®¾å¤‡: {detector.device}")
    
    # è®­ç»ƒæ¨¡å‹
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    print(f"ğŸ”§ æŸå¤±å‡½æ•°ç±»å‹: {loss_type}")
    print(f"ğŸ”§ é”åº¦å‚æ•°: {sharpness} (å€¼è¶Šå¤§è¶Šé”åˆ©ï¼Œæƒ©ç½šè¶Šé‡)")
    print(f"ğŸ¯ é¢„æµ‹å…³é”®ç‚¹: {detector.landmark_names} (ä»…èº«ä½“ä¸­å¿ƒ)")
    if val_loader is not None:
        train_losses, val_losses = detector.train_with_configurable_loss(
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=epochs,
            lr=lr,
            save_dir=save_dir,
            sharpness=sharpness,
            loss_type=loss_type
        )
    else:
        print("âš ï¸  æ²¡æœ‰éªŒè¯é›†ï¼Œå°†åªè¿›è¡Œè®­ç»ƒï¼ˆä¸è¿›è¡ŒéªŒè¯ï¼‰")
        train_losses, val_losses = detector.train_without_validation_configurable_loss(
            train_loader=train_loader,
            epochs=epochs,
            lr=lr,
            save_dir=save_dir,
            sharpness=sharpness,
            loss_type=loss_type
        )
    
    # ä¿å­˜è®­ç»ƒé…ç½®
    config = {
        'data_dir': data_dir,
        'annotations_file': annotations_file,
        'epochs': epochs,
        'batch_size': batch_size,
        'lr': lr,
        'backbone': backbone,
        'train_samples': len(train_paths),
        'val_samples': len(val_paths),
        'test_samples': len(test_paths),
        'landmark_names': detector.landmark_names,
        'loss_type': loss_type,
        'sharpness': sharpness,
        'training_date': datetime.now().isoformat()
    }
    
    config_path = os.path.join(save_dir, 'training_config.json')
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {save_dir}")
    print(f"ğŸ“‹ è®­ç»ƒé…ç½®ä¿å­˜åœ¨: {config_path}")
    
    return detector, train_losses, val_losses


def test_model(model_path: str, test_data_dir: str, test_annotations: str, 
               output_dir: str = 'test_results', same_folder_mode: bool = False):
    """æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹"""
    
    print("="*60)
    print("ğŸ§ª æµ‹è¯•é±¼ä½“å…³é”®ç‚¹æ£€æµ‹æ¨¡å‹")
    print("="*60)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # åŠ è½½æ¨¡å‹
    try:
        detector = FishLandmarkDetector(model_path=model_path)
        if detector.model is None:
            raise ValueError("æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œmodelä¸ºNone")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    if same_folder_mode:
        print("ğŸ“‚ ä½¿ç”¨åŒæ–‡ä»¶å¤¹æ¨¡å¼ï¼šå›¾åƒå’ŒJSONæ–‡ä»¶åœ¨åŒä¸€ç›®å½•")
    loader = FishLandmarkDataLoader(test_data_dir, same_folder_mode=same_folder_mode)
    if test_annotations.endswith('.json'):
        image_paths, landmarks_list = loader.load_from_json(test_annotations)
    else:
        image_paths, landmarks_list = loader.load_from_txt(test_annotations)
    
    image_paths, landmarks_list = loader.validate_data(image_paths, landmarks_list)
    
    print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°: {len(image_paths)}")
    
    if len(image_paths) == 0:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•æ•°æ®")
        return
    
    # æµ‹è¯•æ¨¡å‹
    detector.model.eval()
    errors = []
    
    # åˆ›å»ºä¸è®­ç»ƒç›¸åŒçš„æ•°æ®å˜æ¢
    _, val_transform = create_data_transforms((256, 256))
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ˆä½¿ç”¨ä¸è®­ç»ƒç›¸åŒçš„é¢„å¤„ç†ï¼‰
    test_dataset = FishLandmarkDataset(image_paths, landmarks_list, val_transform)  # ä½¿ç”¨æ‰€æœ‰å…³é”®ç‚¹
    
    for i in range(len(test_dataset)):
        # ä»æ•°æ®é›†è·å–æ ·æœ¬ï¼ˆå·²ç»è¿‡é¢„å¤„ç†ï¼‰
        sample = test_dataset[i]
        image_tensor = sample['image']
        true_landmarks_normalized = sample['landmarks'].numpy()
        image_path = sample['image_path']
        original_size = sample['original_size']
        
        # åŠ è½½åŸå§‹å›¾åƒç”¨äºé¢„æµ‹ï¼ˆpredictæ–¹æ³•ä¼šè‡ªå·±å¤„ç†é¢„å¤„ç†ï¼‰
        original_image = cv2.imread(image_path)
        original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)
        
        # é¢„æµ‹å…³é”®ç‚¹ï¼ˆpredictæ–¹æ³•ä¼šå¤„ç†é¢„å¤„ç†å’Œåæ ‡è½¬æ¢ï¼‰
        pred_landmarks, pred_visibility = detector.predict(original_image_rgb)
        
        # å°†çœŸå®å…³é”®ç‚¹ä»å½’ä¸€åŒ–åæ ‡è½¬æ¢å›åƒç´ åæ ‡
        # ä½¿ç”¨ä¸predictæ–¹æ³•ç›¸åŒçš„è½¬æ¢é€»è¾‘
        original_h, original_w = original_size
        target_size = (256, 256)
        scale = min(target_size[0] / original_w, target_size[1] / original_h)
        true_landmarks_pixel = true_landmarks_normalized * np.array([target_size[0], target_size[1]]) / scale
        
        # è°ƒè¯•ä¿¡æ¯ï¼ˆå‰å‡ ä¸ªæ ·æœ¬ï¼‰
        if i < 3:
            print(f"  è°ƒè¯•æ ·æœ¬ {i+1}:")
            print(f"    åŸå§‹å›¾åƒå°ºå¯¸: {original_image_rgb.shape}")
            print(f"    åŸå§‹å°ºå¯¸: {original_size}")
            print(f"    ç¼©æ”¾æ¯”ä¾‹: {scale:.3f}")
            print(f"    çœŸå®å…³é”®ç‚¹(å½’ä¸€åŒ–): {true_landmarks_normalized}")
            print(f"    çœŸå®å…³é”®ç‚¹(åƒç´ ): {true_landmarks_pixel}")
            print(f"    é¢„æµ‹å…³é”®ç‚¹(åƒç´ ): {pred_landmarks}")
            print(f"    å¯è§æ€§: {pred_visibility}")
        
        # è®¡ç®—è¯¯å·®ï¼ˆç°åœ¨éƒ½åœ¨åƒç´ åæ ‡ç³»ä¸­ï¼‰
        error = np.linalg.norm(pred_landmarks - true_landmarks_pixel, axis=1)
        errors.append(error)
        
        # å¯è§†åŒ–ç»“æœ
        vis_image = detector.visualize_landmarks(original_image_rgb, pred_landmarks, pred_visibility)
        
        # ä¿å­˜ç»“æœ
        image_name = os.path.basename(image_path)
        save_path = os.path.join(output_dir, f"result_{i:03d}_{image_name}")
        cv2.imwrite(save_path, cv2.cvtColor(vis_image, cv2.COLOR_RGB2BGR))
        
        # è®¡ç®—é±¼çš„ç²¾ç¡®ä¸­å¿ƒï¼ˆä½¿ç”¨èº«ä½“ä¸­å¿ƒï¼‰
        fish_center = detector.calculate_fish_center(pred_landmarks, pred_visibility)
        true_center = detector.calculate_fish_center(true_landmarks_pixel, np.ones_like(pred_visibility))
        
        # æ˜¾ç¤ºèº«ä½“ä¸­å¿ƒå’Œå¤´éƒ¨ä¸­å¿ƒçš„é¢„æµ‹ç»“æœ
        pred_body = pred_landmarks[0] if len(pred_landmarks) > 0 else [0, 0]
        pred_head = pred_landmarks[1] if len(pred_landmarks) > 1 else [0, 0]
        true_body = true_landmarks_pixel[0] if len(true_landmarks_pixel) > 0 else [0, 0]
        true_head = true_landmarks_pixel[1] if len(true_landmarks_pixel) > 1 else [0, 0]
        
        print(f"æ ·æœ¬ {i+1:3d}: èº«ä½“ä¸­å¿ƒ é¢„æµ‹=({pred_body[0]:.1f}, {pred_body[1]:.1f}) çœŸå®=({true_body[0]:.1f}, {true_body[1]:.1f}) "
              f"å¤´éƒ¨ä¸­å¿ƒ é¢„æµ‹=({pred_head[0]:.1f}, {pred_head[1]:.1f}) çœŸå®=({true_head[0]:.1f}, {true_head[1]:.1f}) "
              f"ä¸­å¿ƒè¯¯å·®={np.linalg.norm(fish_center - true_center):.1f}px")
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    all_errors = np.concatenate(errors)
    mean_error = np.mean(all_errors)
    std_error = np.std(all_errors)
    max_error = np.max(all_errors)
    
    print(f"\nğŸ“ˆ æµ‹è¯•ç»“æœç»Ÿè®¡:")
    print(f"  å¹³å‡è¯¯å·®: {mean_error:.2f} Â± {std_error:.2f} åƒç´ ")
    print(f"  æœ€å¤§è¯¯å·®: {max_error:.2f} åƒç´ ")
    print(f"  ç»“æœå›¾åƒä¿å­˜åœ¨: {output_dir}")
    
    return mean_error, std_error, max_error


def print_usage_examples():
    """æ‰“å°è¯¦ç»†çš„ä½¿ç”¨ç¤ºä¾‹"""
    print("""
ğŸ“– é±¼ä½“å…³é”®ç‚¹æ£€æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬ä½¿ç”¨ç¤ºä¾‹:

ğŸš€ è®­ç»ƒæ¨¡å¼ç¤ºä¾‹:

1. åŸºæœ¬è®­ç»ƒ:
   python3 train_landmark_model.py --mode train \
       --data_dir ./process_data \
       --annotations ./process_data/train_annotations.json \
       --epochs 100

2. è‡ªå®šä¹‰å‚æ•°è®­ç»ƒ:
   python3 train_landmark_model.py --mode train \
       --data_dir ./process_data \
       --annotations ./process_data/train_annotations.json \
       --epochs 100 \
       --batch_size 128 \
       --lr 0.0005 \
       --backbone efficientnet \
       --exp_name gaussian_$(date +%Y%m%d_%H%M%S)

3. æŒ‡å®šå®éªŒåç§° (è‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³ç›®å½•):
   python3 train_landmark_model.py --mode train \
       --data_dir ./process_data \
       --annotations ./process_data/train_annotations.json \
       --exp_name body_center_only \
       --epochs 50

4. å¿«é€Ÿè®­ç»ƒ (å°‘é‡epochs):
   python train_landmark_model.py --mode train \
       --data_dir ./landmarks/processed_data \
       --annotations train_annotations.json \
       --epochs 50 \
       --batch_size 8

5. é«˜é”åº¦æ¤­åœ†æ ¸è®­ç»ƒ (æ›´ä¸¥æ ¼çš„æƒ©ç½š):
   python3 train_landmark_model.py --mode train \
       --data_dir ./process_data \
       --annotations ./process_data/train_annotations.json \
       --epochs 100 \
       --sharpness 3.0 \
       --loss_type ellipsoid

6. é«˜æ–¯æ ¸æŸå¤±è®­ç»ƒ:
   python3 train_landmark_model.py --mode train \
       --data_dir ./process_data \
       --annotations ./process_data/train_annotations.json \
       --epochs 100 \
       --loss_type gaussian \
       --sharpness 2.0

ğŸ§ª æµ‹è¯•æ¨¡å¼ç¤ºä¾‹:

1. åŸºæœ¬æµ‹è¯•:
   python3 train_landmark_model.py --mode test \
       --model_path ./models/best_fish_landmark_model.pth \
       --test_data_dir ./process_data \
       --test_annotations ./process_data/val_annotations.json

2. è‡ªå®šä¹‰è¾“å‡ºç›®å½•æµ‹è¯•:
   python3 train_landmark_model.py --mode test \
       --model_path ./experiments/gaussian_20250922_153626_20250922_153630/best_fish_landmark_model_gaussian.pth \
       --test_data_dir ./process_data \
       --test_annotations ./process_data/train_annotations.json \
       --output_dir ./my_test_results

    python3 train_landmark_model.py --mode test \
       --model_path ./experiments/ellipsoid_20250922_130057_20250922_130101/best_fish_landmark_model_ellipsoid.pth \
       --test_data_dir ./process_data \
       --test_annotations ./process_data/val_annotations.json \
       --output_dir ./my_test_results_ellipsoid

ğŸ“ æ•°æ®ç›®å½•ç»“æ„è¦æ±‚:
   data_dir/
   â”œâ”€â”€ images/                    # å›¾åƒæ–‡ä»¶
   â”‚   â”œâ”€â”€ train_image1.jpg
   â”‚   â”œâ”€â”€ train_image2.jpg
   â”‚   â””â”€â”€ ...
   â””â”€â”€ landmarks/                 # å…³é”®ç‚¹numpyæ–‡ä»¶
       â”œâ”€â”€ train_image1.npy
       â”œâ”€â”€ train_image2.npy
       â””â”€â”€ ...

ğŸ“„ æ ‡æ³¨æ–‡ä»¶æ ¼å¼:
   # JSONæ ¼å¼ (æ¨è) - ä»…ä½¿ç”¨èº«ä½“ä¸­å¿ƒ
   {
     "train_image1.jpg": {
       "landmarks": [[100, 50], [100, 150]],  # [å¤´éƒ¨ä¸­å¿ƒ, èº«ä½“ä¸­å¿ƒ] - ä»…ä½¿ç”¨èº«ä½“ä¸­å¿ƒ
       "visibility": [1, 1]
     },
     "train_image2.jpg": {
       "landmarks": [[120, 60], [120, 160]],  # [å¤´éƒ¨ä¸­å¿ƒ, èº«ä½“ä¸­å¿ƒ] - ä»…ä½¿ç”¨èº«ä½“ä¸­å¿ƒ
       "visibility": [1, 1]
     }
   }

ğŸ”§ å‚æ•°è¯´æ˜:
   --mode: è¿è¡Œæ¨¡å¼ (train/test)
   --data_dir: æ•°æ®ç›®å½•è·¯å¾„
   --annotations: æ ‡æ³¨æ–‡ä»¶è·¯å¾„
   --epochs: è®­ç»ƒè½®æ•° (é»˜è®¤: 100)
   --batch_size: æ‰¹æ¬¡å¤§å° (é»˜è®¤: 16)
   --lr: å­¦ä¹ ç‡ (é»˜è®¤: 0.001)
   --backbone: æ¨¡å‹æ¶æ„ (resnet18/efficientnet, é»˜è®¤: resnet18)
   --save_dir: æ¨¡å‹ä¿å­˜ç›®å½• (é»˜è®¤: models)
   --test_split: æµ‹è¯•é›†æ¯”ä¾‹ (é»˜è®¤: 0.2)
   --val_split: éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤: 0.2)

âš ï¸  æ³¨æ„äº‹é¡¹:
   - ç¡®ä¿æ•°æ®ç›®å½•åŒ…å«imageså’Œlandmarkså­ç›®å½•
   - å›¾åƒæ–‡ä»¶åå’Œæ ‡æ³¨æ–‡ä»¶ä¸­çš„é”®åå¿…é¡»åŒ¹é…
   - å…³é”®ç‚¹åæ ‡æ ¼å¼: [[x1, y1], [x2, y2]] (å¤´éƒ¨ä¸­å¿ƒ, èº«ä½“ä¸­å¿ƒ) - ä»…ä½¿ç”¨èº«ä½“ä¸­å¿ƒ
   - å¯è§æ€§æ ¼å¼: [1, 1] (1=å¯è§, 0=ä¸å¯è§)
   - è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹å’Œè®­ç»ƒé…ç½®
""")


def main():
    parser = argparse.ArgumentParser(
        description='é±¼ä½“å…³é”®ç‚¹æ£€æµ‹æ¨¡å‹è®­ç»ƒ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  %(prog)s --mode train --data_dir ./data --annotations train_annotations.json --epochs 100
  %(prog)s --mode test --model_path ./models/best_fish_landmark_model.pth --test_data_dir ./test_data --test_annotations test_annotations.json
  %(prog)s --help  # æŸ¥çœ‹è¯¦ç»†ä½¿ç”¨è¯´æ˜
        """
    )
    parser.add_argument('--mode', type=str, choices=['train', 'test'], required=True, 
                       help='è¿è¡Œæ¨¡å¼: train(è®­ç»ƒ) æˆ– test(æµ‹è¯•)')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--data_dir', type=str, 
                       help='æ•°æ®ç›®å½• (åŒ…å«imageså’Œlandmarkså­ç›®å½•)')
    parser.add_argument('--annotations', type=str, 
                       help='æ ‡æ³¨æ–‡ä»¶è·¯å¾„ (JSONæˆ–TXTæ ¼å¼)')
    parser.add_argument('--epochs', type=int, default=100, 
                       help='è®­ç»ƒè½®æ•° (é»˜è®¤: 100)')
    parser.add_argument('--batch_size', type=int, default=16, 
                       help='æ‰¹æ¬¡å¤§å° (é»˜è®¤: 16)')
    parser.add_argument('--lr', type=float, default=0.001, 
                       help='å­¦ä¹ ç‡ (é»˜è®¤: 0.001)')
    parser.add_argument('--backbone', type=str, default='resnet18', 
                       choices=['resnet18', 'efficientnet'], 
                       help='æ¨¡å‹æ¶æ„ (é»˜è®¤: resnet18)')
    parser.add_argument('--save_dir', type=str, default=None, 
                       help='æ¨¡å‹ä¿å­˜ç›®å½• (é»˜è®¤: è‡ªåŠ¨ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„å®éªŒç›®å½•)')
    parser.add_argument('--exp_name', type=str, default='fish_landmark', 
                       help='å®éªŒåç§° (é»˜è®¤: fish_landmark)')
    parser.add_argument('--test_split', type=float, default=0.2, 
                       help='æµ‹è¯•é›†æ¯”ä¾‹ (é»˜è®¤: 0.2)')
    parser.add_argument('--val_split', type=float, default=0.2, 
                       help='éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤: 0.2)')
    parser.add_argument('--same_folder_mode', action='store_true', 
                       help='åŒæ–‡ä»¶å¤¹æ¨¡å¼ï¼šå›¾åƒå’ŒJSONæ–‡ä»¶åœ¨åŒä¸€ç›®å½• (é»˜è®¤: False)')
    parser.add_argument('--sharpness', type=float, default=1.0, 
                       help='é”åº¦å‚æ•° (é»˜è®¤: 1.0, å€¼è¶Šå¤§è¶Šé”åˆ©ï¼Œæƒ©ç½šè¶Šé‡)')
    parser.add_argument('--loss_type', type=str, default='ellipsoid', 
                       choices=['gaussian', 'ellipsoid'],
                       help='æŸå¤±å‡½æ•°ç±»å‹ (é»˜è®¤: ellipsoid) - gaussian: é«˜æ–¯æ ¸æŸå¤±, ellipsoid: æ¤­åœ†æ ¸æŸå¤±')
    
    # æµ‹è¯•å‚æ•°
    parser.add_argument('--model_path', type=str, 
                       help='æ¨¡å‹æ–‡ä»¶è·¯å¾„ (.pthæ–‡ä»¶)')
    parser.add_argument('--test_data_dir', type=str, 
                       help='æµ‹è¯•æ•°æ®ç›®å½•')
    parser.add_argument('--test_annotations', type=str, 
                       help='æµ‹è¯•æ ‡æ³¨æ–‡ä»¶')
    parser.add_argument('--output_dir', type=str, default='test_results', 
                       help='æµ‹è¯•ç»“æœè¾“å‡ºç›®å½• (é»˜è®¤: test_results)')
    
    args = parser.parse_args()
    
    # ç”Ÿæˆå®éªŒç›®å½•åç§°
    if args.save_dir is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        args.save_dir = f"experiments/{args.exp_name}_{timestamp}"
    
    print(f"ğŸ“ å®éªŒç›®å½•: {args.save_dir}")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯å¸®åŠ©è¯·æ±‚
    if '--help' in sys.argv or '-h' in sys.argv:
        print_usage_examples()
        return
    
    if args.mode == 'train':
        if not args.data_dir or not args.annotations:
            print("âŒ é”™è¯¯: è®­ç»ƒæ¨¡å¼éœ€è¦æŒ‡å®š --data_dir å’Œ --annotations")
            print("\nğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
            print("python train_landmark_model.py --mode train --data_dir ./data --annotations train_annotations.json")
            print("\nğŸ“– æŸ¥çœ‹è¯¦ç»†å¸®åŠ©:")
            print("python train_landmark_model.py --help")
            return
        
        print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å¼...")
        train_model(
            data_dir=args.data_dir,
            annotations_file=args.annotations,
            epochs=args.epochs,
            batch_size=args.batch_size,
            lr=args.lr,
            backbone=args.backbone,
            save_dir=args.save_dir,
            test_split=args.test_split,
            val_split=args.val_split,
            sharpness=args.sharpness,
            loss_type=args.loss_type
        )
    
    elif args.mode == 'test':
        if not args.model_path or not args.test_data_dir or not args.test_annotations:
            print("âŒ é”™è¯¯: æµ‹è¯•æ¨¡å¼éœ€è¦æŒ‡å®š --model_path, --test_data_dir å’Œ --test_annotations")
            print("\nğŸ’¡ ä½¿ç”¨ç¤ºä¾‹:")
            print("python train_landmark_model.py --mode test --model_path ./models/best_model.pth --test_data_dir ./test_data --test_annotations test_annotations.json")
            print("\nğŸ“– æŸ¥çœ‹è¯¦ç»†å¸®åŠ©:")
            print("python train_landmark_model.py --help")
            return
        
        print("ğŸ§ª å¼€å§‹æµ‹è¯•æ¨¡å¼...")
        test_model(
            model_path=args.model_path,
            test_data_dir=args.test_data_dir,
            test_annotations=args.test_annotations,
            output_dir=args.output_dir
        )


if __name__ == "__main__":
    import sys
    
    # å¦‚æœæ²¡æœ‰å‚æ•°ï¼Œæ˜¾ç¤ºä½¿ç”¨ç¤ºä¾‹
    if len(sys.argv) == 1:
        print_usage_examples()
        sys.exit(0)
    
    main()
